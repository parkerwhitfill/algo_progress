Unnamed: 0,System,Author(s),Publication date,Year,Reference,Citations,Peer reviewed?,Link,Parameters,Hardware,Training Compute,Epoch,Epoch (pretrain),Epoch (finetune),uncertain,Pretrain Dataset Size,Inferred_compute (6ND),Finetune Dataset Size,Dataset Size,Dataset(s),Perplexity (WT103),Perplexity (WT2),Perplexity (PTB),Zero-shot?,Include?,Uses Cache,Outlier?,Architecture,Base Model,GitHub,Organizations,Organization Categorization,Comments,Complete row,Tokenizer,Vocabulary,algorithmic,optimizer_quality,architecture_quality,attention_mechanism,training_efficiency,activation_functions,normalization_techniques,learning_paradigm,regularization_techniques,special_algorithms,overall_algorithm_quality
0,CD-GraB (WT2),"A. Feder Cooper, Wentao Guo, Khiem Pham, Tiancheng Yuan, Charlie F. Ruan, Yucheng Lu, Christopher De Sa",2023/02/02,2023,CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training,0.0,0.0,https://arxiv.org/pdf/2302.00845.pdf,UNK,,,50,,,0.0,2080000.0,#VALUE!,,2080000.0,WikiText-2,,223.44,,0.0,0.0,0,0,Recurrent,LSTM,,,,,0,GPT2Tokenizer,50257,"Architecture: 2-layer LSTM (used in one experiment; others use MLP and Logistic Regression); Optimizer: SGD with momentum (except GPT2 experiment uses AdamW); LR Schedule: Linear decay learning rate scheduler (some experiments also use decay by 0.1 per 10 epochs); Training: Gradient balancing using PairBalance, Parallel herding, Parameter averaging; Regularization: Weight decay, Dropout = 0 (in one experiment); otherwise unspecified; Special Algorithms: CD-GraB: Coordinated Distributed Gradient Balancing for example ordering, Online PairBalance for improved balancing; Other: Permutation-based example ordering",4.0,3.0,0.0,4.0,3.0,1.0,3.0,3.0,6.0,4.0
1,SRU++ Large,Tao Lei,2021/02/24,2021,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,35.0,,https://arxiv.org/abs/2102.12459,2.34E+08,,1.1e+19,34.08,,,0.0,103000000.0,4.93E+18,0.0,103000000.0,WikiText-103,17.1,,,0.0,1.0,0,0,Recurrent,SRU,https://github.com/asappresearch/sru,,,,1,,260000,"Architecture: SRU++ with single-head attention in each layer. Experiments conducted with attention enabled every k layers, also with 10 layers in total. SRU++ can also include a projection to reduce the number of parameters.; Optimizer: RAdam with default beta values; LR Schedule: Cosine learning rate schedule following Dai et al. (2019); Training: Automatic Mixed Precision (AMP) training, Distributed Data Parallel (DDP); Attention: Single-head scaled dot-product attention; Regularization: Fixed weight decay of 0.1, Dropout with probability tuned on the dev set; Initialization: Residual connection weight alpha is initialized to zero, allowing recurrence to capture sequential patterns during early training.",8.0,5.0,5.0,7.0,6.0,6.0,4.0,5.0,6.0,5.0
2,SRU++ Large only 2 attention layers (k=5),Tao Lei,2021/02/24,2021,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,35.0,,https://arxiv.org/abs/2102.12459,2.25E+08,,8e+18,34.08,,,0.0,103000000.0,4.74E+18,0.0,103000000.0,WikiText-103,17.3,,,0.0,1.0,0,0,Recurrent,SRU,https://github.com/asappresearch/sru,,,,1,,260000,"Architecture: SRU++ with fast recurrence and single-head attention, 10 layers of SRU++; Optimizer: RAdam with default beta values; LR Schedule: Cosine learning rate schedule; Training: Automatic mixed precision (AMP) training, Distributed data parallel (DDP); Attention: Single-head attention; Regularization: Weight decay 0.1, Dropout (tuned); Initialization: Learned scalar alpha for residual connection initialized to zero; Other: Post-layer normalization after attention operation and before matrix multiplication with W°, Skip attention (attention every k layers), Tying the weight matrices of the adaptive embedding layer and adaptive softmax layer",8.0,6.0,3.0,6.0,5.0,5.0,3.0,5.0,4.0,5.0
3,Alleviated TOI 10 (WT103),"Noémien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros Lazaridis, Andreas Fischer, Claudiu Musat",2019/09/18,2019,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,0.0,1.0,https://arxiv.org/abs/1909.08700,UNK,,,1000,,,1.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-103,32.85,,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/nkcr/overlap-ml,,,,0,?,?,"Architecture: LSTM, AWD-LSTM, Transformer, Mixture of Softmaxes (MoS); Optimizer: SGD; LR Schedule: Not explicitly mentioned; Attention: Encoder of Transformer used with convolutional layers for speech emotion recognition; Regularization: Weight-Dropped LSTM (AWD-LSTM); Special Algorithms: Alleviated TOI (Token Order Imbalance); Initialization: Not mentioned",,,,,,,,,,
4,GPT2-LayerFusion-WS,"James O' Neill, Greg Ver Steeg, Aram Galstyan",2020/07/29,2020,Compressing Deep Neural Networks via Layer Fusion,5.0,,https://arxiv.org/pdf/2007.14917,,,,,,,0.0,103000000.0,,2080000.0,105000000.0,,,13.71,,0.0,0.0,0,0,Transformer,GPT,,,,,0,?,?,"Architecture: Transformer (GPT based); Optimizer: Not explicitly stated, assumed to be Adam or similar commonly used with Transformers; LR Schedule: Exponential curriculum schedule (allocate more compression earlier in retraining but more retraining steps later); Training: Layer Fusion (freezing, averaging, mixing); Attention: Multi-head attention (inferred from Transformer architecture); Special Algorithms: Layer Fusion (LF) using Wasserstein Distance, Covariance Alignment, Euclidean Distance, KL Divergence; Initialization: Pretrained",6.0,6.0,5.0,6.0,6.0,5.0,6.0,5.0,7.0,6.0
5,TransfoRNN(d=1024)(2-layer) (WT2),"Tze Yuang Chong, Xuyang Wang, Lin Yang, Junjie Wang",2021/04/04,2021,TransfoRNN: Capturing the Sequential Information in Self-Attention Representations for Language Modeling,0.0,0.0,https://arxiv.org/pdf/2104.01572,9.76E+07,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,,,94.8,,0.0,0.0,0,1,Transformer/RNN,TransfoRNN,,,,,1,Own,33000,"Architecture: TransfoRNN: Transformer layers cascaded with Recurrent Neural Networks (RNNs). The RNNs are implemented as Long Short-Term Memory (LSTM) neural networks.; Optimizer: Stochastic Gradient Descent (SGD) with new-bob learning rate adjustment; LR Schedule: new-bob learning rate adjustment; Attention: Multi-head attention (8 heads); Regularization: layer normalization, residual connection; Initialization: Not mentioned; Other: Positional encoding of the embedding vectors, Input embedding tied to the output embedding",,,,,,,,,,
6,SRU++ Base,Tao Lei,2021/02/24,2021,When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute,35.0,,https://arxiv.org/abs/2102.12459,1.08E+08,,5.8e+18,25.56,,,0.0,103000000.0,1.71E+18,0.0,103000000.0,WikiText-103,18.3,,,0.0,1.0,0,0,Recurrent,SRU,https://github.com/asappresearch/sru,,,,1,,260000,"Architecture: SRU++ architecture with multiple layers, combining fast recurrence (SRU) and attention.  The paper mentions using 10 SRU++ layers for language modeling and 6 SRU++ layers for machine translation experiments. A single-head attention mechanism is used.; Optimizer: RAdam with default beta values (0.9 and 0.999); LR Schedule: Cosine learning rate schedule following Dai et al. (2019), using only 1 cycle for simplicity.; Training: Automatic Mixed Precision (AMP), Distributed Data Parallel (DDP) in Pytorch; Attention: Single-head scaled dot-product attention. Key and value are computed using Q instead of X to reduce parameter size.  The final output U is computed as U = W(Q + a*A) which is a residual connection.; Regularization: Weight decay 0.1 (fixed) or tuned values. Dropout probability tuned according to model size and results on dev set.; Initialization: The scalar 'a' in the residual connection is initialized to zero.; Other: Post-layer normalization after attention operation and before multiplication with W. Attention enabled every k layers, with layers without attention using a dimension projection variant of SRU.",,,,,,,,,,
7,VD-LSTM+REAL Large,"Hakan Inan, Khashayar Khosravi, Richard Socher",2016/11/04,2016,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,397.0,,https://arxiv.org/abs/1611.01462,5.10E+07,,,75,,,0.0,929000.0,2.13E+16,,929000.0,Penn TreeBank,,,68.5,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,10000,"Architecture: 2-layer LSTM; Optimizer: Stochastic gradient descent; LR Schedule: Starts with a learning rate of 1 and decays with a constant rate after a certain epoch. Decay rate is 0.9 or 0.97 depending on the network size.; Training: Dropout, Gradient clipping; Attention: Not applicable; Regularization: Variational Dropout, Weight decay (not used in a specific set of experiments to validate a theoretical result); Special Algorithms: Loss augmentation with KL-divergence between the model's prediction and an estimated target distribution based on word embeddings., Tying the input word embedding matrix to the output classification matrix; Initialization: Not mentioned; Other: Temperature parameter used in the softmax function for the predicted distribution.",,,,,,,,,,
8,Turing-NLG,Corby Rosset,2020/02/13,2020,Turing-NLG: A 17-billion-parameter language model by Microsoft,,,https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/,1.70E+10,,1.57e+22,3.39,,,0.0,46400000000.0,1.60E+22,,46400000000.0,,10.21,,,0.5,1.0,0,0,Transformer,NLG,,,,,1,?,?,,,,,,,,,,,
9,TrellisNet,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018/10/15,2018,Trellis Networks for Sequence Modeling,132.0,,https://arxiv.org/abs/1810.06682,1.80E+08,,,25,,,0.0,103000000.0,2.78E+18,0.0,103000000.0,WikiText-103,29.19,,,0.0,1.0,0,0,Recurrent/Convolutional,TrellisNet,https://github.com/locuslab/trellisnet,,,,1,word-level,268000,"Architecture: Trellis Networks, a new architecture for sequence modeling. It's a temporal convolutional network (TCN) with weight tying across depth and direct injection of the input into deep layers.  It can also generalize truncated recurrent networks with special sparsity structure.; Optimizer: SGD and Adam are used, depending on the experiment; LR Schedule: Learning rate is decayed by a fixed factor once validation error plateaus or according to a fixed schedule; Training: LSTM gating used, truncate backpropagation through time; Regularization: Weight tying across layers is a form of regularization., Variational dropout, Recurrent DropConnect, Weight Normalization, L1 regularization; Initialization: Input layer of TrellisNet initializes to z(0) = 0.; Other: Auxiliary losses at intermediate layers, Larger kernels and dilated convolutions, Parallelism - leveraging the parallel processing in the convolution operation, History compression and repackaging",,,,,,,,,,
10,TrellisNet-MoS (1.4x larger),"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018/10/15,2018,Trellis Networks for Sequence Modeling,132.0,,https://arxiv.org/abs/1810.06682,1.80E+08,,,25,,,0.0,103000000.0,2.78E+18,0.0,103000000.0,Penn TreeBank,29.19,,54.19,0.0,1.0,0,0,Recurrent/Convolutional,TrellisNet,https://github.com/locuslab/trellisnet,,,,1,word-level,268000,"Architecture: Trellis Network.  A temporal convolutional network with special structure, characterized by weight tying across depth and direct injection of the input into deep layers. Generalizes truncated recurrent networks.; Optimizer: SGD for word-level PTB, Adam for Word-level WikiText-103 and character level PTB, and (P)MNIST/CIFAR-10; LR Schedule: Decay the learning rate by a fixed factor once validation error plateaus.; Training: Truncated backpropagation-through-time (BPTT), Deep supervision, Parallelism; Attention: Not applicable; Regularization: Weight tying across layers, Variational dropout (VD), Weight dropout, Weight Normalization; Initialization: Not mentioned; Other: Gated activation based on LSTM cell, Larger kernels and dilated convolutions, Mixed group convolutions, History repackaging",,,,,,,,,,
11,Transformer-XL Large,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",2019/01/09,2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,3155.0,,https://arxiv.org/abs/1901.02860,2.57E+08,,1.09e+19,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-103,18.3,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/kimiyoung/,,,,1,word-level,260000,"Architecture: Transformer-XL, a deep self-attention network with a segment-level recurrence mechanism. The recurrence mechanism reuses hidden states from previous segments as memory for the current segment. The key and value matrices are conditioned on the extended context, which are the hidden states from the previous segment.; Optimizer: Not explicitly mentioned in the provided text. The original paper should be consulted for this information.; LR Schedule: Not explicitly mentioned in the provided text. The original paper should be consulted for this information.; Training: Stop-gradient for the hidden states being reused as memory.; Attention: Multi-head self-attention. The key and value matrices are conditioned on the extended context, which are the hidden states from the previous segment. The paper uses relative positional encodings instead of absolute ones. The attention score is decomposed into four intuitive terms (content-based addressing, content-dependent positional bias, global content bias, and global positional bias).; Regularization: Variational dropout and weight averaging mentioned in relation to performance on Penn Treebank, however it is unknown if these were used on all training runs; Special Algorithms: Segment-level recurrence mechanism, Relative positional encoding scheme; Initialization: Not explicitly mentioned in the provided text.; Other: Masked Softmax",,,,,,,,,,
12,Transformer-XL-ptb,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov",2019/01/09,2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,3155.0,,https://arxiv.org/abs/1901.02860,2.57E+08,,1.09e+19,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,Penn TreeBank,,,54.52,1.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/kimiyoung/,,,,1,word-level,10000,"Architecture: Transformer-XL: enables learning dependency beyond a fixed length with a segment-level recurrence mechanism and a novel positional encoding scheme.  It's a deep self-attention network where hidden states from previous segments are reused as memory for the current segment.; Optimizer: Not explicitly mentioned in the document. Further research may be required to locate the specific optimizer utilized.; LR Schedule: Not explicitly mentioned in the document. Further research may be required to locate the specific learning rate schedule utilized.; Training: Caching hidden states from previous segments, stop-gradient; Attention: Self-attention with relative positional encodings. Includes content-based addressing, content-dependent positional bias, global content bias, and global positional bias; Regularization: dropout, weight average; Special Algorithms: Segment-level recurrence, Relative positional encoding formulation; Initialization: Not mentioned explicitly, though better initialization for RNNs is mentioned in the related work.; Other: Truncated BPTT-like behavior due to recurrence scheme",,,,,,,,,,
13,NoPos,"Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, Omer Levy",2022/03/30,2022,Transformer Language Models without Positional Encodings Still Learn Positional Information,30.0,,https://arxiv.org/abs/2203.16634,1.30E+09,,,199.92,,,0.0,103000000.0,1.61E+20,0.0,103000000.0,The Pile (Subset),20.97,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/adihaviv/NoPos,,,,1,word-level,267000,"Architecture: Transformer, using Transformer-XL as base model; Optimizer: Nesterov accelerated gradient (NAG) for WikiText-103 and Adam for The Pile.; LR Schedule: Warmup steps followed by unspecified decay. The peak learning rate and warmup steps are described in the table 5.; Training: Dropout, Adaptive embedding (for WikiText-103 experiment); Attention: Multi-head attention with varying number of heads according to the model size. 8 attention heads for WikiText-103 experiment; Regularization: Dropout, Weight Decay; Initialization: Not mentioned explicitly in the paper; Other: Causal attention mask",,,,,,,,,,
14,Chinchilla,"Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre",2022/03/29,2022,Training Compute-Optimal Large Language Models,136.0,,https://arxiv.org/abs/2203.15556,7.00E+10,,5.76e+23,1,,,0.0,1400000000000.0,5.88E+23,0.0,1400000000000.0,WikiText-103,7.16,,,1.0,1.0,0,0,Transformer,GPT,,,,,1,SentencePiece,32000,Architecture: Transformer; Optimizer: AdamW; LR Schedule: cosine schedule; Attention: Multi-head Attention; Regularization: Decoupled weight decay regularization (via AdamW); Initialization: Not explicitly mentioned,,,,,,,,,,
15,"ALiBi (L=3072, Lvalid = 3072)","Ofir Press, Noah A. Smith, Mike Lewis",2021/08/27,2021,"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation",124.0,,https://arxiv.org/abs/2108.12409,1.30E+09,,1.8e+20,205,,,0.0,103000000.0,1.65E+20,0.0,103000000.0,WikiText-103,18.3,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/ofirpress/attention_with_linear_biases,,,,1,BERT,29000,"Architecture: Transformer with 16 or 25 transformer layers with a dimension of 1024 or 2048, 8 or 16 heads, and a feedforward inner dimension of 4096 or 8192. Ties the word embedding and softmax matrices.; Optimizer: Not mentioned; LR Schedule: Not mentioned; Training: Causal Masking, Nonoverlapping Inference, Sliding Window Inference; Attention: Multi-head attention with 8 or 16 heads.; Special Algorithms: Attention with Linear Biases (ALiBi): biases query-key attention scores with a penalty that is proportional to their distance. The bias is: softmax(qK + m · [−(i - 1), ..., -2, -1, 0]), where scalar m is a head-specific slope fixed before training.; Initialization: Not mentioned; Other: Head-specific slopes in ALiBi are a geometric sequence. Slopes are not trainable.",,,,,,,,,,
16,Integer Transformer,"Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, Jingbo Zhu",2020/09/17,2020,Towards Fully 8-bit Integer Inference for the Transformer Model,26.0,,https://arxiv.org/pdf/2009.08034,5.97E+07,,,,,,0.0,103000000.0,0.00E+00,,103000000.0,,18.16,,,0.0,1.0,0,0,Transformer,,,,,,1,,32000,"Architecture: Transformer with 6-layer encoder and 6-layer decoder in both base and big settings.; Optimizer: Adam with β₁ = 0.9 and β₂ = 0.997; LR Schedule: Inverse square root learning rate schedule with 8K warmup steps. Learning rate = 0.001/0.0007 for Transformer-base/big.; Training: Scale Propagation, Integer Transformer, de-quantization when necessary, Re-scaling; Attention: Self-attention takes three tensors, Q, K and V, as inputs and produces a tensor with the same size as the output, Attention(Q, K, V) = SoftMax(QKT/√dm)V, standard attention replaced by polynomial attention, PolyAttn(Q, K, V) = Poly(QK^T)V/ΣjPoly(QK^T); Special Algorithms: Scale Propagation, Polynomial Attention, Scale Matching, Integer Transformer; Initialization: The scale is initialized through s = max(|r|) initially, where r is the network input.; Other: L1 Layer Normalization, approximates the standard deviation with its L1-norm equivalent",,,,,,,,,,
17,Integer Transformer,"Ye Lin, Yanyang Li, Tengbo Liu, Tong Xiao, Tongran Liu, Jingbo Zhu",2020/09/17,2020,Towards Fully 8-bit Integer Inference for the Transformer Model,26.0,,https://arxiv.org/pdf/2009.08034,2.47E+08,,,,,,0.0,103000000.0,0.00E+00,,103000000.0,,30.79,,,0.0,1.0,0,0,Transformer,,,,,,1,,32000,"Architecture: Transformer with 6-layer encoder and 6-layer decoder for base setting, and 6/16 layers for lm-base/big. Replaces the exponential function in the standard attention by the polynomial function, and replace the square root function in the layer normalization with the absolute value function.; Optimizer: Adam with β₁ = 0.9 and β₂ = 0.997. Nesterov's accelerated gradient for lm-big training.; LR Schedule: Inverse square root learning rate schedule with 8K warmup steps. Cosine learning rate schedule with 16K warmup steps for lm-big training.; Attention: Polynomial Attention; Special Algorithms: Scale Propagation; Other: L1 Layer Normalization",,,,,,,,,,
18,TaLK Convolution,"Vasileios Lioutas, Yuhong Guo",2020/02/08,2020,Time-aware Large Kernel Convolutions,24.0,,https://arxiv.org/abs/2002.03184,2.40E+08,,,187.43,,,0.0,103000000.0,2.78E+19,0.0,103000000.0,WikiText-103,23.3,,,0.0,1.0,0,0,Convolutional,Transformer,https://github.com/lioutasb/TaLKConvolutions,,,,1,,260000,"Architecture: TaLK Convolution: adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. Uses depthwise version by separating feature channels into multiple groups, each of which shares the same pair of left and right offsets.; Optimizer: Adam optimizer with default values; LR Schedule: Cosine learning rate schedule with linear warm-up for 10K steps from 10^-7 to 10^-3. Inverse square root learning rate schedule for IWSLT De-En.; Training: Gated Linear Unit (GLU), fast Parallel Prefix Sum to compute the integral image; Attention: Not Applicable; Regularization: Output normalization, Offsets Dropout with probability p during training; Special Algorithms: Time-aware Large Kernel Convolutions (TaLK), adaptive convolution based on summation kernel; Initialization: Not mentioned; Other: Used Swish activation functions instead of ReLU",,,,,,,,,,
19,VRNS-RNN-3-3-5,"Brian DuSell, David Chiang",2022/10/04,2022,The Surprising Computational Power of Nondeterministic Stack RNNs,1.0,1.0,https://arxiv.org/pdf/2210.01343,1.50E+06,,,,,,1.0,929000.0,0.00E+00,,929000.0,,,,120.12,0.0,1.0,0,1,Recurrent,RNN,,,,,1,?,?,"Architecture: LSTM controller connected to a differentiable stack.; Optimizer: SGD; LR Schedule: Divided the learning rate by 1.5 whenever validation perplexity did not improve after an epoch.; Training: truncated backpropagation through time (BPTT); Special Algorithms: VRNS-RNN; Initialization: Fully-connected layers (except those in LSTM) used Xavier uniform initialization, other parameters initialized uniformly.; Other: incremental execution technique (limits non-zero entries in tensors).",,,,,,,,,,
20,GPT3-6.7B (rerun of original),"Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",2020/05/28,2020,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,37.0,,https://web.archive.org/web/20221014063419/https://arxiv.org/pdf/2203.03466.pdf,6.70E+09,,1.2e+22,1,,,0.0,499000000000.0,2.01E+22,,499000000000.0,,9.13,,,1.0,1.0,0,0,Transformer,GPT,https://github.com/microsoft/mup,,,,1,custom,?,"Architecture: Transformer; Optimizer: Adam; LR Schedule: See Table 2 for transferable LR schedules; Attention: multi-head attention; Regularization: dropout, weight decay; Special Algorithms: Maximal Update Parametrization (µP); Initialization: LeCun initialization",,,,,,,,,,
21,GPT3-6.7B + muP,"Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, Jianfeng Gao",2022/03/07,2022,Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer,37.0,,https://web.archive.org/web/20221014063419/https://arxiv.org/pdf/2203.03466.pdf,6.70E+09,4.99E+11,1.28e+22,1,,,0.0,499000000000.0,2.01E+22,103000000.0,499000000000.0,,8.56,,,1.0,1.0,0,0,Transformer,GPT,https://github.com/microsoft/mup,,,,1,custom,?,"Architecture: Transformer; Optimizer: Adam; LR Schedule: Linear decay (modified), StepLR, cosine annealing, constant, inverse square-root decay are used for experiments; Attention: Relative attention (compared to absolute attention used in original GPT3 paper); Special Algorithms: Maximal Update Parametrization (µP), µTransfer; Initialization: LeCun Initialization, Glorot initialization is also mentioned",,,,,,,,,,
22,Subformer (83M),"Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo",2021/01/01,2021,Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers,12.0,,https://arxiv.org/abs/2101.00234,8.30E+07,,,70.29,,,0.0,103000000.0,3.61E+18,0.0,103000000.0,WikiText-103,20.88,,,0.0,1.0,0,0,Transformer,Subformer,https://github.com/machelreid/subformer,,,,1,,260000,"Architecture: Transformer-based architecture with two novel techniques: (1) SAFE (Self-Attentive Factorized Embeddings) and (2) Sandwich-style Parameter Sharing. The Subformer consists of an embedding layer, model layers, a sandwich module and projection layers. The embedding and sandwiched layers can have different dimensions than the model layer.; Optimizer: Adam with hyper-parameters β = (0.9, 0.999) and € = 10-6; LR Schedule: Warm up the learning rate to a peak of 5 × 10-4 within 10K iterations and then decay the learning rate with the inverse square root schedule.  In the Language Modeling experiments, the optimizer used is Nesterov's accelerated gradient optimizer, warming up the learning rate to 1.0 for 16K iterations, and then annealing for 270K iterations using a cosine annealing schedule.; Training: mixed precision; Attention: The SAFE technique uses a small self-attention layer to reduce embedding parameter count.; Regularization: dropout, weight decay, label-smoothed cross entropy loss; Special Algorithms: SAFE (Self-Attentive Factorized Embeddings), Sandwich-style Parameter Sharing; Initialization: Same weight initialization scheme as BERT, sampling weights from N (0, 0.02), initializing biases to zero and setting layer normalization parameters β and γ to be 0 and 1, respectively.",,,,,,,,,,
23,Subformer (122M),"Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo",2021/01/01,2021,Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers,12.0,,https://arxiv.org/abs/2101.00234,1.22E+08,,,70.29,,,0.0,103000000.0,5.30E+18,0.0,103000000.0,WikiText-103,19.9,,,0.0,1.0,0,0,Transformer,Subformer,https://github.com/machelreid/subformer,,,,1,,260000,"Architecture: Transformer architecture with two novel techniques: SAFE (Self-Attentive Factorized Embeddings) and Sandwich-style Parameter Sharing. Uses encoder and decoder.; Optimizer: Adam with hyper-parameters β = (0.9, 0.999) and € = 10-6; LR Schedule: Warm up the learning rate to a peak of 5 × 10-4 within 10K iterations and then decay the learning rate with the inverse square root schedule.; Training: mixed precision training; Attention: multi-headed attention in Transformer layers; Regularization: dropout, weight decay of 0.01, label-smoothed cross entropy loss with є = 0.1, Layer normalization with parameters Band γ to be 0 and 1, respectively; Special Algorithms: SAFE (Self-Attentive Factorized Embeddings):  uses a small self-attention layer to reduce embedding parameter count, Sandwich-style Parameter Sharing: shares the middle or central layers, leaving input and output layers to have independent sets of parameters; Initialization: Weight initialization scheme as BERT, sampling weights from N (0, 0.02), initializing biases to zero; Other: Early stopping based on lowest loss on development set, Beam search with beam size of 5 for generation, Tuning length penalty alpha in validation set",,,,,,,,,,
24,Subformer (96M),"Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo",2021/01/01,2021,Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers,12.0,,https://arxiv.org/abs/2101.00234,9.60E+07,,,70.29,,,0.0,103000000.0,4.17E+18,0.0,103000000.0,WikiText-103,20.39,,,0.0,1.0,0,0,Transformer,Subformer,https://github.com/machelreid/subformer,,,,1,,267000,"Architecture: Transformer-based model with Sandwich-style Parameter Sharing and Self-Attentive Factorized Embeddings (SAFE); Optimizer: Adam (β = (0.9, 0.999), ϵ = 10^-6 for machine translation; Nesterov's accelerated gradient optimizer for language modeling); LR Schedule: Inverse square root decay schedule with warmup (10K iterations for machine translation; Cosine annealing with warmup (16k iterations) for language modeling); Training: Label smoothing (ϵ = 0.1), Mixed precision training, Early stopping; Attention: Multi-head attention (details not provided); Regularization: Dropout (0.1, 0.2, or 0.3), Weight decay (0.01); Special Algorithms: Self-Attentive Factorized Embeddings (SAFE), Sandwich-style Parameter Sharing; Initialization: BERT weight initialization scheme (sampling weights from N(0, 0.02), initializing biases to zero and setting layer normalization parameters Band γ to be 0 and 1)",,,,,,,,,,
25,EGRU (PTB),"Anand Subramoney, Khaleelulla Khan Nazeer, Mark Schöne, Christian Mayr, David Kappel",2022/06/13,2022,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time,1.0,1.0,https://arxiv.org/pdf/2206.06178v3.pdf,5.50E+07,,,2500,,,0.0,929000.0,7.66E+17,,929000.0,,,,57.0,0.0,0.0,0,1,Recurrent,,https://github.com/Efficient-Scalable-Machine-Learning/EvNN,,,,1,,,"Architecture: Event-based Gated Recurrent Unit (EGRU), which extends the GRU with units that emit discrete events for communication triggered by a threshold. Three stacked EGRU cells without skip connections. The number of layers was also explored during experimentation.; Optimizer: Adam optimizer with default parameters (0.001 learning rate, β₁ = 0.9, β₂ = 0.999) is used for most experiments. Adam and NT-AvSGD optimization procedures were compared. Some models were trained using SGD. Adam was selected as an optimizer for performance reasons.; LR Schedule: Learning rate is scaled by 80% every 100 epochs. Cosine-annealing learning rate schedule was also used, where the first n/2 epochs were trained at constant learning rate and a cosine decay from A to 0.1 * A was applied for the remaining n/2 epochs.; Training: Surrogate gradients for backpropagating through the non-differential threshold function, Backpropagation through time (BPTT), Gradient clipping, Variable sequence length BPTT; Attention: Not applicable; Regularization: DropConnect applied to the hidden-to-hidden weights, Zoneout, Additional Loss to regularize the output and increase sparsity of the network - applied indirectly to the active outputs and on the auxiliary internal state ct); Special Algorithms: Dynamic activity sparsity mechanism - biological neuron dynamics that makes the communication between RNN units sparse and discrete, Event-based gradient descent using adjoint method, Model compression heuristic based on activity pruning - removing the least active units of the layer; Initialization: Xavier uniform distribution for weights, uniform distribution for biases. Unit thresholds were initialized using a normal distribution with mean 0 and standard deviation of √2, but was transformed to their absolute value after every update. Unit thresholds were initialized using a normal distribution with mean 0 and standard deviation of √2, but was transformed to be between 0 and 1 by passing through a standard sigmoid/logistic function after every update.; Other: Trainable Thresholds, Reparameterize thresholds with a sigmoid function to limit their domain to the interval [0, 1]",,,,,,,,,,
26,"GCRN-M1, dropout","Youngjoo Seo, Michaël Defferrard, Pierre Vandergheynst, Xavier Bresson",2016/12/22,2016,Structured Sequence Modeling with Graph Convolutional Recurrent Networks,674.0,,https://arxiv.org/pdf/1612.07659,4.20E+07,,,13,,,0.0,929000.0,3.04E+15,,929000.0,,,,98.67,0.0,1.0,0,0,Recurrent,GCRN,,,,,1,,10000,"Architecture: GCRN (Graph Convolutional Recurrent Network) combines CNN on graphs and RNN. RNN can be LSTM or GRU. Model 1 stacks a graph CNN for feature extraction and an LSTM for sequence learning (CNN -> LSTM). Model 2 replaces the Euclidean 2D convolution in convLSTM with a graph convolution.; Optimizer: RMSProp; LR Schedule: global learning rate of 1.0. Learning decay function is 0.5max(0,#epoch-4).; Training: back-propagation through time (BPTT), early stopping; Attention: Not applicable; Regularization: gradient clipping, dropout value is 0.75 for language modelling; Special Algorithms: graph CNN which uses Chebyshev polynomials to approximate convolution on graphs; Initialization: biases bi, bf and bo initialized to one such that the gates are initially open; Other: graph construction using k-nearest-neighbor (knn) graph with Euclidean distance and Gaussian kernel between pixel locations",,,,,,,,,,
27,DEQ-Transformer (Post-LN) + Jacobian Regularisation,"Shaojie Bai, Vladlen Koltun, J. Zico Kolter",2021/06/28,2021,Stabilizing Equilibrium Models by Jacobian Regularization,45.0,,https://arxiv.org/abs/2106.14342,9.80E+07,,2.9e+19,23,,,0.0,103000000.0,1.39E+18,0.0,103000000.0,WikiText-103,24.9,,,0.0,1.0,0,0,Transformer,DEQ,,,,,1,,267000,"Architecture: DEQ-Transformer (uses a multi-head self-attention layer as the underlying function); Optimizer: Adam; LR Schedule: cosine learning rate schedule; Training: Implicit Function Theorem, Layer Normalization, Weight Normalization, Group Normalization, Stochastic Application of Jacobian Regularization; Attention: multi-head self-attention; Regularization: Jacobian regularization (Frobenius norm), Weight Decay (however, the paper demonstrates that this method doesn't fix the DEQ stability problem, and sometimes may be counter-productive.); Special Algorithms: Deep Equilibrium Models (DEQs), Broyden's method, Anderson Mixing, Hutchinson Estimator to approximate Jacobian's Frobenius norm; Initialization: Parameters are initialized at the start of training by sampling from N(0,0.01); Other: Uses a fixed number of iterations (NFE limit) to solve for equilibrium",,,,,,,,,,
28,Sparse Wide GPT-3 Small,"Shreyas Saxena, Vithursan Thangarasa, Abhay Gupta, Sean Lie",2023/03/21,2023,Sparse Iso-FLOP Transformations for Maximizing Training Efficiency,0.0,0.0,https://arxiv.org/pdf/2303.11525.pdf,1.30E+09,,,110,,,0.0,103000000.0,8.84E+19,,103000000.0,,20.4,,,0.0,1.0,0,0,Transformer,GPT,https://github.com/CerebrasResearch/Sparse-IFT,,,,1,?,?,"Architecture: Transformer, drop-in replacements for dense layers in DNNs; Optimizer: AdamW; LR Schedule: Learning rate warmup over the first 375M tokens, followed by a cosine decay to 10% of the peak learning rate.; Training: Dynamic Sparse Training (DST) using SET, Gradient clipping at 1.0, Mixed precision training; Attention: Multi-head attention; Regularization: Weight decay 0.1; Special Algorithms: Sparse Iso-FLOP Transformations (Sparse-IFTs)",,,,,,,,,,
29,Relational Memory Core,"Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap",2018/06/05,2018,Relational recurrent neural networks,235.0,,https://arxiv.org/abs/1806.01822,UNK,,,,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-103,31.6,,,0.0,1.0,0,0,Recurrent,LSTM,,Carnegie Mellon University; Google Brain,Industry - Academia Collaboration,,0,,250000,"Architecture: Recurrent Memory Core (RMC) module integrated into an LSTM. RMC uses multi-head dot product attention to allow memories to interact.  The LSTM update is modified, and a gating mechanism is introduced.; Optimizer: Adam; LR Schedule: Tuned between le-5 and 1e-3 for Nth Farthest, le-3 for Program Evaluation, 0.001 and gradients clipped to have a maximum L2 norm of 0.1 for Language Modeling.; Training: backpropagation-through-time truncated, Gating (unit or memory), Layer normalization (in the function); Attention: Multi-head dot product attention with h heads, where h is tuned; Regularization: Dropout (0.5 for Language Modeling), L2 gradient clipping (0.1 for Language Modeling); Special Algorithms: Multi-Head Dot Product Attention (MHDPA) to allow memories to interact; Initialization: Random initialization of memory M; Other: Self-Attention, Tied word embedding matrix parameters to the output softmax for Language Modeling, Scaled dot-product attention",,,,,,,,,,
30,SPALM + RelationLM,"Qi Liu, Dani Yogatama, Phil Blunsom",2022/01/24,2022,Relational Memory-Augmented Language Models,21.0,,https://arxiv.org/pdf/2201.09680,1.24E+08,,,,,,0.0,103000000.0,0.00E+00,,103000000.0,WikiText-103,18.6,,,0.0,0.0,0,0,Transformer,Transformer-XL,,Tianjin University; Microsoft Research; Beijing Institute of Technology,Industry - Academia Collaboration,,1,,267735,"Architecture: Transformer-XL augmented with relational memory. The relational memory stores relation triples. The Transformer-XL component uses multiple self-attention layers.; Optimizer: Adam; LR Schedule: Cosine annealing with 4,000 warmup steps; Training: Gradient backpropagation is stopped for hidden states of extended context in transformer-XL, Dropout; Attention: Scaled dot-product attention is used to attend over the encoded contents of the relational memory; Regularization: Dropout with rate 0.25; Special Algorithms: Dynamic OpenIE is used to extract relations from previous seen text segments of the evaluation set., Gating mechanism to combine the transformer-XL representation and the relational memory; Other: OpenIE is used to obtain relation triples., tf-idf is used to score entities in the observed context for relation retrieval., LSTM is used to encode relation triples",,,,,,,,,,
31,Shortformer,"Ofir Press, Noah A. Smith, Mike Lewis",2020/12/31,2020,Shortformer: Better Language Modeling using Shorter Inputs,43.0,,https://arxiv.org/abs/2012.15832,2.40E+07,,,205,,,0.0,103000000.0,3.04E+18,0.0,103000000.0,WikiText-103,18.15,,,0.0,1.0,0,0,Transformer,Shortformer,https://github.com/ofirpress/shortformer,,,,1,BERT,29000,"Architecture: Transformer with 16 layers, dimension 1,024, 8 heads in each self-attention sublayer, and feedforward sublayers with an inner dimension of 4,096; Optimizer: Not explicitly mentioned but assumed standard transformer optimizers were explored.; LR Schedule: Not explicitly mentioned but standard transformer LR schedules assumed.; Training: Causal Masking; Attention: Multi-head self-attention with 8 heads per layer; Special Algorithms: Staged Training, Position-Infused Attention (PIA); Initialization: Sinusoidal position embeddings, tying word embedding and softmax matrices; Other: Caching of previous outputs for PIA",,,,,,,,,,
32,Word-Independent-SRNN+KN5,"Youssef Oualil, Clayton Greenberg, Mittul Singh, Dietrich Klakow",2017/03/23,2017,Sequential Recurrent Neural Networks for Language Modeling,7.0,,https://arxiv.org/pdf/1703.08068,5.32E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,94.0,0.0,1.0,0,0,Recurrent,RNN,,,,,1,,10000,"Architecture: Sequential Recurrent Neural Network (SRNN): Combines feedforward neural network (FNN) explicit word history with recurrent context information. Hidden layers combine encoding of local context and a recurrent architecture at the projection layer.  Word representations are enhanced using context information. WI-SRNN uses a word-independent recurrence vector; WD-SRNN uses word-dependent vectors.; Optimizer: stochastic gradient descent with momentum; LR Schedule: Learning rate is initialized to 0.4 and halved when no significant improvement is observed in the log-likelihood of the validation data, followed by seven more epochs where the learning rate is halved after each epoch.; Training: Back-Propagation Through Time (BPTT), mini-batch training; Regularization: weight decay; Initialization: normalized initialization; Other: The backward path (red arrows) shows the error propagation during training (this figure does not include BPTT).",,,,,,,,,,
33,Characterizing Verbatim Short-Term Memory in Neural Language Models (108M),"Kristijan Armeni, Christopher Honey, Tal Linzen",2022/10/24,2022,Characterizing Verbatim Short-Term Memory in Neural Language Models,3.0,1.0,https://arxiv.org/pdf/2210.13569.pdf,1.08E+08,,,,,,0.0,103000000.0,0.00E+00,,103000000.0,,40.3,,,0.0,0.0,0,0,,,https://github.com/KristijanArmeni/verbatim-memory-in-NLMs,NT Communication Science Laboratories; Tohoku University,Industry - Academia Collaboration,,1,,,"Architecture: AWD-LSTM with 3 hidden layers, 400-dimensional input embeddings, and 1840-dimensional hidden states was used. The transformer architecture was a 12-layer GPT-2 architecture, and smaller 1-, 3-, and 6-layer transformers were also trained. Embedding dimension was 768 across the transformer architectures.; Optimizer: Not explicitly stated for LSTM. Likely standard optimization algorithm used in AWD-LSTM (see Merity et al., 2018). Not explicitly stated for transformer models.; LR Schedule: Not explicitly stated, but the model was trained until convergence and training was stopped (early stopping) when the loss did not decrease for at least 0.01 bits in 5 consecutive evaluations.; Training: Adaptive Weight Dropping (AWD); Attention: The 12-layer GPT-2 architecture with 12 attention layers and 768-dimensional embedding layer was used as the base transformer model. Smaller transformer models (1,3,6 layers) were trained with 3,3,6 attention heads respectively.; Regularization: Weight dropping (LSTM), Dropout (LSTM); Initialization: Random initialization (for the randomly initialized transformer model)",,,,,,,,,,
34,Selfish-RNN (AWD-LSTM-MoS),"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",2021/01/22,2021,Selfish Sparse RNN Training,31.0,,https://arxiv.org/pdf/2101.09048,1.56E+07,,,1000,,,0.0,2080000.0,1.95E+17,,2080000.0,,,63.05,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/Shiweiliuiiiiiii/Selfish-RNN,,,,1,?,?,"Architecture: RNNs with various models, including stacked LSTMs, RHNs, ON-LSTM, and AWD-LSTM-MoS. The only difference is the number of cell gates.; Optimizer: SNT-ASGD (Sparse Non-monotonically Triggered Averaged Stochastic Gradient Descent). A sparse variant of the non-monotonically triggered averaged stochastic gradient descent optimizer (NT-ASGD); LR Schedule: Decay the pruning rate p to zero with a cosine annealing. The constant learning rate of SNT-ASGD helps to prevent the negative effect of learning rate decay on dynamic sparse training.; Training: Dynamic sparse connectivity; Attention: N/A; Regularization: Cell gate redistribution, Variational Dropout, embedding dropout, hidden layer dropout, output dropout; Special Algorithms: Selfish-RNN, Magnitude Weight Removal, Random Weight Growth; Initialization: Randomly initialized with a uniform sparse distribution in which the sparsity level of each layer is the same S; Other: Use dynamic sparse connectivity and SNT-ASGD together to handle this combinatorial optimization problem.",,,,,,,,,,
35,Selfish-RNN (ON-LSTM),"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",2021/01/22,2021,Selfish Sparse RNN Training,31.0,,https://arxiv.org/pdf/2101.09048,1.13E+07,,,1000,,,0.0,929000.0,6.30E+16,,929000.0,,,,55.82,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/Shiweiliuiiiiiii/Selfish-RNN,,,,1,?,?,"Architecture: Sparse LSTM, RHN, ON-LSTM, AWD-LSTM-MoS. Allows RNN layers to have non-uniform redistribution across cell gates.; Optimizer: SNT-ASGD (Sparse Non-Monotonically Triggered Averaged Stochastic Gradient Descent) and Adam; LR Schedule: Constant learning rate when using SNT-ASGD, learning rate decay for other methods. Cosine annealing for pruning rate.; Training: Sparse Training, Dynamic sparse connectivity (Magnitude weight removal, random weight growth, and cell gate redistribution), Weight initialization of new weights as zero, Enforce sparse structure before forward and after backward pass; Attention: Not applicable; Regularization: Non-uniform redistribution across cell gates, Variational Dropout, embedding dropout, hidden layer dropout, output dropout; Special Algorithms: SNT-ASGD (sparse variant of the non-monotonically triggered averaged stochastic gradient descent optimizer), Cell Gate Redistribution; Initialization: Uniform Sparse Initialization (randomly initialized with a uniform sparse distribution); Other: Magnitude Weight Removal, Random Weight Growth",,,,,,,,,,
36,Selfish-RNN (SNT-ASGD) Stacked LSTMs,"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",2021/01/22,2021,Selfish Sparse RNN Training,31.0,,https://arxiv.org/pdf/2101.09048,2.52E+07,,,100,,,0.0,929000.0,1.40E+16,,929000.0,,,,71.42,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/Shiweiliuiiiiiii/Selfish-RNN,,,,1,?,?,"Architecture: Stacked LSTMs, RHNs, ON-LSTM, AWD-LSTM-MoS. The layers have a non-uniform redistribution across cell gates during training.; Optimizer: SNT-ASGD (Sparse Non-monotonically Triggered Averaged Stochastic Gradient Descent). Also uses Adam and Momentum SGD for comparison.; LR Schedule: Constant learning rate. Learning rate decay via cosine annealing for the pruning rate.; Training: Dynamic sparse connectivity, Weight tying (for some models); Attention: Not applicable; Regularization: Variational Dropout, Weight Tying, DropConnect; Special Algorithms: SNT-ASGD: Sparse variant of NT-ASGD to address the issue of non-active weights remaining zero for sparse training., Dynamic Sparse Connectivity: Uniform sparse initialization, magnitude weight removal, random weight growth, and cell gate redistribution.; Initialization: Uniform sparse initialization: Randomly initialized with a uniform sparse distribution where the sparsity level of each layer is the same.; Other: Magnitude Weight Removal: Fraction p of weights with the smallest magnitude are removed after each training epoch, Random Weight Growth: After weight removal, the same number of weights are randomly grown to keep parameter count fixed., Cell Gate Redistribution: redistribute weights to cell gates dependently, so gates with larger-magnitude weights receive more parameters., Decay the pruning rate to zero with cosine annealing to enforce the sparse structure., Enforce the sparse structure before the forward pass and after the backward pass by setting all zero weights to zero and initializing all newly activated weights to zero.",,,,,,,,,,
37,Selfish-RNN (SNT-ASGD)RHNs,"Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy",2021/01/22,2021,Selfish Sparse RNN Training,31.0,,https://arxiv.org/pdf/2101.09048,7.60E+06,,,500,,,0.0,929000.0,2.12E+16,,929000.0,,,,64.03,0.0,1.0,0,0,Recurrent,RHN,https://github.com/Shiweiliuiiiiiii/Selfish-RNN,,,,1,?,?,"Architecture: Sparse RNNs, including stacked LSTMs, RHNs, and ON-LSTM. Allows for non-uniform redistribution across cell gates in RNN layers.; Optimizer: SNT-ASGD (Sparse Non-monotonically Triggered Averaged Stochastic Gradient Descent); LR Schedule: Constant learning rate for SNT-ASGD, cosine annealing for pruning rate decay to zero.; Training: Dynamic sparse connectivity, Weight redistribution across cell gates (RNN layers), Magnitude weight removal (non-RNN layers), Random weight growth (non-RNN layers), Decay the pruning rate; Attention: Not applicable; Regularization: Non-uniform redistribution across cell gates for regularization, Variational Dropout (from referenced papers), Weight Tying (from referenced papers), DropConnect (from referenced papers); Special Algorithms: SNT-ASGD: A sparse variant of non-monotonically triggered averaged stochastic gradient descent; Initialization: Uniform sparse initialization: randomly initialized with a uniform sparse distribution where the sparsity level of each layer is the same.; Other: Dynamic Sparse Connectivity: uniform sparse initialization, magnitude weight removal, random weight growth, and cell gate redistribution together.",,,,,,,,,,
38,"Segatron XL base, M=384","He Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, Ming Li",2020/04/30,2020,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,13.0,,https://arxiv.org/abs/2004.14996,1.51E+08,,,18.64,,,0.0,103000000.0,1.74E+18,,103000000.0,WikiText-103,22.5,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/rsvp-ai/segatron_aaai,,,,1,BERT,29000,"Architecture: Transformer-XL base model, Transformer (BERT-base/large), Segatron, relative position encoding, memory extension; Optimizer: Adam; LR Schedule: learning rate warm-up over the first 1% of the total steps and with linear decay of the learning rate; Attention: Multi-head self-attention; Special Algorithms: Segment-aware Transformer (Segatron); Other: Masked Language Modeling (MLM)",,,,,,,,,,
39,"Segatron XL large, M=384","He Bai, Peng Shi, Jimmy Lin, Yuqing Xie, Luchen Tan, Kun Xiong, Wen Gao, Ming Li",2020/04/30,2020,Segatron: Segment-Aware Transformer for Language Modeling and Understanding,13.0,,https://arxiv.org/abs/2004.14996,2.57E+08,,,167.02,,,0.0,103000000.0,2.65E+19,,103000000.0,WikiText-103,17.1,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/rsvp-ai/segatron_aaai,,,,1,BERT,29000,"Architecture: Transformer-XL based with Segment-Aware Transformer (Segatron), BERT based with Segment-Aware Transformer (Segatron) - SegaBERT; Optimizer: Adam (β₁=0.9, β₂=0.999); LR Schedule: Learning rate warm-up over the first 1% of the total steps and with linear decay of the learning rate.; Attention: Multi-head self-attention (base model: 12 heads, large model: 24 heads); Special Algorithms: Segment-aware Transformer (Segatron) replaces original token position encoding with a combined position encoding of paragraph, sentence, and token.",,,,,,,,,,
40,Gopher (7.1B),"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",2021/12/08,2021,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",441.0,,https://arxiv.org/abs/2112.11446,7.10E+09,,6.31e+23,1.00,,,0.0,300000000000.0,1.28E+22,,300000000000.0,WikiText-103,10.81,,,1.0,1.0,0,0,Transformer,GPT,,Carnegie Mellon University; Intel Labs,Industry - Academia Collaboration,,1,Own,32000,"Architecture: Autoregressive Transformer architecture; Optimizer: Adam (Kingma and Ba, 2014); LR Schedule: Warm-up the learning rate from 10-7 to the maximum learning rate over the first 1500 steps, and then decay it 10x using a cosine schedule; Training: Mixed precision training using bfloat16 numerical format, Gradient clipping based on the global gradient norm using a clipping value of 1 (reduced to 0.25 for 7.1B model and Gopher for improved stability); Attention: Relative positional encoding scheme; Initialization: Not mentioned explicitly; Other: RMSNorm instead of LayerNorm, Open-vocabulary tokenization via a mixture of byte-pair encoding (BPE) with a backoff to UTF-8 bytes",,,,,,,,,,
41,Gopher (280B),"Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, Geoffrey Irving",2021/12/08,2021,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher",441.0,,https://arxiv.org/abs/2112.11446,2.80E+11,,1.28e+22,1.00,,,0.0,300000000000.0,5.04E+23,,300000000000.0,WikiText-103,8.12,,,1.0,1.0,0,0,Transformer,GPT,,Facebook AI Research,Industry,,1,Own,32000,"Architecture: Autoregressive Transformer architecture with two modifications: RMSNorm instead of LayerNorm and relative positional encoding scheme.; Optimizer: Adam; LR Schedule: Warm-up the learning rate from 10^-7 to the maximum learning rate over the first 1500 steps, and then decay it 10x using a cosine schedule.; Training: Gradient clipping, bfloat16 numerical format, Mixed precision training; Attention: Multi-head attention",,,,,,,,,,
42,QRNN,"Stephen Merity, Nitish Shirish Keskar, James Bradbury, Richard Socher",2018/02/01,2018,Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours,4.0,0.0,https://mlsys.org/Conferences/doc/2018/50.pdf,1.35E+08,,3.6e+17,14.00,,,0.0,103000000.0,1.17E+18,0.0,103000000.0,WikiText-103,33.0,,,0.0,1.0,0,0,Recurrent/Convolutional,QRNN,,,,,1,Word-level,267735,,,,,,,,,,,
43,EGRU (WT2),"Anand Subramoney, Khaleelulla Khan Nazeer, Mark Schöne, Christian Mayr, David Kappel",2022/06/13,2022,Efficient recurrent architectures through activity sparsity and sparse back-propagation through time,1.0,1.0,https://arxiv.org/pdf/2206.06178v3.pdf,7.40E+07,,,2500,,,0.0,2080000.0,2.31E+18,,2080000.0,,,68.9,,0.0,0.0,0,1,Recurrent,,https://github.com/Efficient-Scalable-Machine-Learning/EvNN,,,,1,,,"Architecture: Event-based Gated Recurrent Unit (EGRU) - extends GRU with units that emit discrete events for communication triggered by a threshold. EGRU consists of three stacked EGRU cells without skip connections.; Optimizer: Adam optimizer with default parameters (0.001 learning rate, β₁ = 0.9, β2 = 0.999) for DVS128 gesture task. For language modelling Adam and NT-AvSGD; LR Schedule: Learning rate is scaled by 80% every 100 epochs for DVS128 gesture task. Cosine annealing learning rate schedule. where the first n/2 epochs were trained at constant learning rate λ, and a cosine decay from λ to 0.1 * λ was applied for the remaining n/2 epochs.; Training: BPTT with variable sequence length, gradient clipping, surrogate gradients for backpropagating through the non-differential threshold function, data augmentation (random crop, translation, and rotation); Attention: None; Regularization: DropConnect (applied to the hidden-to-hidden weights) - hidden to hidden dropout ph of 0.4 for CNN+EGRU(256) and 0.08 for CNN+EGRU(795), Zoneout (applied for CNN), weight decay, activity regularisation, temporal activity regularisation; Special Algorithms: Dynamic activity sparsity mechanism based on thresholding and event generation, Event-based gradient descent using adjoint method; Initialization: Weights were initialized using Xavier uniform distribution, and biases were initialized using a uniform distribution. Unit thresholds were initialized using a normal distribution and then transformed to be between 0 and 1 by passing through a sigmoid function after every update.; Other: The authors define a surrogate gradient as a piecewise linear function that is nonzero for internal states between the threshold plus/minus epsilon to decide whether or not to emit an event., Tied weights of the final softmax layer to the embedding layer (for language modeling)",,,,,,,,,,
44,TRIMELMext (7M),"Zexuan Zhong, Tao Lei, Danqi Chen",2022/05/25,2022,Training Language Models with Memory Augmentation,35.0,,https://arxiv.org/abs/2205.12674,7.00E+06,,,47.72,,,0.0,103000000.0,2.06E+17,103000000.0,206000000.0,WikiText-103,42.36,,,0.0,0.0,0,0,Transformer,Transformer-XL,https://github.com/princeton-nlp/TRIME,Tsinghua University; Beijing Academy of Artificial Intelligence; MIT CSAIL; Shanghai Qi Zhi Institute,Academia,,1,,,"Architecture: Transformer; Optimizer: Adam; LR Schedule: Cosine and Inverse Sqrt; Training: Training with In-batch Memories, Back-propagation to update memory representations, Linear interpolation to output probability distribution; Attention: Self-attention; Special Algorithms: TRIME (Training with In-batch Memories), BM25 (Best Matching 25) for packing lexically similar segments; Other: Contrastive learning for training objective, Packing consecutive segments from the same document in a training batch, Considering Local, Long-term, and External memories",,,,,,,,,,
45,AFP+FPI (PTB),"Zhengxiong Wang, Anton Ragni",2021/06/04,2021,Approximate Fixed-Points in Recurrent Neural Networks,1.0,0.0,https://arxiv.org/pdf/2106.02417,2.04E+06,,,20,,,0.0,929000.0,2.27E+14,,929000.0,Penn TreeBank,,,129.4,0.0,0.0,0,0,Recurrent,AFP,,,,,1,,,"Architecture: Elman network; Optimizer: Adam; LR Schedule: Not specified; Attention: Not applicable; Special Algorithms: Approximate Fixed-Points (AFP), Fixed-Point Iteration (FPI); Initialization: History states initialized to zeros, other parameters initialized using uniform distribution U(-0.05, 0.05)",,,,,,,,,,
46,rTop-k(distributed setting),"Leighton Pate Barnes, Huseyin A. Inan, Berivan Isik, Ayfer Ozgur",2020/05/21,2020,rTop-k: A Statistical Estimation Approach to Distributed SGD,41.0,,https://arxiv.org/pdf/2005.10761,6.90E+07,,,38,,,0.0,929000.0,1.46E+16,,929000.0,,,,82.49,0.0,1.0,0,0,Recurrent,RNN,,"Tianjin University, Tianjin, China; Beijing Institute of Technology, Beijing, China",Academia,,1,Word-level?,?,"Architecture: 2-layer LSTM language model architecture with 1500 hidden units per layer; Optimizer: Vanilla SGD; LR Schedule: learning rate schedule (i.e. weight initialization, learning rate schedule, batch size) as in [49].; Training: error compensation, gradient clipping, warm-up strategy, local gradient accumulation strategy; Special Algorithms: rTop-k: new gradient sparsification scheme that concatenates random-k and top-k sparsification; Initialization: weight initialization as in [49]",,,,,,,,,,
47,LTM,"Anupiya Nugaliyadde, Kok Wai Wong, Ferdous Sohel, Hong Xie",2019/04/18,2019,Language Modeling through Long Term Memory Network,19.0,,https://arxiv.org/pdf/1904.08936,UNK,,,,,,0.0,929000.0,#VALUE!,,929000.0,,,,83.0,0.0,1.0,0,0,Recurrent,LTM,,"KU Leuven, Leuven, Belgium; Apple",Industry - Academia Collaboration,,0,,,"Architecture: Long Term Memory network (LTM) with four gates. The first three gates impact the inputs, and the last gate controls and generalises the cell state. The LTM cell state does not reset itself like LSTMs.; Optimizer: Not explicitly mentioned; LR Schedule: Not explicitly mentioned; Attention: Not applicable; Initialization: Not explicitly mentioned; Other: Scaling the cell state with sigmoid functions to prevent exploding and vanishing gradients., Emphasis on current input by passing Lt1 and Lt2 through a dot operation to create Lt.",,,,,,,,,,
48,Transformer-C,"Simeng Sun, Mohit Iyyer",2021/04/08,2021,Revisiting Simple Neural Probabilistic Language Models,10.0,,https://arxiv.org/abs/2104.03474,1.48E+08,,,19.88,,,0.0,103000000.0,1.82E+18,0.0,103000000.0,WikiText-103,25.1,,,0.0,1.0,0,0,Transformer,Transformer,https://github.com/SimengSun/revisit-nplm,,,,1,Word-level,267735,"Architecture: The paper introduces two variations of the Transformer: Transformer-N, where the first self-attention layer is replaced with an NPLM-style local concatenation layer, and Transformer-C, where the first self-attention layer has a constrained window size of 5 tokens.; Optimizer: Adam with β₁ = 0.9 and β₂ = 0.999; LR Schedule: Linearly warmed up learning rate for 4K steps and then anneal with one cycle cosine learning rate scheduler. A constant learning rate is also mentioned for some NPLM experiments.; Training: Dropout, Residual Connections, Layer Normalization; Attention: Transformer-C uses local attention in the first layer, with a window size of 5. The original Transformer model uses standard self-attention.; Regularization: Dropout; Initialization: Not mentioned in provided text.; Other: Adaptive Softmax is used for datasets with large vocabularies., Tied token embeddings with weights in the softmax layer for some experiments., Learned 1-D convolution kernel applied to prefix embeddings beyond the local context window",,,,,,,,,,
49,AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (WT2),"Stephen Merity, Nitish Shirish Keskar, Richard Socher",2017/08/07,2017,Regularizing and Optimizing LSTM Language Models,1176.0,,https://arxiv.org/abs/1708.02182,3.30E+07,,,750,,,0.0,2080000.0,3.09E+17,,2080000.0,WikiText-2,,52.0,,0.0,1.0,1,0,Recurrent,LSTM,https://github.com/salesforce/awd-lstm-lm,,,,1,,30000,"Architecture: 3-layer LSTM (tied); Optimizer: Averaged SGD (ASGD) variant called NT-ASGD; LR Schedule: Constant learning rate with non-monotonic averaging trigger; Attention: Not applicable; Regularization: Weight-dropped LSTM using DropConnect on hidden-to-hidden weights, Embedding dropout, Activation Regularization (AR), Temporal Activation Regularization (TAR), Weight tying; Special Algorithms: Non-monotonically Triggered ASGD (NT-ASGD), Variable length backpropagation sequences; Initialization: Embedding weights initialized uniformly in [-0.1, 0.1], other weights initialized between [-√(1/H), √(1/H)] where H is hidden size",,,,,,,,,,
50,AWD-LSTM - 3-layer LSTM (tied) + continuous cache pointer (PTB),"Stephen Merity, Nitish Shirish Keskar, Richard Socher",2017/08/07,2017,Regularizing and Optimizing LSTM Language Models,1176.0,,https://arxiv.org/abs/1708.02182,2.40E+07,,,500,,,0.0,929000.0,6.69E+16,,929000.0,Penn TreeBank,,,52.8,0.0,1.0,1,0,Recurrent,LSTM,https://github.com/salesforce/awd-lstm-lm,,,,1,,10000,"Architecture: 3-layer LSTM (tied); Optimizer: Averaged SGD (ASGD) with a non-monotonically triggered averaging. Uses SGD for iterations, returning average of iterates past a dynamically determined threshold T.; LR Schedule: Constant learning rate; Training: Randomized-length backpropagation through time (BPTT), Gradient clipping with maximum norm 0.25; Attention: Not applicable; Regularization: Weight-dropped LSTM (DropConnect on hidden-to-hidden weights), Embedding dropout, Activation Regularization (AR), Temporal Activation Regularization (TAR), Weight Tying (between embedding and softmax layer); Special Algorithms: Non-monotonically Triggered ASGD (NT-ASGD); Initialization: Embedding weights initialized uniformly in [-0.1, 0.1], other weights initialized based on hidden size H.; Other: Variable length backpropagation sequences, Independent embedding size and hidden size",,,,,,,,,,
51,GLM-130B,"Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, Jie Tang",2022/10/05,2022,GLM-130B: An Open Bilingual Pre-trained Model,131.0,,https://openreview.net/forum?id=-Aw0rrrPUF,1.30E+11,,,1,,,1.0,632000000000.0,4.93E+23,,632000000000.0,,10.76,10.55,18.9,1.0,0.0,0,0,Transformer,GLM,https://github.com/THUDM/GLM-130B,,,,1,,,,,,,,,,,,,
52,Large regularized LSTM,"Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals",2014/09/08,2014,Recurrent Neural Network Regularization,3224.0,,https://arxiv.org/abs/1409.2329,6.60E+07,,9.1e+16,55,,,0.0,929000.0,2.02E+16,,929000.0,Penn TreeBank,,,78.4,0.0,1.0,0,0,Recurrent,LSTM,"https://github.com/wojzaremba/lstm., ",,,,1,,10000,"Architecture: 2-layer LSTM; Optimizer: Not specified; LR Schedule: Constant learning rate with decay. The learning rate is decreased by a factor after a certain number of epochs.; Attention: Not applicable; Regularization: Dropout applied to non-recurrent connections (input and output connections); Initialization: Weights initialized uniformly within a specific range (-0.05 to 0.05 for the medium LSTM, -0.04 to 0.04 for the large LSTM, and -0.1 to 0.1 for the non-regularized LSTM); Other: Gradient clipping (norm of gradients normalized by minibatch size)",,,,,,,,,,
53,Transformer-XL + RMT,"Aydar Bulatov, Yuri Kuratov, Mikhail S. Burtsev",2022/07/14,2022,Recurrent Memory Transformer,19.0,,https://web.archive.org/web/20220715153256/https://arxiv.org/pdf/2207.06881.pdf,2.47E+08,2.06E+08,,UNK,,,0.0,103000000.0,#VALUE!,103000000.0,206000000.0,,23.99,,,0.0,1.0,0,0,Recurrent/Transformer,Transformer-XL,"https://github.com/booydar/transformer-xl, https://github.com/kimiyoung/transformer-xl, https://github.com/GokuMohandas/fast-weights/blob/539fb10e3c384d5f782af2560bf28631cd0eaa61/, https://github.com/kimiyoung/transformer-xl, ",,,,1,Word-level,267735,"Architecture: Memory-augmented segment-level recurrent Transformer (RMT). Transformer with special memory tokens added to the input or output sequence. Memory passing from previous to current segments.; Optimizer: Adam; LR Schedule: Linear schedule learning rate starting from 0.00025; Training: Backpropagation Through Time (BPTT), Gradient Checkpointing (mentioned as a possible technique to alleviate GPU RAM issues); Attention: Standard self-attention with a causal attention mask applied only to tokens of the input sequence; Initialization: Not explicitly mentioned",,,,,,,,,,
54,Variational RHN + WT,"Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, Jürgen Schmidhuber",2016/07/12,2016,Recurrent Highway Networks,493.0,,https://arxiv.org/abs/1607.03474,2.30E+07,,,20,,,0.0,929000.0,2.56E+15,,929000.0,Penn TreeBank,,,65.4,0.0,1.0,0,0,Recurrent,RHN,https://github.com/julian121266/RecurrentHighwayNetworks,,,,1,Word-level?,10000,"Architecture: Recurrent Highway Networks (RHNs). RHN layer consists of one or multiple Highway layers in the recurrent state transition. L represents the recurrence depth. Each highway layer uses transforms H, T, and C (transform and carry gates); Optimizer: Stochastic Gradient Descent with momentum; LR Schedule: Learning rate decay starting at a particular epoch (Penn Treebank, Enwik8). Also used decay based on validation loss improving or not.; Training: Truncated Backpropagation Through Time (TBPTT); Attention: Not applicable; Regularization: Variational Dropout (Gal, 2015), Weight decay; Initialization: Gaussian distribution for weights, Uniform Distribution or small range such as [-0.04, 0.04], bias of -4 was set for the transform gate",,,,,,,,,,
55,VD-RHN,"Julian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutník, Jürgen Schmidhuber",2016/07/12,2016,Recurrent Highway Networks,493.0,,https://arxiv.org/abs/1607.03474,3.20E+07,,,20,,,0.0,929000.0,3.57E+15,,929000.0,Penn TreeBank,,,68.5,0.0,1.0,0,0,Recurrent,RHN,https://github.com/julian121266/RecurrentHighwayNetworks,,,,1,Word-level?,10000,"Architecture: Recurrent Highway Networks (RHN) - L stacked Highway layers inside the recurrent transition; Optimizer: SGD with momentum (momentum 0.9); LR Schedule: Initial learning rate of 0.2, learning rate decay of 1.02 starting at 20 epochs; Training: Truncated backpropagation, variational dropout; Attention: N/A; Regularization: Weight decay (varies between experiments), Gal dropout (variational dropout), weight-tying of input and output mappings (optionally); Initialization: Weights initialized from a uniform distribution between [-0.04, 0.04]. Initial bias of -4 for the transform gate.",,,,,,,,,,
56,S + I-Attention (3),"Artyom Gadetsky, Ilya Yakubovskiy, Dmitry Vetrov",2018/06/26,2018,Conditional Generators of Words Definitions,56.0,,https://arxiv.org/abs/1806.10090,UNK,,,35.00,,,0.0,103000000.0,#VALUE!,1080000.0,104000000.0,Oxford Dictionary,43.54,,,1.0,1.0,0,0,Recurrent,LSTM,https://github.com/sbos/AdaGram.jl,University of California San Diego,Academia,,0,,,"Architecture: LSTM networks with an embedding layer; the attention-based model has its own embedding layer, mapping context words to vector representations. S + I-Attention uses a feed forward neural network as an attention mechanism.; Optimizer: Adam; LR Schedule: annealing learning rate by a factor of 10 if validation loss doesn't decrease per epochs; Attention: Attention mechanisms used in two ways. First, adaptive skip-gram vector representations. Second, an attention neural network as a feed-forward neural network which uses a soft binary mask dependent on word context to extract components of word embedding relevant to corresponding meaning.; Initialization: Embeddings for language models were initialized by Google Word2Vec vectors and were fine-tuned.",,,,,,,,,,
57,HSO,"Davis Yoshida, Kevin Gimpel",2021/12/16,2021,Reconsidering the Past: Optimizing Hidden States in Language Models,1.0,1.0,https://web.archive.org/web/20230220145200/https://arxiv.org/pdf/2112.08653.pdf,3.45E+08,4.10E+09,3.45e+20,UNK,,,0.0,4000000000.0,#VALUE!,103000000.0,4100000000.0,,20.3,,,0.0,1.0,0,0,Transformer,GPT,,,,,1,GPT2Tokenizer,50257,"Architecture: Transformer-XL and GPT-2; Optimizer: Adam; LR Schedule: Not explicitly mentioned but a fixed learning rate is used in experiments.; Attention: Attention mechanisms are implicitly used as part of the Transformer architecture, specifically Transformer-XL's attention and GPT-2's attention. For GPT-2, backpropagation is only performed into the key and value vectors of attention.; Special Algorithms: Hidden-State Optimization (HSO); Initialization: Transformer-XL is initialized from the HuggingFace Transformers model trained on WikiText-103. GPT-2 models are initialized from the OpenAI checkpoints.; Other: Window size k for processing tokens., Adam's beta1 and beta2 parameters.",,,,,,,,,,
58,Decay RNN,"Gantavya Bhatt, Hritik Bansal, Rishubh Singh, Sumeet Agarwal",2020/05/17,2020,How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?,7.0,,https://arxiv.org/abs/2005.08199,1.40E+06,,,,,,0.0,,0.00E+00,,0.0,Lizen et al 2016,76.67,,,1.0,0.0,0,0,Recurrent,,https://github.com/bhattg/Decay-RNN-ACL-SRW2020,Stanford University; Salesforce Research,Industry - Academia Collaboration,,0,,,"Architecture: Decay RNN (DRNN), a recurrent neural network with a decay parameter 'a' to model the decaying nature of voltage in a neuron membrane. Uses ReLU activation and incorporates Dale's principle by setting the last 20% of entries in Wdale to -1, representing inhibitory connections.; Optimizer: Adam (learning rate 0.001); LR Schedule: Constant; Attention: Not applicable; Regularization: Dropout rate of 0.2 (for the language model experiment), Dale's principle is seen as a regularizer; Initialization: Decay parameter 'a' is initialized to 0.8; Other: Intrinsic skip connection derived out of its formulation, Coupled scalar gates",,,,,,,,,,
59,DiffStk-MRNN,"Ankur Mali, Alexander Ororbia, Daniel Kifer, Clyde Lee Giles",2020/04/04,2020,Recognizing Long Grammatical Sequences Using Recurrent Networks Augmented With An External Differentiable Stack,8.0,,https://arxiv.org/pdf/2004.07623,1.01E+06,,,50,,,1.0,929000.0,2.82E+14,,929000.0,,,,115.0,0.0,1.0,0,0,Recurrent,,,,,,1,?,?,"Architecture: RNN augmented with an external, differentiable stack. The RNN itself consists of an input layer, a hidden layer with recurrent connections, and an output layer. The models explored include DiffStk-RNN, DiffStk-LSTM, DiffStk-MRNN, DiffStk-MLSTM, and DiffStk-MIRNN.; Optimizer: Adam with initial learning rate 2e-3; LR Schedule: Patience schedule - if a gain was not observed in validation within the last three times it was checked, the learning rate was halved.; Training: Back-propagation through time (BPTT) with a look-back that extended 50 steps, Next-step prediction scheme which entails: 1) predicting the recognition label y of the string at each time step, measuring efficacy with a mean squared error (MSE) loss, and, 2) predicting the symbol xt given a history of symbols observed thus far x<t (using a cross-entropy loss as is characteristic of RNN language models)., Carry forward state update: Whenever there exists a symbol that leads to more than one No-OP operation on the stack, we propose carrying forward the previous hidden state.; Regularization: Gradients were hard-clipped to have a maximum magnitude of 15, Noise added to hidden weights:  ϵ[i] ~ N(μ, σ²); Other: Negative sampling - mixing with sampled negative cases some number of 'difficult' samples, which are similar/close to positive strings but would still be rejected by the target CFG., Sequential and Incremental training",,,,,,,,,,
60,Grown to Prune Two-layer stacked LSTM,"Xin Yuan, Pedro Savarese, Michael Maire",2020/07/30,2020,Growing Efficient Deep Networks by Structured Continuous Sparsification,37.0,,https://arxiv.org/pdf/2007.15353,,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,78.68,0.0,0.0,0,0,Recurrent,LSTM,,"Tsinghua University; Beijing National Research Center for Information Science and Technology; Beijing University of Posts and Telecommunications; Huawei Noah’s Ark Lab, Hong Kong, China",Industry - Academia Collaboration,,0,,,"Architecture: 2-layer stacked LSTM; Optimizer: SGD with momentum 0.9; LR Schedule: Initial learning rate is 0.1, learning rate is divided by 10 at epochs 80 and 120. A structure-wise temperature scheduler is used: β = β0 * γ ^ ts, where ts is a counter increased when the indicator variable is sampled as 1, and γ is a hyperparameter. ; Attention: N/A; Regularization: weight decay 10^-4, structured continuous sparsification, L0 regularization terms encourage sparsity; Special Algorithms: Growing with Structured Continuous Sparsification; Initialization: Mask weights such that a single filter is activated in each layer.; Other: Budget-Aware growing process that adapts regularization based on target sparsity., Sampling indicators qc,1 ~ Bern(σ(βsc,l)) and record the index idx where q value is 1, then Update ts[idx] = ts[idx] + 1, and Update β using Eq. (7).",,,,,,,,,,
61,Engin-Medium(NE),"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",2021/12/11,2021,Show and Write: Entity-aware Article Generation with Image Information,0.0,0.5,https://arxiv.org/pdf/2112.05917,3.55E+08,,,3,,,0.0,,0.00E+00,,0.0,,,15.4,,0.0,1.0,0,0,,,,,,,0,,,"Architecture: GPT2 architecture; Optimizer: Adam; LR Schedule: Linear warm-up, then constant learning rate; Attention: The architecture uses GPT2, which utilizes multi-head self-attention. Specific attention details not provided.; Special Algorithms: Named-entity extraction module, Entity-aware module; Initialization: Not mentioned; Other: Top-p sampling for decoding",,,,,,,,,,
62,GPT-2+Active-SGD,"Davood Wadi, Marc Fredette, Sylvain Senecal",2023/01/24,2023,Read the Signs Towards Invariance to Gradient Descent’s Hyperparameter Initialization,0.0,0.0,https://arxiv.org/pdf/2301.10133.pdf,1.24E+08,,,200,,,0.0,2080000.0,3.10E+17,,2080000.0,,,20.59,,0.0,1.0,0,0,Transformer,GPT,,,,,1,,50257,"Architecture: Transformer; Optimizer: SGD with momentum, AdamW, RAdam, AdaBelief, and ActiveLR meta-algorithm on top of these optimizers; LR Schedule: ActiveLR adjusts the learning rate at each epoch based on the change in the sign of the cumulative gradients for each parameter.; Attention: Multi-head attention is used. The number of attention heads is 12 for WikiText-2.; Regularization: dropout probability of 0.1 for WikiText-2, weight decay depends on learning rate for CIFAR-10; Special Algorithms: ActiveLR: Optimization meta algorithm that localizes the learning rate and adapts them at each epoch according to whether the gradient at each epoch changes sign or not.; Initialization: Not explicitly mentioned, but the paper investigates the invariance to gradient descent’s hyperparameter initialization, specifically the learning rate.; Other: Meta-learning rate adaptation (ActiveLR)",,,,,,,,,,
63,NAS+ESS (23M),"Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li",2020/05/06,2020,Learning Architectures from an Extended Search Space for Language Modeling,12.0,,https://arxiv.org/pdf/2005.02593,2.30E+07,,,30,,,0.0,103000000.0,4.26E+17,0.0,103000000.0,,,,45.6,0.0,0.0,0,0,Recurrent,RNN,,,Beihang University; Chongqing University; Mila,,1,,,Architecture: Learned recurrent cell structures with intra-cell and inter-cell connections.  The inter-cell connections learn how to connect the current cell with previous cells and input vectors. Intra-cell architecture is a single layer network of ht-1 and xt using the equation: e1 = tanh(ht-1.W(h) + xt.W(x)).; Optimizer: Gradient-based optimization via Differentiable Architecture Search (DARTS); LR Schedule: Learning rate was set as 3 × 10-3 for the intra-cell architecture and 1 × 10-3 for the inter-cell architecture.; Special Algorithms: Joint learning method to perform intra-cell and inter-cell NAS simultaneously; Other: DARTS represents networks as a directed acyclic graph (DAG) and search for the appropriate architecture on it.,,,,,,,,,,
64,RFA-GATE-Gaussian-Stateful Big,"Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, Lingpeng Kong",2021/03/03,2021,Random Feature Attention,200.0,,https://arxiv.org/abs/2103.02143,2.42E+08,,,47.72,,,0.0,103000000.0,7.14E+18,0.0,103000000.0,WikiText-103,23.5,,,0.0,1.0,0,0,Transformer,Transformer-XL,,,,,1,,268000,"Architecture: Transformer-XL with Random Feature Attention (RFA), can be used as drop-in replacement for conventional softmax attention; Optimizer: Adam; LR Schedule: Vaswani et al. (2017) learning rate schedule; Training: gradient clipping, early stopping; Attention: Random Feature Attention (RFA) using random feature methods to approximate the softmax function. The gated variant of RFA adds a learned gating mechanism, optionally learning with recency bias. RFA can be causal or cross attention, with a multi-headed variant.; Special Algorithms: RFA-GATE: learning with recency bias; Initialization: Weights are initialized from N(0, Id) and reparameterized to learn the variance during training.; Other: Kernel perspective of softmax, Uses Random Fourier Features to approximate softmax. Can use gaussian or arc-cosine kernels, Learned gating mechanism (RFA-GATE) multiplies a learned scalar gate against the hidden state.",,,,,,,,,,
65,Char-CNN-BiLSTM,"Chris Larson, Tarek Lahlou, Diana Mingels, Zachary Kulis, Erik Mueller",2019/06/13,2019,Telephonetic: Making Neural Language Models Robust to ASR and Semantic Noise,1.0,0.0,https://arxiv.org/pdf/1906.05678,UNK,,,,,,0.0,929000.0,#VALUE!,,929000.0,,,,37.49,0.0,1.0,0,0,Recurrent,LSTM,,iCoSys Institute; EPFL; Swisscom; University of Fribourg,Industry - Academia Collaboration,,0,,,"Architecture: Char-CNN-BiLSTM. Convolutional filters are used to convolve over character embeddings for each word in the input sequence. Two highway transformations are then applied prior to the application of two bi-directional LSTM layers.; Optimizer: Mini-batch stochastic gradient descent with Nesterov momentum; LR Schedule: Manual learning rate annealing; Training: Masked LM training procedure, probabilistic masking, bi-directional LSTM head, mini-batch stochastic gradient descent; Special Algorithms: Telephonetic data augmentation framework",,,,,,,,,,
66,R-Transformer,"Zhiwei Wang, Yao Ma, Zitao Liu, Jiliang Tang",2019/07/12,2019,R-Transformer: Recurrent Neural Network Enhanced Transformer,93.0,,https://arxiv.org/abs/1907.05572,1.58E+07,,,100,,,0.0,888000.0,8.40E+15,,888000.0,Penn TreeBank,,,84.38,0.0,1.0,0,0,Transformer,R-Transformer,https://github.com/DSE-MSU/R-transformer,,,,1,Word-level,10000,"Architecture: Multi-layer architecture built on RNNs and Transformer. Each layer has 3 components: Local RNN (LocalRNN), Multi-head attention networks and Position-wise feedforward networks. These networks are connected by a residual and layer normalization operation.; Optimizer: Same optimizer for all models during comparison, chosen according to validation performance; LR Schedule: Learning rate is chosen from the same set of values according to validation performance. Learning rate is annealed such that it is reduced when validation performance reaches plateau.; Training: Gradient clipping, Layer Normalization, Residual connection; Attention: Multi-head attention mechanism; Regularization: Dropout; Special Algorithms: LocalRNN: A local recurrent neural network to process signals within a local window ending at a given position. It operates on local windows of all the positions identically and independently and produces a latent representation for each of them.; Other: Position-wise feedforward networks which conduct non-linear feature transformation",,,,,,,,,,
67,Adaptive Input Transformer + RD,"Xiaobo Liang, Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu",2021/06/28,2021,R-Drop: Regularized Dropout for Neural Networks,245.0,,https://web.archive.org/web/20220518153557/https://arxiv.org/pdf/2106.14448.pdf,2.47E+08,1.03E+08,8.2e+19,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,,18.07,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/dropreg/R-Drop,,,,1,?,?,"Architecture: Transformer, Adaptive Input Transformer; Optimizer: Adam; LR Schedule: inverse_sqrt; Training: R-Drop regularization, KL-divergence loss, Dropout, Label Smoothing; Attention: Multi-head attention; Regularization: dropout, weight decay; Special Algorithms: R-Drop: Regularized Dropout for Neural Networks; Other: bidirectional Kullback-Leibler (KL) divergence between the two distributions, The final training objective combines a negative log-likelihood loss and a KL-divergence loss.",,,,,,,,,,
68,Pythia-12b,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023/04/03,2023,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.0,,https://arxiv.org/abs/2304.01373,1.20E+10,,,1,,,0.0,300000000000.0,2.16E+22,,300000000000.0,,,10.54,,0.5,1.0,0,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,,,,1,https://arxiv.org/abs/2204.06745,50257,"Architecture: Decoder-only Transformer architecture following Brown et al. (2020) with parallel attention and feedforward technique. It uses fully dense layers.; Optimizer: Adam with Zero Redundancy Optimizer (ZERO); LR Schedule: Cosine decay learning rate schedule which decays to a minimum of 0.1x the starting learning rate.; Training: Data parallelism, Tensor parallelism, Flash Attention; Attention: Fully dense attention layers, Flash Attention; Regularization: Hidden dropout (0), Weight decay (0.01); Initialization: Small-init; Other: Rotary embeddings, Untied embedding / unembedding matrices, Gradient clipping (1.0)",,,,,,,,,,
69,Pythia-6.9b,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023/04/03,2023,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.0,,https://arxiv.org/abs/2304.01373,6.90E+09,,,1,,,0.0,300000000000.0,1.24E+22,,300000000000.0,,,11.41,,0.5,1.0,0,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,,,,1,https://arxiv.org/abs/2204.06745,50257,"Architecture: Transformer model, follows Brown et al. (2020) with a few deviations, uses parallelized attention and feedforward, uses rotary embeddings, uses fully dense layers; Optimizer: Adam with Zero Redundancy Optimizer (ZERO); LR Schedule: Cosine decay to a minimum of 0.1x their maximum LR; Training: data parallelism, tensor parallelism, Flash Attention; Attention: Parallelized attention; Regularization: hidden-dropout 0, weight-decay 0.01; Initialization: Model initialization methods introduced by Wang & Komatsuzaki (2021), small-init; Other: untied embedding/unembedding matrices, gradient clipping 1.0, layer normalization, scaled-upper-triang-masked-softmax-fusion",,,,,,,,,,
70,Pythia-160m,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023/04/03,2023,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.0,,https://arxiv.org/abs/2304.01373,1.60E+08,,,1,,,0.0,300000000000.0,2.88E+20,,300000000000.0,,,33.43,,0.5,1.0,0,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,,,,1,https://arxiv.org/abs/2204.06745,50257,"Architecture: Transformer, following Brown et al. (2020) with some deviations. Parallelized attention and feedforward technique introduced by Wang & Komatsuzaki (2021) and adopted by Black et al. (2022); Chowdhery et al. (2022). Fully dense layers are used. Untied embedding/unembedding matrices.; Optimizer: Adam, leveraging Zero Redundancy Optimizer (ZERO) (Rajbhandari et al., 2020); LR Schedule: cosine decay to a minimum LR of 0.1 * optimizer.params.lr; Training: data parallelism (Goyal et al., 2017), tensor parallelism (Shoeybi et al., 2019), Flash Attention (Dao et al., 2022); Attention: Flash Attention (Dao et al., 2022); Regularization: hidden dropout of 0, weight decay of 0.01; Initialization: small-init, using model initialization methods introduced by Wang & Komatsuzaki (2021) and adopted by (Black et al., 2022; Chowdhery et al., 2022); Other: rotary embeddings (Su et al., 2021) used as positional embedding, gradient clipping with value of 1.0",,,,,,,,,,
71,Pythia-1b,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023/04/03,2023,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.0,,https://arxiv.org/abs/2304.01373,1.00E+09,,,1,,,0.0,300000000000.0,1.80E+21,,300000000000.0,,,16.45,,0.5,1.0,0,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,,,,1,https://arxiv.org/abs/2204.06746,50257,"Architecture: Transformer with fully dense layers (no sparse attention), parallelized attention and feedforward; Optimizer: Adam with Zero Redundancy Optimizer (ZERO); LR Schedule: Cosine decay to a minimum learning rate of 0.1 times the maximum learning rate.; Training: Data parallelism, Tensor parallelism, Flash Attention; Attention: Dense attention layers; Regularization: Hidden Dropout (0.0), Weight Decay (0.01); Special Algorithms: Rotary Embeddings; Initialization: small-init (Wang & Komatsuzaki, 2021); Other: Untied embedding/unembedding matrices",,,,,,,,,,
72,Pythia-1.4b,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023/04/03,2023,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.0,,https://arxiv.org/abs/2304.01373,1.40E+09,,,1,,,0.0,300000000000.0,2.52E+21,,300000000000.0,,,14.72,,0.5,1.0,0,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,,,,1,https://arxiv.org/abs/2204.06747,50257,"Architecture: Transformer, following Brown et al. (2020) with a few notable deviations based on recent advances in best practices for large scale language modeling. Used parallel attention and feedforward approach. Rotary embeddings introduced by Su et al. (2021) are used as the positional embedding type. Untied embedding/unembedding matrices.; Optimizer: Adam with Zero Redundancy Optimizer (ZERO); LR Schedule: Cosine decay; Training: data parallelism, tensor parallelism, Flash Attention; Attention: Fully dense layers; Regularization: hidden dropout, weight decay; Initialization: Model initialization methods introduced by Wang & Komatsuzaki (2021) and adopted by (Black et al., 2022; Chowdhery et al., 2022)",,,,,,,,,,
73,Pythia-410m,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023/04/03,2023,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.0,,https://arxiv.org/abs/2304.01373,4.10E+08,,,1,,,0.0,300000000000.0,7.38E+20,,300000000000.0,,,20.11,,0.5,1.0,0,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,,,,1,https://arxiv.org/abs/2204.06748,50257,"Architecture: Transformer with n layers, where n varies based on model size (6 to 36). Parallel attention and feedforward approach, dense layers for models. Untied embedding/unembedding matrices.; Optimizer: Adam with Zero Redundancy Optimizer (ZERO); LR Schedule: Cosine decay to a minimum of 0.1x their maximum LR; Training: Flash Attention, Data parallelism, Tensor parallelism, Gradient Clipping; Attention: Flash Attention; Regularization: Hidden dropout 0, Weight decay 0.01; Special Algorithms: Rotary Embeddings; Initialization: Small init, Wang init for output layer",,,,,,,,,,
74,Local Transformer,"Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier",2020/03/12,2020,Efficient Content-Based Sparse Attention with Routing Transformers,349.0,,https://arxiv.org/abs/2003.05997,2.89E+10,,,,,,0.0,103000000.0,0.00E+00,,103000000.0,WikiText-103,19.8,,,0.0,1.0,0,0,Transformer,Local transformer,https://github.com/google-research/google-research/tree/master/routing_transformer,,,,1,,,"Architecture: Routing Transformer, which combines clustering-based sparse attention with classical local attention. Key components include linear projections for queries, keys, and values, and a clustering module based on online k-means.; Optimizer: Adam (except for PG-19 where Adafactor was used); LR Schedule: Learning rate schedule described in Vaswani et al. (2017) (except for PG-19 where a linear warmup over 10,000 steps followed by a rsqrt_normalized_decay was used); Attention: Sparse attention using k-means clustering to route queries to keys belonging to the same cluster. Attention is scaled by 1/sqrt(d) after being performed.; Regularization: Attention dropout and ReLU dropout (rate 0.3), Weight decay (used on some datasets, not all), Layer Normalization; Special Algorithms: Sparse routing module based on online k-means; Initialization: Not explicitly mentioned, but weights are learned.; Other: Relative position encoding (Shaw et al., 2018), Sharing keys and queries for causal attention",,,,,,,,,,
75,TF-LM-discourse LSTM (PTB),"Lyan Verwimp, Hugo Van hamme, Patrick Wambacq",2018/05/01,2018,TF-LM: TensorFlow-based Language Modeling Toolkit,7.0,,https://aclanthology.org/L18-1470.pdf,UNK,,,39,,,0.0,929000.0,#VALUE!,,929000.0,,,,84.1,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/lverwimp/tf-lm,,,,0,,,,,,,,,,,,,
76,(ensemble): AWD-LSTM-DOC (fin) × 5 (WT2),"Sho Takase, Jun Suzuki, Masaaki Nagata",2018/08/30,2018,Direct Output Connection for a High-Rank Language Model,36.0,,https://arxiv.org/abs/1808.10143,1.85E+08,,,300,,,0.0,2080000.0,6.93E+17,,2080000.0,WikiText-2,,53.09,,0.0,0.0,0,0,Recurrent,LSTM,https://github.com/nttcslab-nlp/doc_lm,,,,1,,,"Architecture: AWD-LSTM consisting of three LSTM layers; Optimizer: Averaged stochastic gradient descent; LR Schedule: The paper mentions the non-monotone interval, implying the use of a non-monotone learning rate schedule, but the exact details are not specified.; Attention: Not applicable; Regularization: Weight Drop, Dropout, Coefficient of variation of Equation 10; Special Algorithms: Direct Output Connection (DOC); Initialization: Not mentioned; Other: Word Tying",,,,,,,,,,
77,Pythia-2.8b,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023/04/03,2023,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.0,,https://arxiv.org/abs/2304.01373,2.80E+09,,,1,,,0.0,300000000000.0,5.04E+21,,300000000000.0,,,12.69,,0.5,1.0,0,0,Transformer,Pythia,https://github.com/EleutherAI/pythia,,,,1,https://arxiv.org/abs/2204.06749,50257,"Architecture: Transformer, following Brown et al. (2020) with a few deviations. Uses fully dense layers. Employs parallelized attention and feedforward technique.; Optimizer: Adam with Zero Redundancy Optimizer (ZERO); LR Schedule: cosine decay to a minimum of 0.1x their maximum LR.; Training: data parallelism, tensor parallelism, Flash Attention, gradient clipping, gradient accumulation; Attention: Uses Flash Attention for improved throughput. Uses rotary embeddings.; Regularization: hidden dropout = 0.0, weight decay = 0.01; Initialization: Wang & Komatsuzaki (2021) model initialization methods, small-init for init method, untied embedding / unembedding matrices; Other: All models trained with a batch size of 1024 samples with sequence length of 2048. Training intermediate checkpoints made available.",,,,,,,,,,
78,RHN(depth=40),"Ron Shoham, Haim Permuter",2018/05/23,2018,Highway State Gating for Recurrent Highway Networks: improving information flow through time,0.0,1.0,https://arxiv.org/pdf/1805.09238,UNK,,,300,,,0.0,929000.0,#VALUE!,,929000.0,,,,63.6,0.0,1.0,0,0,Recurrent,RHN,,National Tsing Hua University; Google,Industry - Academia Collaboration,,0,,,"Architecture: Recurrent Highway Network (RHN) with Highway State Gating (HSG). The HSG cell generates a new state by a weighted combination of the previous state and the output of the RHN cell. Transition depth is varied (10, 20, 30, 40 layers).; Optimizer: Not explicitly mentioned in the text.; LR Schedule: Learning rate exponentially decreased at each epoch.; Attention: Not applicable; Regularization: Variational dropout, L2 weight decay; Special Algorithms: Highway State Gating (HSG); Initialization: Initial bias of -2.5 was used for both the RHN and the HSG gates.",,,,,,,,,,
79,2-layer skip-LSTM + dropout tuning (PTB),"Gábor Melis, Charles Blundell, Tomáš Kočiský, Karl Moritz Hermann, Chris Dyer, Phil Blunsom",2018/05/23,2018,Pushing the bounds of dropout,14.0,,https://arxiv.org/abs/1805.09208,5.40E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,Penn TreeBank,,,55.3,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level?,?,"Architecture: 2-layer skip-LSTM; Optimizer: Not explicitly mentioned in the provided text; LR Schedule: Not explicitly mentioned in the provided text; Training: Dropout, Variational Dropout; Attention: Not applicable; Regularization: Weight decay (in relation to the KL term approximation), Dropout; Special Algorithms: Extended power mean family, Power mean family of models; Initialization: Weights are initialized with a zero mean Gaussian prior; Other: Deterministic dropout for evaluation, MC Dropout (Geometric Mean and Arithmetic Mean), Exploration of geometric vs arithmetic mean predictions, Tuning softmax temperature",,,,,,,,,,
80,Temporal Convolutional Attention-based Network(TCAN) (PTB),"Hongyan Hao, Yan Wang, Yudi Xia, Jian Zhao, Furao Shen",2020/02/28,2020,Temporal Convolutional Attention-based Network For Sequence Modeling,33.0,,https://arxiv.org/pdf/2002.12530,1.30E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,26.92,0.0,0.0,0,0,,,https://github.com/haohy/TCAN,University of Washington,Industry,,1,,,"Architecture: Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism. TCAN includes two parts, Temporal Attention (TA) and Enhanced Residual (ER).  The encoder maps input sequence to continuous representations. Dilated causal convolution is applied across L layers, with kernel sizes used as hidden layers. Decoder generates output sequence.; Optimizer: Adam; LR Schedule: constant; Training: gradient clipping; Attention: Temporal Attention (TA) that integrates the influence of previous time steps into the current time step. Three linear transformations f, g, and h are used to map the input to keys, queries and values, and the lower triangular part of the weight matrix is extracted to shield the weight of future time steps.; Initialization: nan",,,,,,,,,,
81,2-layer skip-LSTM + dropout tuning (WT2),"Gábor Melis, Charles Blundell, Tomáš Kočiský, Karl Moritz Hermann, Chris Dyer, Phil Blunsom",2018/05/23,2018,Pushing the bounds of dropout,14.0,,https://arxiv.org/abs/1805.09208,5.40E+06,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,WikiText-2,,63.7,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level?,?,"Architecture: 2-layer skip-LSTM; Optimizer: Not explicitly stated, but SGD is implied as the training occurs by minimizing the expectation of the loss over randomly sampled dropout masks.; LR Schedule: Not explicitly mentioned, but it is likely tuned using Vizier during evaluation time.; Training: dropout, minimising the expectation of the loss over randomly sampled dropout masks; Attention: Not applicable; Regularization: weight decay (equivalent to the KL term), dropout; Special Algorithms: Power Mean Family of Models, Extended Power Mean Family of Models; Initialization: Weights are initialized with zero mean gaussian prior.; Other: Deterministic Dropout at Evaluation Time, Model selection at evaluation time by tuning dropout rate and averaging method, Softmax Temperature Tuning",,,,,,,,,,
82,GCNN-14,"Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier",2016/12/23,2016,Language Modeling with Gated Convolutional Networks,2176.0,,https://arxiv.org/abs/1612.08083,UNK,,,,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-103,63.7,,108.7,0.0,1.0,0,0,Recurrent,GCNN,,,,,0,,,"Architecture: Gated Convolutional Network (GCNN) with stacked convolutions and pre-activation residual blocks. The architecture involves a lookup table for word embeddings, convolutional layers with Gated Linear Units (GLU), and a softmax layer (or adaptive softmax) for prediction. Number of layers varies (GCNN-9, GCNN-14, etc.). The GLU gating mechanism is h₁(X) = (X * W + b) ⊗ σ(X * V + c). Blocks have a bottleneck structure.; Optimizer: Nesterov's momentum; LR Schedule: Learning rate sampled uniformly in the interval [1., 2.]; Training: Parallelization, Weight Normalization; Attention: Not applicable; Regularization: Gradient Clipping; Special Algorithms: Gated Linear Units (GLU); Initialization: Kaiming initialization; Other: Shifting convolutional inputs to prevent kernels from seeing future words, Adaptive Softmax",,,,,,,,,,
83,Tensor-Transformer(1core)+PN (PTB),"Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer",2020/03/17,2020,PowerNorm: Rethinking Batch Normalization in Transformers,60.0,,https://arxiv.org/pdf/2003.07845,1.20E+07,,,30,,,0.0,929000.0,2.01E+15,,929000.0,,,,47.6,0.0,1.0,0,0,Transformer,Tensorized Transformer,https://github.com/sIncerass/powernorm,,,,1,Word-level,10000,"Architecture: Transformer (Tensor-Transformer); Optimizer: Adam (inferred from the fairseq-py settings used for implementation); LR Schedule: Followed the learning rate schedule in (Wang et al., 2019) that is 2.0 times common post-normalization transformer (Vaswani et al., 2017), and warmup steps identical to learning rate schedule in the optimizer (from Appendix A); Training: pre-normalization, label smoothing (epsilon = 0.1), gradient accumulation (if mini-batch is small); Attention: multi-linear attention mechanism with masking for language modeling task; Regularization: dropout (0.3 for big model, 0.0 for small model), layer-scale layer; Special Algorithms: Power Normalization (PN), PN-V; Initialization: Not explicitly specified, but using existing LN hyperparameters; Other: synchronized mean/variance/quadratic mean of different batches at different nodes, alpha in the forward and backward steps are different and tuned over 0.9, 0.95, 0.99",,,,,,,,,,
84,Tensor-Transformer(1core)+PN (WT103),"Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, Kurt Keutzer",2020/03/17,2020,PowerNorm: Rethinking Batch Normalization in Transformers,60.0,,https://arxiv.org/pdf/2003.07845,8.53E+07,,,30,,,0.0,103000000.0,1.58E+18,,103000000.0,,17.9,,,0.0,1.0,0,0,Transformer,Tensorized Transformer,https://github.com/sIncerass/powernorm,,,,1,Word-level,268000,"Architecture: Transformer (specifically, Tensorized Transformer); Optimizer: Adam; LR Schedule: Learning rate schedule used is the same as Vaswani et al., 2017.; Training: Gradient accumulation, Label smoothing; Attention: Multi-linear attention mechanism with masking; Regularization: Dropout 0.3, Layer-scale (before the normalization layer); Special Algorithms: Power Normalization (PN), PN-V; Initialization: Not explicitly mentioned, but depth-scaled initialization is implied by referencing the base transformer architecture.; Other: Pre-normalization setting (normalization layer right before the multi-head attention module and point-wise feed-forward network module)., Synchronized mean/variance/quadratic mean for distributed training of PN-V and PN., The alpha in the forward and backward steps of PN is tuned separately.",,,,,,,,,,
85,Pointer Sentinel-LSTM (medium),"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2016/09/26,2016,Pointer Sentinel Mixture Models,1558.0,,https://arxiv.org/abs/1609.07843,2.10E+07,,,64,,,0.0,929000.0,7.49E+15,,929000.0,Penn TreeBank,,,70.9,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,10000,"Architecture: Pointer Sentinel-LSTM: LSTM with a pointer network and a sentinel value for gating the pointer and LSTM components. The pointer network uses attention to select a word from the past context, and the sentinel decides when to use the pointer or the standard LSTM softmax.; Optimizer: Not explicitly mentioned, but section 5.2 says that the learning rate is halved when validation perplexity worsens. The gradients are also rescaled if their global norm exceeds 1.; LR Schedule: Learning rate is halved when validation perplexity is worse than the previous iteration, and training stops when validation perplexity fails to improve for three epochs, or when 64 epochs are reached.; Training: Truncated Backpropagation Through Time (BPTT), Model averages outputs of the RNN from up to L timesteps back (L=100)., Regenerating historical outputs of RNN when updating gradients.; Attention: Pointer network uses attention to select a word from the past context. The attention score is computed using an inner product between a query vector (derived from the current hidden state) and the past hidden states.; Regularization: Dropout (variational inference based dropout and Zoneout), Gradient Clipping (global norm exceeds 1); Special Algorithms: Pointer Sentinel Mixture Model: Novel architecture that combines a pointer network and a standard softmax classifier via a sentinel gate.; Initialization: Not mentioned in the text.; Other: Gating function to control the mixture of pointer and softmax outputs. The gate is influenced by both the RNN hidden state and the pointer window's hidden states., Pointer component always looks L timesteps into the past if L past timesteps are available. k1=1, k2=L BPTT configuration.",,,,,,,,,,
86,Zoneout + Variational LSTM (WT2),"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2016/09/26,2016,Pointer Sentinel Mixture Models,1558.0,,https://arxiv.org/abs/1609.07843,2.10E+07,,,64,,,0.0,2080000.0,1.68E+16,,2080000.0,WikiText-2,,100.9,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,33278,"Architecture: Pointer Sentinel-LSTM: LSTM combined with a pointer network and a sentinel for gating between the LSTM output and the pointer network output. Specifically, the model contains a softmax-RNN component (LSTM) and a pointer network component. The query vector for the pointer network is computed as tanh(WhN-1 + b), where hN-1 is the last output of the RNN.  It is combined through a mixture model using a gating function.; Optimizer: Not explicitly stated, but the gradients are rescaled if their global norm exceeds 1.; LR Schedule: Learning rate is halved when validation perplexity is worse than the previous iteration. Training stops when validation perplexity fails to improve for three epochs or when 64 epochs are reached.; Training: Truncated Backpropagation Through Time (BPTT), Mixture Model training; Attention: Pointer sum attention mechanism for selecting words from the input context. The attention scores are computed using the inner product of a query vector (derived from the RNN hidden state) and the previous RNN output states.; Regularization: Dropout: Zoneout and Variational Inference based Dropout. Zoneout is used for recurrent connections within the LSTM. Variational Inference based Dropout is applied on the input to each RNN layer and on the output of the final RNN layer. A dropout value of 0.5 is used for both dropout connections.; Special Algorithms: Pointer Sentinel Mixture Model; Initialization: Not mentioned; Other: Gating function for dynamically selecting between the RNN's softmax output and the pointer network's output, integrated directly into the pointer computation",,,,,,,,,,
87,Pointer Sentinel-LSTM,"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2016/09/26,2016,Pointer Sentinel Mixture Models,1558.0,,https://arxiv.org/abs/1609.07843,2.00E+07,,,64,,,0.0,2080000.0,1.60E+16,,2080000.0,WikiText-2,,80.8,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,33278,"Architecture: 2-layer LSTM with Pointer Sentinel Mixture Model; Optimizer: Not specified in the paper explicitly; LR Schedule: Halve the learning rate when validation perplexity is worse than the previous iteration, stopping training when validation perplexity fails to improve for three epochs; Training: truncated backpropagation through time (BPTT), regenerating the window of RNN outputs used by the pointer component after each gradient update; Attention: Pointer network with attention to select element from the input. Attention sum mechanism where probability mass assigned to a word is the sum of the probability mass given to all token positions where the word appears; Regularization: gradient clipping with global norm of 1, zoneout (on recurrent connections), variational inference based dropout (on input and output of RNN layers), dropout probability of 0.5; Special Algorithms: Pointer Sentinel Mixture Model: Combines a standard softmax with a pointer network, using a gating function (sentinel) to decide when to use the softmax vocabulary or the pointer component., Modified softmax that includes a 'sentinel' to indicate when to use the pointer network., Pointer sum attention; Initialization: Not specified in the paper explicitly; Other: The pointer network matches against a window of the L most recent words., Used L = 100 for the window length",,,,,,,,,,
88,Deep RNN,"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2013/12/11,2013,Pointer Sentinel Mixture Models,1558.0,,https://arxiv.org/abs/1609.07843,6.00E+06,,,64,,,0.0,929000.0,2.14E+15,,929000.0,Penn TreeBank,,,107.5,0.0,1.0,0,0,Recurrent,,,,,,1,,10000,"Architecture: Pointer sentinel-LSTM model. LSTM is used as the RNN component. The pointer network uses a query mechanism produced from applying an MLP to the last output of the RNN to identify likely matching words from the past. The sentinel is used to indicate when to use the RNN vs. the pointer.; Optimizer: Not explicitly mentioned; LR Schedule: The learning rate is halved when validation perplexity is worse than the previous iteration. Training stops when validation perplexity fails to improve for three epochs or when 64 epochs are reached.; Training: Truncated backpropagation through time (BPTT); Attention: Pointer sum attention: The pointer network selects the member of the input sequence with the maximal attention score as output. Gating mechanism decides when to rely on the softmax classifier vs. the pointer component.; Regularization: Zoneout (used for the recurrent connections within the LSTM), Variational inference based dropout (dropout mask for a layer is locked across timesteps, used on the input to each RNN layer and also on the output of the final RNN layer), Gradient clipping (gradients are rescaled if their global norm exceeds 1); Special Algorithms: Pointer sentinel mixture model; Initialization: Not explicitly mentioned; Other: The pointer component always looks L timesteps into the past if L past timesteps are available. The code selects k₁ = 1 and k₂ = L such that for each timestep backpropagation is performed for L timesteps and advances one timestep at a time. Only the loss for the final predicted word is used for backpropagation through the window.",,,,,,,,,,
89,MGK 8 heads (small),"Tam Nguyen, Tan M. Nguyen, Dung D. Le, Duy Khuong Nguyen, Viet-Anh Tran, Richard G. Baraniuk, Nhat Ho, Stanley J. Osher",2021/10/16,2021,Improving Transformers with Probabilistic Attention Keys,12.0,,https://arxiv.org/pdf/2110.08678,4.00E+07,,,120,,,0.0,103000000.0,2.97E+18,0.0,103000000.0,,33.93,,,0.0,0.0,0,0,Transformer,Transformer,https://github.com/minhtannguyen/transformer-mgk,DeepMind; University of Oxford,,,1,,,"Architecture: Transformer with a Mixture of Gaussian Keys (Transformer-MGK). Replaces redundant heads in a transformer with a mixture of keys at each head. A variation called Transformer-MLK uses linear attention.; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: Not explicitly mentioned, but learning rate warm-up (2000 steps) is used for language modeling.; Attention: Multi-head attention.  Each head uses a mixture of Gaussian keys, each key is modelled as a mixture of M Gaussians.; Regularization: dropout 10%; Special Algorithms: Transformer with a Mixture of Gaussian Keys (Transformer-MGK), Transformer with a Mixture of Linear Keys (Transformer-MLK); Initialization: Keys in shifted versions are initialized from a standard normal distribution.; Other: Expectation-Maximization (EM) algorithm with Soft and Hard E-steps for inference and learning of Gaussian Mixture Model components.",,,,,,,,,,
90,Zoneout + Variational LSTM (PTB),"Stephen Merity, Caiming Xiong, James Bradbury, Richard Socher",2016/09/26,2016,Pointer Sentinel Mixture Models,1558.0,,https://arxiv.org/abs/1609.07843,2.10E+07,,,64,,,0.0,929000.0,7.49E+15,,929000.0,Penn TreeBank,,,80.6,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,10000,"Architecture: 2-layer LSTM with pointer sentinel mixture model; Optimizer: Not explicitly stated, but gradients are rescaled if their global norm exceeds 1, implying some gradient based optimization algorithm is being used.; LR Schedule: Learning rate is halved when validation perplexity is worse than the previous iteration. Training stops when validation perplexity fails to improve for three epochs or when 64 epochs are reached.; Training: truncated backpropagation through time (BPTT); Attention: Pointer network component using inner product between a query vector (derived from RNN hidden state) and previous RNN output states to compute attention scores. The final output is a mixture of the standard softmax classifier and this pointer mechanism, controlled by a sentinel gate.; Regularization: zoneout (for recurrent connections), variational inference based dropout (on input and output of each RNN layer), gradient clipping (global norm 1); Special Algorithms: pointer sentinel mixture model; Initialization: Not explicitly mentioned; Other: Mixture model combining a standard softmax classifier with a pointer network. The sentinel vector is used as a gate to determine when to use the softmax vocabulary or pointer mechanism.",,,,,,,,,,
91,GPT-2-Medium+Pixelfly,"Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher Ré",2021/11/30,2021,Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,29.0,,https://arxiv.org/pdf/2112.00029,2.03E+08,,,100,,,0.0,103000000.0,1.25E+19,0.0,103000000.0,,21.0,,,0.0,1.0,0,0,Transformer,GPT,https://github.com/HazyResearch/pixelfly,,,,1,GPT2Tokenizer,50257,"Architecture: Transformer; Optimizer: AdamW; LR Schedule: Not specified, but learning rates used are provided in table forms (e.g. 0.0005, 0.001); Training: Fixed sparsity pattern, Block butterfly, Low-rank matrices, Hardware block-oriented efficiency, Gradient Descent, ReLU networks; Attention: Attention is sparsified using flat block butterfly and low-rank matrices.; Regularization: Weight Decay, Drop Path; Special Algorithms: Pixelated Butterfly, Flat Block Butterfly, Compute budget allocation; Initialization: Weight matrices are initialized randomly; Other: Sparsity",,,,,,,,,,
92,GPT-2-Small+Pixelfly,"Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, Christopher Ré",2021/11/30,2021,Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models,29.0,,https://arxiv.org/pdf/2112.00029,6.80E+07,,,100,,,0.0,103000000.0,4.20E+18,0.0,103000000.0,,22.5,,,0.0,1.0,0,0,Transformer,GPT,https://github.com/HazyResearch/pixelfly,,,,1,GPT2Tokenizer,50257,"Architecture: Transformer, with variations in the implementation of butterfly + low-rank matrix sparsity; Optimizer: AdamW; LR Schedule: Unspecified decay, used constant learning rate; Attention: Sparsified with Pixelated Butterfly. Variations: BigBird, Longformer-style attention can also be expressed with this architecture; Regularization: weight decay, drop path (or 0, in some configurations); Special Algorithms: Pixelated Butterfly: Combines flat block butterfly and low-rank matrices, Allocation of Compute Budget for Network Layers based on matrix and hardware block size, Model sparsification: training with a fixed sparsity mask; Initialization: Unspecified normal initialization; Other: Sparsity pattern inspired by butterfly + low-rank matrices, Block butterfly matrices for hardware efficiency, Flat butterfly matrices for parallelization, NTK convergence and generalization analysis for sparse networks, The models are trained from scratch as usual",,,,,,,,,,
93,MGK 4 heads (medium),"Tam Nguyen, Tan M. Nguyen, Dung D. Le, Duy Khuong Nguyen, Viet-Anh Tran, Richard G. Baraniuk, Nhat Ho, Stanley J. Osher",2021/10/16,2021,Improving Transformers with Probabilistic Attention Keys,12.0,,https://arxiv.org/pdf/2110.08678,9.00E+07,,,120,,,0.0,103000000.0,6.67E+18,0.0,103000000.0,,28.86,,,0.0,0.0,0,0,Transformer,Transformer,https://github.com/minhtannguyen/transformer-mgk,Salesforce Resarch,Industry,,1,,,"Architecture: Transformer with a Mixture of Gaussian Keys (Transformer-MGK), where the attention key in each head is replaced by a GMM; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: Learning rate warm-up with Adam optimizer; Attention: Scaled dot-product attention is modified using Mixture of Gaussian Keys (MGK) to allow each attention head to focus on different parts of the input sequence. Also explores Mixture of Linear Keys (MLK) for efficient computation.; Regularization: fix the variance parameter of the keys to be sqrt(D) in standard softmax attention; Special Algorithms: Mixture of Gaussian Keys (MGK) Attention, Mixture of Linear Keys (MLK) Attention; Initialization: Initialized keys of MGK from a standard normal distribution. Priors initialized to 0.5.; Other: Hard E-step and Soft E-step inference algorithms for the Mixture of Gaussian Keys attention.",,,,,,,,,,
94,PermuteFormer,Peng Chen,2021/09/06,2021,PermuteFormer: Efficient Relative Position Encoding for Long Sequences,12.0,,https://arxiv.org/pdf/2109.02377,3.30E+07,,3.1e+18,30,,,0.0,103000000.0,6.12E+17,0.0,103000000.0,,32.49,,,0.0,1.0,0,0,Transformer,Performer,https://github.com/cpcp1998/PermuteFormer,,,,1,Word-level,268000,"Architecture: PermuteFormer, a Performer-based model with relative position encoding. It applies position-dependent transformation on queries and keys to encode positional information into the attention module. It scales linearly on long sequences.; Optimizer: Adam; LR Schedule: Learning rates are manually tuned on Transformer to match the results reported by other papers. Then, these hyper-parameters are fixed on training of Performer and PermuteFormer.; Attention: Multi-head attention. The position-aware permutation permutes elements of each token's query / key feature along the head size dimension in each attention head. Depending on the token's position, the permutation applied to query / key feature is different.; Special Algorithms: PermuteFormer applies a position-aware permutation on query features and key features to encode positional information.; Initialization: We randomly sample π at initialization of the neural network and fix its value during the whole training process.",,,,,,,,,,
95,Characterizing Verbatim Short-Term Memory in Neural Language Models (117M),"Kristijan Armeni, Christopher Honey, Tal Linzen",2022/10/24,2022,Characterizing Verbatim Short-Term Memory in Neural Language Models,3.0,1.0,https://arxiv.org/pdf/2210.13569.pdf,1.17E+08,,,,,,0.0,103000000.0,0.00E+00,,103000000.0,,37.5,,,0.0,0.0,0,0,,,https://github.com/KristijanArmeni/verbatim-memory-in-NLMs,NT Communication Science Laboratories; Tohoku University,Industry - Academia Collaboration,,1,,,"Architecture: The paper examines two architectures: Attention-based Transformers (specifically, retrained BPE tokenizer on Wikitext-103 with 12-layer GPT-2 architecture and smaller versions with 1-, 3-, and 6-layers) and Long Short-Term Memory Networks (AWD-LSTM); Optimizer: Not explicitly stated in this paper, but it is stated that parameters for AWD-LSTM training can be found in Table 4 which links to a github repository which contains the specific optimizer implementation used; LR Schedule: Not explicitly stated in this paper, but it is stated that parameters for AWD-LSTM training can be found in Table 4 which links to a github repository which contains the specific learning rate schedule implementation used; Training: adaptive weight-dropped (AWD) (LSTM), Early stopping; Attention: GPT-2 small with 12 attention layers and 768-dimensional embedding layer. Attention mechanism analyzed by randomly permuting key and query matrices.; Regularization: weight drop (LSTM); Initialization: random initialization of transformers",,,,,,,,,,
96,DOC + Finetune∗ + Partial Shuffle (WT2),Ofir Press,2019/03/11,2019,Partially Shuffling the Training Data to Improve Language Models,4.0,0.0,https://arxiv.org/abs/1903.04167,6.73E+07,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,WikiText-2,,57.85,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/ofirpress/PartialShuffle,,,,1,Word-level,33278,"Architecture: Recurrent Language Model using LSTM; Optimizer: Likely SGD (Stochastic Gradient Descent), as shuffling is mentioned with relation to SGD; LR Schedule: Not explicitly specified, but finetuning is mentioned.; Training: Back propagation through time (BPTT), Batching; Attention: Not applicable; Special Algorithms: Partial Shuffle; Initialization: Last hidden state from sub-sequence m becomes initial hidden state for subsequence m+1, otherwise default initial hidden state.",,,,,,,,,,
97,DOC + Finetune∗ + Partial Shuffle (PTB),Ofir Press,2019/03/11,2019,Partially Shuffling the Training Data to Improve Language Models,4.0,0.0,https://arxiv.org/abs/1903.04167,3.70E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,Penn TreeBank,,,52.0,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/ofirpress/PartialShuffle,"Johns Hopkins University, New York University",Academia,,1,Word-level,10000,"Architecture: Recurrent, based on LSTM (from context and references); Optimizer: Likely SGD (as mentioned in the abstract regarding shuffling requirements); LR Schedule: Not specified in detail; Attention: Not applicable; Special Algorithms: Partial Shuffle method; Initialization: Default initial hidden state is used when starting with a new sub-sequence/batch; Other: Back propagation through time (BPTT), Training sequence is split into sub-sequences of length b, Batching of sub-sequences",,,,,,,,,,
98,OPT-125M (finetuned on PTB),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,1.25E+08,,,1.666666667,,,0.0,180000000000.0,2.25E+20,929000.0,180000000000.0,,,,16.5,0.0,1.0,0,0,Transformer,OPT,,,,,1,GPT2Tokenizer,50257,"Architecture: Decoder-only Transformer ranging from 125M to 175B parameters. Number of layers, attention heads, and embedding size vary depending on the model size. Sequence length of 2048.; Optimizer: AdamW with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens.; Training: Fully Sharded Data Parallel, Megatron-LM Tensor Parallelism, Dynamic Loss Scaling, Gradient Predivide Factor; Attention: Details on attention mechanisms are not provided other than the number of attention heads.; Regularization: Weight decay 0.1, Dropout 0.1 (no dropout on embeddings); Initialization: Weights are initialized using a normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0.; Other: ReLU activation, Gradient clipping at 1.0 (later reduced to 0.3 in some instances), Adam state kept in FP32, model weights in FP16",,,,,,,,,,
99,LSTM+GraB,"Yucheng Lu, Wentao Guo, Christopher De Sa",2022/05/22,2022,GraB: Finding Provably Better Data Permutations than Random Reshuffling,7.0,,https://arxiv.org/pdf/2205.10733,UNK,,,50,,,0.0,2080000.0,#VALUE!,,2080000.0,,,199.4,,0.0,0.0,0,0,Recurrent,LSTM,https://github.com/EugeneLYC/GraB,,,,0,,,"Architecture: 2-layer LSTM; Optimizer: momentum SGD (with its default value 0.9); LR Schedule: ReduceLROnPlateau from Pytorch (mode='min', factor=0.1, patience=5, threshold=5). Initial learning rate is set to be 5.; Training: Gradient Accumulation; Regularization: Weight Decay; Special Algorithms: Gradient Balancing algorithm (GraB); Other: Offline Stale-Gradient Herding, Online Vector Balancing",,,,,,,,,,
100,OPT-66B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,6.60E+10,,,1.666666667,,,1.0,180000000000.0,1.19E+23,0.0,180000000000.0,,,9.34,,1.0,1.0,0,0,Transformer,OPT,,,,,1,GPT2Tokenizer,50257,"Architecture: Decoder-only Transformer with varying number of layers, attention heads, and embedding sizes depending on the model size (125M to 175B parameters).; Optimizer: AdamW with (β1, β2) = (0.9, 0.95) and weight decay of 0.1; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B (or 375M tokens in smaller baselines), and decaying down to 10% of the maximum LR over 300B tokens.; Training: Gradient norm clipping at 1.0 (or 0.3 in some cases), Dynamic loss scaling, Gradient predivide factor; Attention: Multi-head attention (details in Table 1, where #H indicates number of attention heads); Regularization: Dropout of 0.1 (except for embeddings), Weight decay 0.1; Initialization: Weights initialized using a normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0.; Other: ReLU activation, Sequence length of 2048",,,,,,,,,,
101,OPT-2.7B (finetuned on WT2),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,2.70E+09,,,1.666666667,,,0.0,180000000000.0,4.86E+21,2080000.0,180000000000.0,,,10.27,,0.0,1.0,0,0,Transformer,OPT,,,,,1,GPT2Tokenizer,50257,"Architecture: Decoder-only pre-trained Transformer language models, ranging from 125M to 175B parameters. The architecture follows Brown et al. (2020); Optimizer: AdamW optimizer with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens. A number of mid-flight changes to LR were also required.; Training: Dropout of 0.1 is used throughout except for embeddings, Gradient norms are clipped at 1.0, except for some mid-flight changes that reduce this threshold down from 1.0 to 0.3, A gradient predivide factor is included to reduce the risk of over/underflows when computing the gradient across all ranks; Attention: Multi-head attention.  Table 1 provides specifics on attention heads per model size.; Regularization: Weight decay of 0.1, Dropout of 0.1, Gradient clipping to 1.0 or 0.3; Initialization: Weight initialization settings from the Megatron-LM codebase are followed, using a normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0.; Other: ReLU activation, Sequence length of 2048",,,,,,,,,,
102,LSTM-MemoryAug (WT2),"Ke Li, Daniel Povey, Sanjeev Khudanpur",2020/09/29,2020,Neural Language Modeling With Implicit Cache Pointers,4.0,1.0,https://arxiv.org/pdf/2009.13774,2.85E+07,,,,,,1.0,2080000.0,0.00E+00,,2080000.0,,,74.3,,0.0,0.0,0,0,Recurrent,LSTM,,,,,1,,,"Architecture: RNNLM with extended output layer of size L and memory-augmentation unit. Incorporates cache-inspired pointer mechanism. Can also be applied to Transformer-based LMs, using the context vector from the last Transformer block as the hidden state.; Optimizer: SGD (for RNNLMs and also explicitly mentioned for Transformer LMs since they tried Adam but SGD performed better); LR Schedule: Not explicitly mentioned; Training: truncated back-propagation through time (BPTT); Attention: Implicitly uses cache pointer mechanism, but no explicit attention mechanism like multi-head attention; Regularization: dropout (0.5 for PTB/WikiText-2, 0.1 for SWBD+Fisher), weight decay (implied since SGD is used, but value not explicitly mentioned); Special Algorithms: Implicit Cache Pointers, Memory Augmentation; Initialization: Not explicitly mentioned; Other: tied embedding and output matrices, state-carry for N-best rescoring (copying initial hidden state from best hypothesis of previous utterance), Frequency-agnostic word embeddings (Frage)",,,,,,,,,,
103,T2R 75% + Pretrain,"Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",2021/03/24,2021,Finetuning Pretrained Transformers into RNNs,30.0,,https://arxiv.org/abs/2103.13076,4.50E+08,,1.93e+19,34.47,,,1.0,103000000.0,9.59E+18,103000000.0,206000000.0,WikiText-103,18.5,,,0.0,0.0,0,0,Transformer,ELU,https://github.com/jungokasai/T2R/,Facebook AI Research; Google Brain,Industry,,1,,,"Architecture: Transformer with multi-head attention, feedforward, and layer normalization modules. Softmax attention is replaced with dot product after single-layer MLP feature mapping.  The model is then finetuned.; Optimizer: Adam (for machine translation); LR Schedule: Cosine learning rate schedule with cycles (for language modeling), linearly warmed up. Inverse square (for machine translation); Training: Teacher forcing, Layer Normalization, Mixed Precision Training, Distributed Training over 8 GPUs; Attention: Multi-head attention, converted to recurrent computation. Attention patterns are learned by finetuning.; Regularization: Dropout (0.2 language modeling, 0.3 MT), Weight Decay (0.01 machine translation), Label smoothing (0.1 machine translation); Special Algorithms: Transformer-to-RNN (T2R): a method to convert a pretrained transformer to an efficient RNN.; Initialization: Pretrained transformer or random initialization; Other: Gradient clipping (0.1)",,,,,,,,,,
104,RHN+HSG(depth=40),"Ron Shoham, Haim Permuter",2018/05/23,2018,Highway State Gating for Recurrent Highway Networks: improving information flow through time,0.0,1.0,https://arxiv.org/pdf/1805.09238,UNK,,,300,,,0.0,929000.0,#VALUE!,,929000.0,,,,61.7,0.0,1.0,0,0,Recurrent,RHN,,Eindhoven University of Technologyl; University of Twente,Academia,,0,,,"Architecture: Recurrent Highway Network (RHN) cell with Highway State Gating (HSG). The HSG cell is a weighted combination of the previous state and the output of the RHN cell.; Optimizer: Not explicitly mentioned in the paper.; LR Schedule: Learning rate exponentially decreased at each epoch.; Attention: Not applicable; Regularization: Variational dropout, L2 weight decay; Special Algorithms: Highway State Gating (HSG); Initialization: Initial bias of -2.5 was used for both the RHN and HSG gates.",,,,,,,,,,
105,OPT-6.7B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,6.70E+09,,,1.666666667,,,0.0,180000000000.0,1.21E+22,0.0,180000000000.0,,,10.86,,1.0,1.0,0,0,Transformer,OPT,,,,,1,GPT2Tokenizer,50257,"Architecture: Decoder-only Transformer ranging from 125M to 175B parameters.; Optimizer: AdamW with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1.; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens. Mid-flight changes to LR were also required.; Training: Gradient clipping, Dynamic loss scaling, ReLU activation; Attention: Multi-head attention (number of heads varies by model size, see Table 1 in the paper); Regularization: Dropout of 0.1 (except for embeddings), Weight decay of 0.1; Initialization: Weights initialized using a normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0.; Other: Gradient predivide factor to reduce the risk of over/underflows when computing the gradient across all ranks (splitting the division by the world size of N into two division operations by √N)",,,,,,,,,,
106,OPT-2.7B (finetuned on PTB),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,2.70E+09,,,1.666666667,,,0.0,180000000000.0,4.86E+21,929000.0,180000000000.0,,,,10.8,0.0,1.0,0,0,Transformer,OPT,,,,,1,GPT-2 byte level BPE tokenizer,52000,"Architecture: Decoder-only Transformer with details of the number of layers, attention heads, and embedding size specified in Table 1.; Optimizer: AdamW optimizer with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1.; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens. The batch sizes are constant throughout the course of training.; Training: Dropout of 0.1 (except embeddings), Gradient clipping at 1.0 (or 0.3), Gradient predivide factor to reduce risk of over/underflows, Dynamic loss scaling; Attention: Multi-head attention (details of the number of heads specified in Table 1); Regularization: Weight decay 0.1, Dropout of 0.1 (except embeddings); Initialization: Weight initialization using a normal distribution with zero mean and standard deviation of 0.006, as in Megatron-LM. Output layers are scaled by 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0.; Other: ReLU activation, Sequence length of 2048 tokens",,,,,,,,,,
107,OPT-1.3B (finetuned),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,1.30E+09,,,1.666666667,,,0.0,180000000000.0,2.34E+21,2080000.0,180000000000.0,,,12.22,,0.0,1.0,0,0,Transformer,OPT,,,,,1,GPT-2 tokenizer,52000,"Architecture: Decoder-only Transformer language models, Table 1 provides number of layers, number of attention heads, and embedding size for each model size.; Optimizer: AdamW with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens.; Training: Dropout of 0.1 (except on embeddings), Gradient clipping at 1.0 (reduced to 0.3 mid-flight), Gradient predivide factor to reduce over/underflows, Dynamic loss scaling; Attention: Not explicitly mentioned, but the model is a Transformer, so it uses multi-headed self-attention.; Regularization: Weight decay 0.1, Dropout 0.1; Initialization: Weight initialization follows Megatron-LM, using a normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0; Other: ReLU activation, Sequence length of 2048",,,,,,,,,,
108,OPT-2.7B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,2.70E+09,,,1.666666667,,,1.0,180000000000.0,4.86E+21,0.0,180000000000.0,,,12.47,,1.0,1.0,0,0,Transformer,OPT,,,,,1,GPT-2 tokenizer,52000,"Architecture: Decoder-only Transformer architecture with varying number of layers, attention heads and embedding sizes. The specific details for each model size are listed in Table 1 of the paper.; Optimizer: AdamW with (β1, β2) = (0.9, 0.95) and weight decay of 0.1; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps for OPT-175B (or over 375M tokens for smaller baselines), and decaying down to 10% of the maximum LR over 300B tokens. Mid-flight changes to LR were also required.; Training: Gradient clipping at 1.0 (or reduced to 0.3 during mid-flight adjustments), Gradient predivide factor to reduce risk of over/underflows, Dynamic loss scaling to avoid underflows, Fully Sharded Data Parallel (FSDP), Megatron-LM Tensor Parallelism; Attention: Details on the specific attention mechanism are not explicitly stated, but the number of attention heads is specified for each model size in Table 1.; Regularization: Dropout of 0.1 (except for embeddings), Weight Decay 0.1; Initialization: Weights initialized using a normal distribution with zero mean and standard deviation of 0.006 (Megatron-LM codebase settings). Output layers are scaled by a 1.0/√2L term. All bias terms are initialized as 0.; Other: ReLU activation, Sequence length of 2048",,,,,,,,,,
109,OPT-175B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,1.75E+11,,4.16e+23,1.666666667,,,0.0,180000000000.0,3.15E+23,0.0,180000000000.0,,,8.35,,1.0,1.0,0,0,Transformer,OPT,,,,,1,GPT-2 tokenizer,52000,"Architecture: Decoder-only Transformer with details such as number of layers (#L), number of attention heads (#H), and the embedding size (dmodel) varying across different model sizes. Architectural details are displayed in Table 1.; Optimizer: AdamW optimizer with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1.; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens.; Training: Gradient clipping (norm at 1.0, reduced to 0.3 in some mid-flight changes), Dynamic loss scaling, Fully Sharded Data Parallel, Megatron-LM Tensor Parallelism, ReLU activation; Attention: Multi-head attention (number of heads #H specified in Table 1); Regularization: Weight decay of 0.1, Dropout of 0.1 throughout (except for embeddings); Initialization: Weights initialized using a normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0.; Other: Gradient predivide factor to reduce the risk of over/underflows when computing the gradient across all ranks",,,,,,,,,,
110,OPT-13B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,1.30E+10,,,1.666666667,,,1.0,180000000000.0,2.34E+22,0.0,180000000000.0,,,10.13,,1.0,1.0,0,0,Transformer,OPT,,,,,1,GPT-2 tokenizer,52000,"Architecture: Decoder-only Transformer, with #L layers, #H attention heads and dmodel embedding size.  See table 1 in the paper for details for each specific model.; Optimizer: AdamW with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1.; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens.; Training: Gradient Clipping, Dynamic Loss Scaling, Gradient Predivide Factor; Attention: Multi-head attention (see table 1 for the number of heads #H); Regularization: Dropout of 0.1 (except for embeddings), Weight Decay of 0.1; Initialization: Weights initialized with a normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0.; Other: ReLU activation, Sequence length of 2048",,,,,,,,,,
111,OPT-125M (finetuned),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,1.25E+08,,,1.666666667,,,0.0,180000000000.0,2.25E+20,2080000.0,180000000000.0,,,19.85,,0.0,1.0,0,0,Transformer,OPT,,,,,1,GPT-2 tokenizer,52000,"Architecture: Decoder-only pre-trained Transformer language model.; Optimizer: AdamW with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in our smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens. (Mid-flight changes to learning rate also occurred.); Training: gradient clipping (norm clipped to 1.0, sometimes reduced to 0.3 mid-flight), dynamic loss scaling; Attention: Multi-head attention used, number of heads varies by model size; Regularization: dropout of 0.1 (except for embeddings), weight decay of 0.1; Initialization: Weights initialized using a normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0.; Other: ReLU activation, Sequence length of 2048, gradient predivide factor",,,,,,,,,,
112,Alleviated TOI 10 (PTB),"Noémien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros Lazaridis, Andreas Fischer, Claudiu Musat",2019/09/18,2019,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,0.0,1.0,https://arxiv.org/abs/1909.08700,UNK,,,1000,,,1.0,103000000.0,#VALUE!,0.0,103000000.0,Penn TreeBank,,,56.46,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/nkcr/overlap-ml,,,,0,,,"Architecture: LSTM, AWD-LSTM, Transformer; Optimizer: SGD; LR Schedule: Not mentioned; Training: Weight Dropping for AWD-LSTM; Attention: Self-attention used in Transformer; Regularization: Not explicitly mentioned; Special Algorithms: Maximum Over Softmax (MoS), Alleviated TOI (Token Order Imbalance), Prime Batch Sizes; Initialization: Not mentioned",,,,,,,,,,
113,OPT-1.3B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,1.30E+09,,,1.666666667,,,0.0,180000000000.0,2.34E+21,,180000000000.0,,,16.41,,1.0,1.0,0,0,Transformer,OPT,,,,,1,GPT-2 tokenizer,52000,"Architecture: Decoder-only Transformer with varying number of layers, attention heads and embedding size; Optimizer: AdamW with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in our smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens.; Training: Gradient clipping, Dynamic loss scaling, ReLU activation; Attention: Multi-head attention (details vary according to model size); Regularization: Dropout 0.1 (not applied to embeddings), Weight decay 0.1; Special Algorithms: Gradient predivide factor to reduce the risk of over/underflows; Initialization: Weights initialized using a normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0.; Other: Sequence Length 2048",,,,,,,,,,
114,OPT-30B,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,3.00E+10,,,1.666666667,,,0.0,180000000000.0,5.40E+22,0.0,180000000000.0,,,10.67,,1.0,1.0,0,0,Transformer,OPT,,,,,1,GPT-2 tokenizer,52000,"Architecture: Decoder-only pre-trained Transformer Language Models; Optimizer: AdamW with (β1, β2) set to (0.9, 0.95), weight decay of 0.1; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens.; Training: gradient clipping, dynamic loss scaling; Attention: Not explicitly stated, but implied multi-head attention due to the Transformer architecture and mentioning the number of attention heads.; Regularization: dropout 0.1 (except for embeddings), weight decay 0.1; Initialization: Normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0.; Other: gradient predivide factor",,,,,,,,,,
115,OPT-350M,"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,1.25E+08,,,1.666666667,,,0.0,180000000000.0,2.25E+20,,180000000000.0,,,25.42,,1.0,1.0,0,0,Transformer,OPT,,,,,1,GPT-2 tokenizer,52000,"Architecture: Decoder-only Transformer; Optimizer: AdamW with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in our smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens.; Training: Gradient clipping, Dynamic loss scaling, Fully Sharded Data Parallel with Megatron-LM Tensor Parallelism, ReLU activation; Attention: Multi-head attention (details in Table 1: Number of attention heads varies with model size); Regularization: Dropout of 0.1 (except on embeddings), Weight decay of 0.1; Initialization: Normal distribution with zero mean and standard deviation of 0.006, standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0",,,,,,,,,,
116,Transformer-XL + SIS,"Sagar Verma, Jean-Christophe Pesquet",2021/05/03,2021,Sparsifying Networks via Subdifferential Inclusion,9.0,,https://web.archive.org/web/20220122141508/http://proceedings.mlr.press/v139/verma21b/verma21b.pdf,2.46E+08,1.03E+08,1.04e+19,UNK,,,0.0,103000000.0,#VALUE!,,103000000.0,,21.1,,,0.0,0.0,0,0,Transformer,Transformer-XL,https://sagarverma.github.io/compression,University of Washington; Facebook AI Research; Allen Institute for AI,Industry - Academia Collaboration,,1,,,,,,,,,,,,,
117,OPT-1.3B (finetuned on PTB),"Susan Zhang∗ , Stephen Roller∗ , Naman Goyal∗ , Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott† , Sam Shleifer† , Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer",2022/06/21,2022,OPT: Open Pre-trained Transformer Language Models,656.0,,https://arxiv.org/pdf/2205.01068.pdf?fbclid=IwAR2zobF7YoESj0HLKULhilGRV-jvNXKr2bkW_b3MqPfFZ6rEyagDP654QFo,1.30E+09,,,1.666666667,,,0.0,180000000000.0,2.34E+21,929000.0,180000000000.0,,,,12.02,0.0,1.0,0,0,Transformer,OPT,,,,,1,GPT-2 byte level BPE tokenizer,52000,"Architecture: Decoder-only Transformer language models. The models largely follow the architecture of Brown et al. (2020).; Optimizer: AdamW optimizer with (β1, β2) set to (0.9, 0.95), and weight decay of 0.1; LR Schedule: Linear learning rate schedule, warming up from 0 to the maximum learning rate over the first 2000 steps in OPT-175B, or over 375M tokens in smaller baselines, and decaying down to 10% of the maximum LR over 300B tokens.; Training: Dynamic loss scaling (to avoid underflows), ReLU Activation, Fully Sharded Data Parallel, Megatron-LM Tensor Parallelism; Attention: Not explicitly mentioned, but since it's a Transformer, it uses multi-head self-attention; Regularization: Dropout of 0.1 (but not on embeddings), Weight decay of 0.1; Initialization: Weight initialization follows Megatron-LM codebase, using a normal distribution with zero mean and standard deviation of 0.006. Standard deviation for output layers are scaled by a 1.0/√2L term where L is the total number of layers. All bias terms are initialized as 0.; Other: Gradient clipping norms at 1.0 (except for some mid-flight changes), Gradient predivide factor to reduce the risk of over/underflows",,,,,,,,,,
118,GLM-10B,"Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",2021/03/18,2021,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,131.0,,https://arxiv.org/abs/2103.10360,1.00E+10,,,1,,,0.0,632000000000.0,3.79E+22,,632000000000.0,,,12.0,22.52,1.0,0.0,0,0,Transformer,GLM,https://github.com/THUDM/GLM,,,,1,,,"Architecture: Transformer with several modifications: (1) rearranged order of layer normalization and the residual connection, (2) single linear layer for output token prediction, (3) ReLU replaced with GeLUs; Optimizer: Adam; LR Schedule: Cosine decay; Training: 2D positional encodings, Autoregressive blank infilling; Attention: Multi-head attention (12 or 16 heads depending on model size); Regularization: Dropout 0.1, Weight decay 0.1 or 0.01 depending on model; Special Algorithms: Autoregressive Blank Infilling objective: random span shuffling, 2D Positional Encoding; Other: Multi-task pretraining by jointly optimizing the blank infilling objective with document-level or sentence-level objectives.",,,,,,,,,,
119,AWD-LSTM,"Gábor Melis, Chris Dyer, Phil Blunsom",2017/07/18,2017,On the State of the Art of Evaluation in Neural Language Models,555.0,,https://arxiv.org/abs/1707.05589,3.30E+07,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,WikiText-2,,65.8,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,GPT-2 tokenizer,52000,"Architecture: LSTM stacked as layers with additive skip connections, optional down-projection; Optimizer: Adam with β₁ = 0 (effectively RMSProp without momentum, but with Adam's bias correction); LR Schedule: Learning rate is multiplied by 0.1 whenever validation performance does not improve during 30 consecutive checkpoints.; Attention: not applicable; Regularization: input dropout, intra-layer dropout, output dropout, variational dropout or recurrent dropout, weight decay; Initialization: model starts with zero state at the beginning of training and test time, with probability 0.01 a constant zero state is provided as the initial state during training",,,,,,,,,,
120,B2T connection (16L),"Sho Takase, Shun Kiyono, Sosuke Kobayashi, Jun Suzuki",2022/06/01,2022,On Layer Normalizations and Residual Connections in Transformers,4.0,1.0,https://web.archive.org/web/20220602013934/https://arxiv.org/pdf/2206.00330.pdf,2.47E+08,1.03E+08,2.8e+19,150,,,0.0,103000000.0,2.29E+19,0.0,103000000.0,,19.2,,,0.0,1.0,0,0,Transformer,Transformer-XL,,,,,1,,300000,"Architecture: Transformer-based encoder-decoder architecture. Investigates both Post-LN and Pre-LN configurations, and proposes a new connection called Bottom-to-Top (B2T) connection; Optimizer: Adam (specified as 'Adam B' in hyperparameter table); LR Schedule: Inverse sqrt decay; Training: Layer Normalization (LN), Residual Connections, Gradient Clipping, Dropout, Byte-Pair-Encoding (BPE); Attention: Multi-head attention; Regularization: Dropout, Word Dropout; Special Algorithms: Bottom-to-Top (B2T) connection: A novel residual connection that skips over layer normalizations to improve stability and performance. Equations provided for its implementation (Eq. 5, 6, 7, 8); Initialization: Multiplied initial parameter values (except embeddings) by 0.1",,,,,,,,,,
121,RSM,"David Rawlinson, Abdelrahman Ahmed, Gideon Kowadlo",2019/05/28,2019,Learning distant cause and effect using only local and immediate credit assignment,3.0,1.0,https://arxiv.org/pdf/1905.11589,,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,166.0,0.0,0.0,0,0,,,https://github.com/Cerenaut/rsm,,,,0,,,"Architecture: Recurrent Sparse Memory (RSM), a predictive autoencoder derived from sparse autoencoders. Single-layer model is a standard recurrent network with inhibition, rank-based sparse masking, and integration & normalization of recurrent input. Allows for multiple RSM layers.; Optimizer: Stochastic gradient descent; LR Schedule: Not mentioned; Training: Sparse coding (fixed top-k sparsity), Inhibition (to encourage cells to learn unique contexts), Leaky-ReLU nonlinearities in the classifier network (to reduce accumulation of dead cells), Dropout (to generalize learned sequences); Attention: No explicit attention mechanism is used, but the authors suggest self-attention as future work to improve selective remembering.; Regularization: L2 regularization (small L2 value improved classifier performance); Special Algorithms: Rank-based sparse masking; Initialization: Recurrent input (xR) is initialized as zeros; Other: Feedback input and parameters required when stacking more than one RSM layer with bi-directional connections, Forgetting of the recurrent state with a fixed probability during training., Integration of recurrent input over time.",,,,,,,,,,
122,AWD-LSTM-MoS+Noisin+dynamic evaluation ,"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018/05/03,2018,Noisin: Unbiased Regularization for Recurrent Neural Networks,26.0,,https://arxiv.org/pdf/1805.01500,2.20E+07,,,400,,,0.0,929000.0,4.91E+16,,929000.0,,,,47.6,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,10000,"Architecture: LSTM (Long Short-Term Memory), including variants with dropout (dropout-LSTM). Also tested with AWD-LSTM-MoS (from Yang et al., 2017) with dynamic evaluation and Noisin. The number of layers is 2, with either 650 or 1500 hidden units each for medium and large network sizes respectively.; Optimizer: Average Stochastic Gradient Descent (SGD); LR Schedule: Initial learning rate of 30. Divided by a factor of 1.2 if the perplexity on the validation set deteriorates.; Training: Truncated backpropagation through time (BPTT); Attention: Not applicable; Regularization: Noisin (noise injection), Weight decay (not used in basic experiments but AWD-LSTM from Yang et al. might contain), Dropout; Special Algorithms: Noisin (unbiased noise injection); Initialization: Embedding weights initialized uniformly in [-0.1, 0.1]. Other weights initialized uniformly between [-1/sqrt(H), 1/sqrt(H)], where H is the number of hidden units in a layer. Biases initialized to 0.",,,,,,,,,,
123,LSTM+Noise(Beta),"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018/05/03,2018,Noisin: Unbiased Regularization for Recurrent Neural Networks,26.0,,https://arxiv.org/pdf/1805.01500,5.10E+07,,,200,,,0.0,2080000.0,1.27E+17,,2080000.0,,,82.9,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,30000,"Architecture: LSTM, and mentions ERNN as a building block. LSTM can be built with noise injection to form a multi-layered noise-injected LSTM; Optimizer: average stochastic gradient descent; LR Schedule: Initial learning rate of 30, divided by 1.2 if perplexity on the validation set deteriorates.; Training: truncated backpropagation through time; Attention: Not applicable; Regularization: Noisin, Weight decay or pointers were not used in the basic settings, Gradient clipping (maximum norm of 0.25); Special Algorithms: Noisin: injects noise into the hidden states of an RNN, Unbiased noise injection; Initialization: Embedding weights initialized uniformly in [-0.1, 0.1]. Other weights initialized uniformly between [-1/sqrt(H), 1/sqrt(H)], where H is the number of hidden units in a layer. Biases initialized to 0.",,,,,,,,,,
124,Dropout-LSTM+Noise(Laplace),"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018/05/03,2018,Noisin: Unbiased Regularization for Recurrent Neural Networks,26.0,,https://arxiv.org/pdf/1805.01500,1.30E+07,,,200,,,0.0,2080000.0,3.24E+16,,2080000.0,,,82.1,,0.0,1.0,0,0,Recurrent,,,,,,1,,30000,"Architecture: LSTM and dropout-LSTM. The LSTM transition function composes four ERNNs, three with sigmoid activations and one with a tanh activation. Can also be an ERNN; Optimizer: Average stochastic gradient descent; LR Schedule: Initial learning rate of 30, divided by a factor of 1.2 if the perplexity on the validation set deteriorates.; Training: Truncated backpropagation through time; Attention: Not applicable; Regularization: Noisin (noise injection at hidden states) multiplicative and additive noise variations studied, Dropout; Special Algorithms: Noisin; Initialization: Embedding weights initialized uniformly in [-0.1, 0.1]. Other weights initialized uniformly in [-1/sqrt(H), 1/sqrt(H)] where H is the number of hidden units. Biases initialized to 0.; Other: Gradient clipping (maximum norm of 0.25)",,,,,,,,,,
125,Dropout-LSTM+Noise(Bernoulli) (WT2),"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018/05/03,2018,Noisin: Unbiased Regularization for Recurrent Neural Networks,26.0,,https://arxiv.org/pdf/1805.01500,5.10E+07,,,200,,,0.0,2080000.0,1.27E+17,,2080000.0,,,76.8,,0.0,1.0,0,0,Recurrent,,,,,,1,,30000,"Architecture: LSTM, dropout-LSTM. Mentions ERNN as simpler RNN. The LSTM transition function composes four ERNNs, three with sigmoid activations and one with a tanh activation.; Optimizer: Truncated backpropagation through time with average stochastic gradient descent; LR Schedule: Initial learning rate of 30, divided by 1.2 if the perplexity on the validation set deteriorates.; Training: truncated backpropagation through time; Attention: Not applicable; Regularization: Noisin (injects noise into hidden states), Dropout (for dropout-LSTM), Gradient clipping (maximum norm of 0.25), Noise Injection (additive or multiplicative); Special Algorithms: Noisin; Initialization: Embedding weights initialized uniformly in [-0.1, 0.1]. Other weights were initialized uniformly between [-1/sqrt(H), 1/sqrt(H)], where H is the number of hidden units in a layer. Biases were initialized to 0.; Other: Use of auxiliary noise variables to compute hidden states, Monte Carlo approximation for the expected log-likelihood under injected noise, Noise level control via noise spread parameter",,,,,,,,,,
126,Dropout-LSTM+Noise(Bernoulli) (PTB),"Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei",2018/05/03,2018,Noisin: Unbiased Regularization for Recurrent Neural Networks,26.0,,https://arxiv.org/pdf/1805.01500,5.10E+07,,,200,,,0.0,929000.0,5.69E+16,,929000.0,,,,66.1,0.0,1.0,0,0,Recurrent,,,,,,1,Word-level,10000,"Architecture: LSTM, LSTM with dropout; Optimizer: Average Stochastic Gradient Descent; LR Schedule: Initial learning rate of 30, divided by a factor of 1.2 if perplexity on validation set deteriorates; Training: truncated backpropagation through time; Attention: Not applicable; Regularization: Noise Injection, gradient clipping (maximum norm 0.25), Dropout (for dropout-LSTM); Special Algorithms: Noisin; Initialization: Embedding weights initialized uniformly in [-0.1, 0.1]. All other weights initialized uniformly between [-sqrt(1/H), sqrt(1/H)] where H is the number of hidden units in a layer. All biases were initialized to 0.",,,,,,,,,,
127,GRU + p-tHSM (pretrain via Brown) (PTB),"Nan Jiang, Wenge Rong, Min Gao, Yikang Shen, Zhang Xiong",2017/08/19,2017,Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models,5.0,,https://www.researchgate.net/profile/Yikang-Shen-2/publication/318830618_Exploration_of_Tree-based_Hierarchical_Softmax_for_Recurrent_Language_Models/links/5b2c050aa6fdcc8506bc6f4a/Exploration-of-Tree-based-Hierarchical-Softmax-for-Recurrent-Language-Models.pdf,5.20E+06,,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,6.97E+05,,,128.78,0.0,0.0,0,0,Recurrent,GRU,,,,,1,,,,,,,,,,,,,
128,L_UL-seq,"Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston",2019/08/12,2019,Neural Text Generation with Unlikelihood Training,365.0,,https://arxiv.org/abs/1908.04319,2.47E+08,,,,,,0.0,103000000.0,0.00E+00,0.0,103000000.0,WikiText-103,25.42,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/facebookresearch/unlikelihood_training,,,,1,Word-level,260000,"Architecture: 16-layer Transformer with 8 attention heads, embedding dimension 1024, and fully-connected dimension 4096; the architecture is based on Baevski and Auli (2019) but with standard embedding and softmax layers.; Optimizer: Not explicitly stated in paper, but mentions training each model on 8 GPUs; LR Schedule: Not explicitly stated, but the best validation perplexity of saved models were selected.; Training: Token-level unlikelihood training, Sequence-level unlikelihood training; Attention: Multi-head attention with 8 heads; Special Algorithms: Unlikelihood Training (LUL-token, LUL-seq, LUL-token+seq); Initialization: Not mentioned",,,,,,,,,,
129,Characterizing Verbatim Short-Term Memory in Neural Language Models (182M),"Kristijan Armeni, Christopher Honey, Tal Linzen",2022/10/24,2022,Characterizing Verbatim Short-Term Memory in Neural Language Models,3.0,1.0,https://arxiv.org/pdf/2210.13569.pdf,1.82E+08,,,,,,0.0,103000000.0,0.00E+00,,103000000.0,,41.9,,,0.0,0.0,0,0,,,https://github.com/KristijanArmeni/verbatim-memory-in-NLMs,NT Communication Science Laboratories; Tohoku University,Industry - Academia Collaboration,,1,,,"Architecture: AWD-LSTM with 3 hidden layers and 400-dimensional input embeddings, 1840-dimensional hidden states. Transformers with 1, 3, 6 and 12 layers and GPT-2 architecture; Optimizer: Unknown for LSTM (implementation from repository), Adam for Transformers; LR Schedule: Unknown for LSTM (implementation from repository), Early stopping when the loss did not decrease for at least 0.01 bits in 5 consecutive evaluations for Transformers; Training: Unknown; Attention: Multi-head attention in Transformers; Regularization: Adaptive Weight Dropping (AWD) for LSTM; Initialization: Random initialization of weights in Transformers",,,,,,,,,,
130,Frage-AWD-LSTM-MemoryAug-NeuralCache (PTB),"Ke Li, Daniel Povey, Sanjeev Khudanpur",2020/09/29,2020,Neural Language Modeling With Implicit Cache Pointers,4.0,1.0,https://arxiv.org/pdf/2009.13774,2.40E+07,,,,,,1.0,929000.0,0.00E+00,,929000.0,,,,52.5,0.0,1.0,1,0,Recurrent,,,,,,1,Word-level,10000,"Architecture: LSTM and Transformer-based language models. The architecture is enhanced with a 'pointer component' that extends the output layer to represent the L immediately preceding words in history. The 'memory augmented pointer component' introduces an additional unit to capture the burstiness of words likely to reoccur.; Optimizer: SGD (for LSTM and the described enhancements). The paper mentions that Adam was tried for Transformer LMs but SGD worked better.; LR Schedule: Not explicitly mentioned; Training: Truncated backpropagation through time (BPTT); Attention: The paper compares against and incorporates aspects of attention mechanisms (pointer networks) but does not itself explicitly use standard attention.; Regularization: Dropout (0.5 for PTB and WikiText-2, 0.1 for SWBD+Fisher), Weight tying between embedding and output matrices; Special Algorithms: Implicit Cache Pointers (pointer component and memory augmented pointer component), Memory augmentation to learn the burstiness of words; Initialization: Initial LM state for each utterance is initialized with the last LM state of the best hypothesis for the previous utterance during rescoring (state-carry)",,,,,,,,,,
131,Frage-AWD-LSTM-MemoryAug-NeuralCache (WT2),"Ke Li, Daniel Povey, Sanjeev Khudanpur",2020/09/29,2020,Neural Language Modeling With Implicit Cache Pointers,4.0,1.0,https://arxiv.org/pdf/2009.13774,3.30E+07,,,,,,1.0,2080000.0,0.00E+00,,2080000.0,,,55.6,,0.0,1.0,1,0,Recurrent,,,,,,1,,33000,"Architecture: RNNLMs (LSTM) and Transformer-based LMs. Extended output layer to represent L preceding words in history. Memory augmentation unit to learn burstiness of words.  The output is extended by a predefined size L to represent L preceding words in history.; Optimizer: SGD. For Transformer LMs, Adam was attempted but did not provide better results than SGD.; LR Schedule: Not explicitly mentioned.; Training: Truncated Backpropagation Through Time (BPTT); Attention: Not used directly, but an implicit cache pointer mechanism is introduced.; Regularization: Dropout rate 0.5 for PTB and WikiText-2, 0.1 for SWBD+Fisher, Embedding and output matrices are tied; Special Algorithms: Cache-inspired pointer mechanism:  the output is extended by a predefined size L to represent L preceding words in history.  Additional unit introduced to capture the probability that the current word may be a self-trigger.; Initialization: Initial LM state is initialized with the last LM state of the best hypothesis for the previous utterance for ASR tasks; Other: Frequency-agnostic word embeddings are incorporated into a variant of the AWD-LSTM model (Frage-AWD-LSTM)., The supervision vector is set to have additional ones in history positions where the target was previously seen (at-least-one-hot vector).",,,,,,,,,,
132,ADP-FAIRSEQ+NGRAMRES,"Huayang Li, Deng Cai, Jin Xu, Taro Watanabe",2022/10/26,2022,N-gram Is Back: Residual Learning of Neural Text Generation with n-gram Language Model,0.0,1.0,https://web.archive.org/web/20221027013457/https://arxiv.org/pdf/2210.14431.pdf,2.47E+08,2.02E+08,,UNK,,,0.0,103000000.0,#VALUE!,101000000.0,204000000.0,,18.2,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/ghrua/NgramRes,,,,1,Word-level,260000,"Architecture: Transformer, specifically a 6-layer encoder and 6-layer decoder Transformer model.; Optimizer: Not explicitly mentioned, likely the default optimizer for fairseq.; LR Schedule: Not explicitly mentioned, likely the default schedule used in fairseq.; Training: Dropout (rate 0.3), Weight decay (rate 0.001); Attention: Multi-head attention with 4 heads; Regularization: Dropout, Weight decay; Special Algorithms: N-gram Residual Learning (NGRAMRES), NGRAMRES-ANNEAL which linearly decreases the alpha value after each update; Initialization: Random Initialization; Other: Logits are combined using a weighted sum, controlled by hyperparameter alpha.",,,,,,,,,,
133,ADP-FAIRSEQ,"Huayang Li, Deng Cai, Jin Xu, Taro Watanabe",2022/10/26,2022,N-gram Is Back: Residual Learning of Neural Text Generation with n-gram Language Model,0.0,1.0,https://web.archive.org/web/20221027013457/https://arxiv.org/pdf/2210.14431.pdf,2.47E+08,1.01E+08,,UNK,,,0.0,103000000.0,#VALUE!,,103000000.0,,18.9,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/ghrua/NgramRes,,,,1,Word-level,260000,"Architecture: Transformer-XL as the base model. Uses a Transformer model with 6 encoder layers and 6 decoder layers for machine translation. In the language modeling task, GPT-2 base and Adaptive Input (ADP) models are employed.; Optimizer: Not explicitly mentioned, but uses the default hyperparameters of the chosen models, so we can assume Adam.; LR Schedule: Default hyperparameters are used, which are usually coupled to a scheduler. The NGRAMRES-ANNEAL approach decreases the value of α (a hyperparameter to control the smoothness of the logits of the n-gram distribution) linearly after each update in machine translation experiments. Alpha becomes zero after 10k steps.; Attention: Multi-head attention, number of attention heads set to 4 for machine translation with smaller IWSLT datasets (hidden size of FFN sublayers set to 1024).; Regularization: dropout rate is set to 0.3, and the weight decay rate is set to 0.001 in the translation models; Special Algorithms: Residual learning to combine the n-gram and neural LMs.  The loss function is a modified MLE loss to train the neural LM that approximates the residual function between the real-data distribution and the n-gram prediction distribution. Kneser-Ney smoothing is used in the n-gram language model component.; Initialization: Models are trained from random initialization.",,,,,,,,,,
134,Byte-mLSTM+emb+WN+VD,"Ben Krause, Liang Lu, Iain Murray, Steve Renals",2016/09/26,2016,Multiplicative LSTM for sequence modelling,216.0,,https://arxiv.org/pdf/1609.07959,4.60E+07,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,,,88.8,,1.0,1.0,0,0,Recurrent,LSTM,https://github.com/benkrause/mLSTM,,,,1,Word-level,33000,"Architecture: Multiplicative LSTM (mLSTM), combines LSTM and multiplicative recurrent neural network architectures. mLSTM has flexible input-dependent transitions. Standard LSTM cell with the output gate outside the tanh.; Optimizer: RMSprop with normalized updates (initial experiments), Adam (follow-up experiments); LR Schedule: Exponential decay (initial experiments), Linear decay (follow-up experiments); Attention: N/A; Regularization: Weight Normalization, Variational Dropout (dropout mask is shared across a sequence), Early stopping; Initialization: Scaled orthogonal initializations for the hidden weights, initial forget gate bias of 3",,,,,,,,,,
135,Multi-cell LSTM,"Thomas Cherian, Akshay Badola, Vineet Padmanabhan",2018/11/15,2018,Multi-cell LSTM Based Neural Language Model,6.0,,https://arxiv.org/pdf/1811.06477,7.20E+06,,,50,,,0.0,929000.0,2.01E+15,,929000.0,,,,77.12,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,10000,"Architecture: 2-layer Multi-cell LSTM; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: Learning rate annealing; Training: Dropout, Back-propagation through time; Attention: Not applicable; Regularization: Dropout on all connections other than recurrent connections; Special Algorithms: Multi-cell LSTM node architecture, Multiple memory cell selection strategies: Simple Mean, Weighted Sum, Random Selection, Max Pooling, Min-Max Pooling, Learnable Weights for the cells; Initialization: Hidden states are initialized to 0; Other: Gradient Clipping",,,,,,,,,,
136,True-Regularization+Finetune+Dynamic-Eval,"Yangyang Shi, Mei-Yuh Hwang, Xin Lei, Haoyu Sheng",2019/04/08,2019,Knowledge Distillation For Recurrent Neural Network Language Modeling With Trust Regularization,24.0,,https://arxiv.org/pdf/1904.04163,7.00E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,47.6,0.0,0.0,0,0,Recurrent,RNN,,,,,1,,,"Architecture: High-rank RNNLM with mixture of softmaxes (MoS); Optimizer: Not explicitly stated, but the techniques closely follow Merity et al. (2017), which used SGD with momentum.; LR Schedule: Not explicitly stated, but the techniques closely follow Merity et al. (2017).; Training: Knowledge Distillation, Trust Regularization; Attention: Not applicable; Regularization: DropConnect, Variational Dropout, Embedding Dropout, Activation Regularization (for teacher model only); Special Algorithms: Trust Regularization (TR) method; Initialization: Initialized with different random seeds.; Other: Mixture of Softmaxes (MoS)",,,,,,,,,,
137,Monarch-GPT-2-Medium,"Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher Ré",2022/04/01,2022,Monarch: Expressive Structured Matrices for Efficient and Accurate Training,28.0,,https://arxiv.org/pdf/2204.00595,1.65E+08,,,110,,,0.0,4000000000.0,4.36E+20,,4000000000.0,,20.3,,,0.0,1.0,0,0,Transformer,GPT,https://github.com/HazyResearch/monarch,University of Texas at Austin,Academia,,1,?,?,"Architecture: Transformer; Optimizer: Adam, AdamW, LAMB; LR Schedule: Varies based on model and task (Tables 9, 10, 11). Learning rate scheduler not explicitly specified but described in Yuan et al. [107]; Training: Gradient accumulation (effective batch size is increased using gradient accumulation), Mixed precision training (fp16 and fp32); Attention: Transformer attention mechanisms (projection matrices used in attention blocks); Regularization: Dropout (some regularization strength reduction for Monarch versions), Stochastic depth (some reduction for Monarch versions); Special Algorithms: Monarch matrix parameterization, Reverse Sparsification; Initialization: Initialized randomly, IFFT initialization for mSENSE",,,,,,,,,,
138,Monarch-GPT-2-Small,"Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, Christopher Ré",2022/04/01,2022,Monarch: Expressive Structured Matrices for Efficient and Accurate Training,28.0,,https://arxiv.org/pdf/2204.00595,7.20E+07,,,110,,,0.0,4000000000.0,1.90E+20,,4000000000.0,,20.7,,,0.0,1.0,0,0,Transformer,GPT,https://github.com/HazyResearch/monarch,,,,1,?,?,"Architecture: Transformer with Monarch matrices replacing dense weight matrices in attention blocks (projection matrices) and FFN blocks (linear layers). Number of blocks in block-diagonal matrices is 4.; Optimizer: AdamW; LR Schedule: See tables 9, 10, 11 to find specific values for learning rate, weight decay, drop path etc. 
Adoption of hyperparameters from Yuan et al. [107] (Table 9); Training: Gradient accumulation to fit in GPU memory, Mixed precision (fp16 and fp32), DeepSpeed ZeRO optimizer stage 1 to shard the optimizer states; Attention: Projection matrices in attention blocks are replaced by Monarch matrices; Regularization: Dropout regularization, Stochastic depth (reduced amount for Monarch models because they are smaller than dense models)., Weight decay (Table 9); Special Algorithms: Monarch matrix parametrization, Algorithm to project dense weight matrices into Monarch matrices; Initialization: Monarch matrices are initialized randomly and trained. Monarch matrices in mSENSE are intialized to IFFT; Other: Reverse sparsification: training with Monarch weight matrices for most of the time, then transitioning to dense weight matrices, 4-step FFT algorithm",,,,,,,,,,
139,"Mogrifier (d2, MoS2, MC) + dynamic eval","Gábor Melis, Tomáš Kočiský, Phil Blunsom",2019/09/04,2019,Mogrifier LSTM,109.0,,https://arxiv.org/abs/1909.01792,3.50E+07,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,WikiText-2,,38.6,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/deepmind/lamb,,,,1,Word-level,33000,"Architecture: LSTM with Mogrifier module. The Mogrifier module consists of 'r' rounds of mutual gating between the input 'x' and the previous output 'h_prev'. Low-rank factorization is applied to the Q and R matrices used in the Mogrifier.; Optimizer: Adam with beta1 = 0, resembling RMSProp without momentum; LR Schedule: Not explicitly stated, but implied to be reduced upon validation set performance stagnation, switching to averaging weights after a certain number of checkpoints with no improvement or at 80% of training time.; Training: Gradient clipping (norm 10), Weight Averaging, BPTT, Dropout (input, inter-layer, state, output); Attention: Not applicable; Regularization: L2 penalty, Input Dropout, Inter-layer Dropout, State Dropout, Output Dropout; Special Algorithms: Mogrifier module for mutual gating of input and previous output; Initialization: Random initialization, multiplication with constant 2 to have transformations close to the identity; Other: Dynamic evaluation (optional)",,,,,,,,,,
140,"Mogrifier (d2, MC) + dynamic eval","Gábor Melis, Tomáš Kočiský, Phil Blunsom",2019/09/04,2019,Mogrifier LSTM,109.0,,https://arxiv.org/abs/1909.01792,2.40E+07,,,,,,0.0,923000.0,0.00E+00,,923000.0,Penn TreeBank,,,44.8,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/deepmind/lamb,,,,1,Word-level,10000,"Architecture: Mogrifier LSTM: An extension to the LSTM with mutual gating of the current input and the previous output.; Optimizer: Adam with β₁ = 0 (resembles RMSProp without momentum); LR Schedule: Not explicitly stated, but hyperparameters are tuned using a black-box hyperparameter tuner based on batched Gaussian Process Bandits.; Training: Gradient clipping (norm 10), Averaging weights after a certain number of checkpoints with no improvement in validation cross-entropy or at 80% of the training time.; Attention: Not applicable; Regularization: l2 penalty, Input Dropout, Inter Layer Dropout, State Dropout, Output Dropout; Special Algorithms: Mogrifier: Mutual gating of input and previous output for richer interaction modeling.; Initialization: Multiplication with the constant 2 ensures that randomly initialized Q², R² matrices result in transformations close to identity.; Other: Dynamic evaluation (Krause et al. 2017)",,,,,,,,,,
141,TransformerXL + FWL,"Kevin Clark, Kelvin Guu, Ming-Wei Chang, Panupong Pasupat, Geoffrey Hinton, Mohammad Norouzi",2022/12/05,2022,Meta-Learning Fast Weight Language Models,2.0,1.0,https://web.archive.org/web/20221207113900/https://arxiv.org/pdf/2212.02475.pdf,2.57E+08,2.06E+08,,UNK,,,0.0,103000000.0,#VALUE!,103000000.0,206000000.0,,16.6,,,0.0,1.0,0,0,Transformer,Transformer-XL,,,,,1,Word-level,260000,"Architecture: Transformer-XL with Fast Weight Layers (FWLs). FWLs are added on top of the transformer after the last attention layer.; Optimizer: Implied to be standard gradient descent in initial experiments; the text mentions trying Adam but it did not outperform the simpler one in initial experiments.; LR Schedule: Learned step sizes for each weight matrix/vector in the FWL, performing slightly better than having one global step size.; Training: Second-order gradients are computed for FWL parameters, backpropagating through the gradient updates. This allows the model to learn to produce effective gradient updates., Gradient clipping; Attention: Linear Attention is used to compute the fast weight output. The implementation uses a mixed chunk attention method for efficiency.; Special Algorithms: Fast Weight Layers (FWLs); Initialization: Fast weights are initialized with the slow weights.; Other: FWLs use gradient information from previous tokens to update their parameters, expressed as linear attention., Model is trained to expect the gradient update and adapt quickly., Model is trained to produce effective gradient updates.",,,,,,,,,,
142,Progressive LRD,"Habib Hajimolahoseini, Walid Ahmed, Mehdi Rezagholizadeh, Vahid Partovinia, Yang Liu",2022/10/12,2022,Strategies for Applying Low Rank Decomposition to Transformer-Based Models,0.0,1.0,https://web.archive.org/web/20221130215920/https://neurips2022-enlsp.github.io/papers/paper_33.pdf,3.10E+07,1.03E+08,6.2e+19,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,,22.0,,,0.0,0.0,0,0,Transformer,GPT,,OpenAI,Industry,,1,,,,,,,,,,,,,
143,TRIMELMlong (150M),"Zexuan Zhong, Tao Lei, Danqi Chen",2022/05/25,2022,Training Language Models with Memory Augmentation,35.0,,https://arxiv.org/abs/2205.12674,1.50E+08,,,139.81,,,0.0,103000000.0,1.30E+19,103000000.0,206000000.0,WikiText-103,22.66,,,0.0,0.0,0,0,Transformer,Transformer-XL,https://github.com/princeton-nlp/TRIME,Microsoft,Industry,,1,,,"Architecture: Transformer; Optimizer: Adam; LR Schedule: Cosine annealing, Inverse sqrt; Training: Contrastive Representation Learning, Training with In-batch Memories, Gradient Clipping; Attention: Multi-head attention; Special Algorithms: TRIME (Training with In-batch Memories), BM25 for batching segments; Initialization: Not mentioned; Other: Adaptive Softmax, Adaptive Token Embeddings, Scaled Dot-Product for similarity function",,,,,,,,,,
144,Multipop Adaptive Continuous Stack (WT2),"Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, Phil Blunsom",2018/02/15,2018,Memory Architectures in Recurrent Neural Network Language Models,59.0,,https://openreview.net/forum?id=SkFqf0lAZ,2.60E+07,,,UNK,,,0.0,2080000.0,#VALUE!,,2080000.0,,,72.4,,0.0,1.0,0,0,Recurrent,RNN,,,,,1,Word-level,33000,,,,,,,,,,,
145,DARTS (second order),"Hanxiao Liu, Karen Simonyan, Yiming Yang",2018/06/24,2018,DARTS: Differentiable Architecture Search,3990.0,,https://arxiv.org/abs/1806.09055,2.30E+07,,1.1e+16,300,,,0.0,929000.0,3.85E+16,,929000.0,Penn TreeBank,,,55.7,0.0,0.0,0,0,NAS,DARTS,https://github.com/quark0/darts,Seoul National University; Hanyang University,Academia,,1,,,"Architecture: Computation cell is a directed acyclic graph with N nodes. Each node is a latent representation, and each edge is associated with an operation. The cell has two input nodes and one output node. The output is obtained by applying a reduction operation (e.g. concatenation) to all intermediate nodes.; Optimizer: Momentum SGD for weights, Adam for architecture variables; LR Schedule: Cosine annealing without restart for weights, constant for architecture variables; Training: Continuous relaxation of the architecture search space, Bilevel optimization, Gradient descent, Variational Dropout, Path Dropout; Regularization: Weight decay, Cutout, Path Dropout; Special Algorithms: Differentiable Architecture Search (DARTS), Approximated architecture gradient with a first-order and second-order approximation.; Initialization: Zero initialization for architecture variables.; Other: Operations on the edges are initially unknown and learned during the training process, Top-k strongest operations are retained at the end.",,,,,,,,,,
146,SPALM + kNN,"Dani Yogatama, Cyprien de Masson d’Autume, Lingpeng Kong",2021/04/26,2021,Adaptive Semiparametric Language Models,70.0,,https://web.archive.org/web/20230210050534/https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00371/100688/Adaptive-Semiparametric-Language-Models,UNK,1.10E+08,,UNK,,,0.0,110000000.0,#VALUE!,,110000000.0,,17.66,,,0.0,0.0,0,0,Transformer,Transformer,,University of Waterloo; Microsoft Research,Industry - Academia Collaboration,,0,,,,,,,,,,,,,
147,Multipop Adaptive Continuous Stack (PTB),"Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, Phil Blunsom",2018/02/15,2018,Memory Architectures in Recurrent Neural Network Language Models,59.0,,https://openreview.net/forum?id=SkFqf0lAZ,1.10E+07,,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,,,,63.5,0.0,1.0,0,0,Recurrent,RNN,,,,,1,Word-level,10000,,,,,,,,,,,
148,RGC+ASQ (WT2),"Jiarui Fang, Haohuan Fu, Guangwen Yang, Cho-Jui Hsieh",2018/08/13,2018,RedSync : Reducing Synchronization Traffic for Distributed Deep Learning,28.0,,https://arxiv.org/pdf/1808.04357,2.09E+08,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,,,87.84,,0.0,0.0,0,0,,,,Naver AI Lab; Yonsei University,Industry,,1,,,"Architecture: 2-layer LSTM language model architecture with 1500 hidden units per layer; Optimizer: vanilla SGD with gradient clipping and Nesterov momentum SGD; LR Schedule: Learning rate decays when no improvement has been made in validation loss.  A warm-up training, by exponentially decreasing the compression ratio in the first few epochs, is generally adopted to accelerate convergence.; Training: gradient clipping; Attention: Not applicable; Special Algorithms: Alternating Signs Quantization (ASQ), RedSync which combines RGC-based sparsification and quantization techniques; Initialization: Weights of encoder and decoder are tied.; Other: Momentum masking and momentum correction (for Nesterov momentum SGD)",,,,,,,,,,
149,Megatron-LM (2.5B),"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",2019/09/17,2019,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,905.0,,https://arxiv.org/abs/1909.08053,2.50E+09,,4e+21,4.40,,,0.0,46400000000.0,3.06E+21,,46400000000.0,WikiText-103; CC-Stories; RealNews; OpenWebtext,12.76,,,1.0,,0,0,Transformer,GPT,https://github.com/NVIDIA/Megatron-LM,,,,1,Own,51200,"Architecture: Transformer-based language model (GPT-2 and BERT variations) using Encoder or Decoder only, with multi-head attention and feed forward layers. GPT-2 and BERT use GeLU non-linearities and layer normalization to the input of the multi-head attention and feed forward layers.; Optimizer: Adam with weight decay 0.01; LR Schedule: GPT-2: learning rate of 1.5e-4 with a warmup period of 3k iterations before following a single cycle cosine decay over the remaining 297k iterations, stopping the decay at a minimum learning rate of 1e-5. BERT: learning rate of 1.0e-4 warmed up over 10,000 iterations and decayed linearly over 2 million iterations.; Training: mixed precision training with dynamic loss scaling, activation checkpointing after every transformer layer, Model Parallelism, Data Parallelism, Gradient Clipping (global gradient norm clipping of 1.0), weight scaling immediately before residual layers; Attention: Multi-head attention. Hidden size per attention head is kept constant at 96 for GPT-2 scaling analysis.; Regularization: weight decay 0.01, dropout 0.1, Layer Normalization; Initialization: Weights initialized with a simple normal distribution W ~ N(0,0.02); Other: Sentence Order Prediction head replacing the Next Sentence Prediction head for BERT.",,,,,,,,,,
150,Megatron-LM (8.3B),"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",2019/09/17,2019,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,905.0,,https://arxiv.org/abs/1909.08053,8.30E+09,,9.1e+21,4.40,,,0.0,46400000000.0,1.02E+22,,46400000000.0,WikiText-103; CC-Stories; RealNews; OpenWebtext,10.81,,,1.0,1.0,0,0,Transformer,GPT,https://github.com/NVIDIA/Megatron-LM,,,,1,Own,51200,"Architecture: Transformer-based model with decoder architecture similar to GPT-2 and encoder architecture similar to BERT. It uses GeLU nonlinearities and layer normalization to the input of the multi-head attention and feed-forward layers. BERT uses layer normalization to outputs.; Optimizer: Adam with weight decay of 0.01; LR Schedule: For GPT-2, cosine decay with warmup. For BERT, linear decay with warmup.; Training: Mixed precision training with dynamic loss scaling, Activation checkpointing; Attention: Multi-head attention; Regularization: Dropout of 0.1, Weight decay 0.01, Global gradient norm clipping of 1.0; Special Algorithms: Intra-layer model parallelism, Model parallelism by fusing GEMMs in MLP and self-attention layers, Scaling weights before residual layers by 1/sqrt(2N); Initialization: Weights initialized with a normal distribution W ~ N(0, 0.02); Other: Rearranging layer normalization and residual connections in BERT-like models",,,,,,,,,,
151,Megatron-LM (355M),"Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro",2019/09/17,2019,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,905.0,,https://arxiv.org/abs/1909.08053,3.55E+08,,4.35e+20,4.40,,,0.0,46400000000.0,4.35E+20,,46400000000.0,WikiText-103; CC-Stories; RealNews; OpenWebtext,19.31,,,1.0,1.0,0,0,Transformer,GPT,https://github.com/NVIDIA/Megatron-LM,,,,1,Own,51200,"Architecture: Transformer architecture; GPT-2 (decoder) and BERT (encoder); GELU nonlinearities and layer normalization to the input of multi-head attention and feed forward layers; Optimizer: Adam with weight decay 0.01; LR Schedule: GPT-2: learning rate of 1.5e-4, warmup period of 3k iterations, single cycle cosine decay over 297k iterations, minimum learning rate of 1e-5. BERT: learning rate of 1.0e-4, warmed up over 10,000 iterations and decayed linearly over 2 million iterations.; Training: Mixed precision training with dynamic loss scaling, Activation checkpointing; Attention: Multi-head attention; Regularization: Dropout 0.1, Weight decay 0.01, Global gradient norm clipping of 1.0; Special Algorithms: Model Parallelism: intra-layer model parallelism for MLP and Self-Attention blocks., Hybrid Model and Data Parallelism; Initialization: Weights initialized with a normal distribution W ~ N(0,0.02) then scaled by 1/√2N immediately before residual layers, where N is the number of transformer layers.",,,,,,,,,,
152,RGC+ASQ (PTB),"Jiarui Fang, Haohuan Fu, Guangwen Yang, Cho-Jui Hsieh",2018/08/13,2018,RedSync : Reducing Synchronization Traffic for Distributed Deep Learning,28.0,,https://arxiv.org/pdf/1808.04357,6.90E+07,,,40,,,0.0,929000.0,1.54E+16,,929000.0,,,,74.69,0.0,0.0,0,0,,,,Naver AI Lab; Yonsei University,Industry,,1,,,"Architecture: 2-layer LSTM language model architecture with 1500 hidden units per layer; Optimizer: vanilla SGD with gradient clipping; LR Schedule: Learning rate decays when no improvement has been made in validation loss; Training: gradient clipping; Special Algorithms: RedSync:  which combines RGC-based sparsification and quantization techniques together to compress transmitting gradient size of each node to its 0.1%., Alternating Signs Quantization (ASQ); Other: tied the weights of encoder and decoder",,,,,,,,,,
153,MMLSTM,"Kai Shuang, Rui Li, Mengyu Gu, Jonathan Loo, Sen Su",2019/12/05,2019,Major–Minor Long Short-Term Memory for Word-Level Language Model,14.0,,http://repository.uwl.ac.uk/id/eprint/6490/1/Loo_etal_IEEE_TNNLS_2019_Major-minor_long_short-term_memory_for_word-level_language_model.pdf,7.50E+07,,,50,,,0.0,103000000.0,2.32E+18,0.0,103000000.0,WikiText-103,44.69,,,0.0,1.0,0,1,Recurrent,LSTM,,,,,1,Word-level,268000,,,,,,,,,,,
154,LSTM-300units,"Martin Sundermeyer, Ralf Schlüter, Hermann Ney",2012/09/01,2012,LSTM Neural Networks for Language Modeling,2503.0,,http://www.quaero.org/media/files/bibliographie/sundermeyer_lstm_neural_interspeech2012.pdf,1.20E+07,,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,,,,114.5,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,10000,,,,,,,,,,,
155,LSTM+Adam+Lookahead,"Michael R. Zhang, James Lucas, Geoffrey Hinton, Jimmy Ba",2019/07/19,2019,"Lookahead Optimizer: k steps forward, 1 step back",612.0,,https://arxiv.org/pdf/1907.08610,7.19E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,57.72,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/michaelrzhang/lookahead,,,,1,?,?,Architecture: LSTM; Optimizer: Adam; LR Schedule: warmup-then-decay; Special Algorithms: Lookahead,,,,,,,,,,
156,TCN (13M),"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018/02/15,2018,Convolutional Sequence Modeling Revisited,64.0,,https://openreview.net/forum?id=rk8wKk-R-,1.30E+07,,,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-103,,,90.17,0.0,0.0,0,1,Convolutional,TCN,,,,,1,,,,,,,,,,,,,
157,D-LSRC(200)+KN5,"Youssef Oualil, Mittul Singh, Clayton Greenberg, Dietrich Klakow",2017/08/22,2017,Long-Short Range Context Neural Networks for Language Modeling,19.0,,https://arxiv.org/pdf/1708.06555,7.16E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,92.0,0.0,1.0,0,0,Recurrent,LSRC,,,,,1,Word-level,10000,"Architecture: Long-Short Range Context (LSRC) network. An adaptation of LSTM with two separate hidden states that evolve in time: one for local (short) context and one for global (long) context.; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: The learning rate is halved when no significant improvement of the validation data log-likelihood is observed. Then, we continue with seven more epochs while halving the learning rate after each epoch.; Training: Back-Propagation Through Time (BPTT) algorithm; Attention: Not applicable; Regularization: Weight decay; Initialization: Normalized initialization proposed in (Glorot and Bengio, 2010)",,,,,,,,,,
158,D-LSRC(100)+KN5,"Youssef Oualil, Mittul Singh, Clayton Greenberg, Dietrich Klakow",2017/08/22,2017,Long-Short Range Context Neural Networks for Language Modeling,19.0,,https://arxiv.org/pdf/1708.06555,5.97E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,93.0,0.0,1.0,0,0,Recurrent,LSRC,,,,,1,Word-level,10000,"Architecture: Long-Short Range Context (LSRC) network. It is an adaptation of the Long-Short Term Memory (LSTM) network. It explicitly models the local (short) and global (long) context using two separate hidden states that evolve in time. The local state H evolves according to a simple recurrent model similar to RNN. The global state H follows the LSTM model.; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: The learning rate is halved when no significant improvement of the validation data log-likelihood is observed. Then, we continue with seven more epochs while halving the learning rate after each epoch.; Training: Back-Propagation Through Time (BPTT); Regularization: Weight decay; Initialization: Normalized initialization proposed in (Glorot and Bengio, 2010)",,,,,,,,,,
159,bRSM + cache,"Jeremy Gordon, David Rawlinson, Subutai Ahmad",2019/12/02,2019,Long Distance Relationships without Time Travel: Boosting the Performance of a Sparse Predictive Autoencoder in Sequence Modeling,4.0,0.0,https://arxiv.org/abs/1912.01116,2.55E+06,,,15,,,0.0,929000.0,2.13E+14,,929000.0,,,,103.5,0.0,1.0,1,0,Recurrent,bRSM,,,,,1,,10000,"Architecture: bRSM (Boosted Recurrent Sparse Memory), a predictive recurrent autoencoder with recurrent connections, sparse activations, and a boosting rule. The bRSM memory is organized into m groups (or mini-columns), each composed of n cells.  The bRSM also has a 'flattened' architecture where each group was set to have only one cell.; Optimizer: Not explicitly stated, but it is trained to generate the next input using a simple classifier network composed of a 2-layer fully connected ANN using leaky ReLU nonlinearities, which is trained concurrently but independently to the RSM network (not sharing gradients). The LSTM baseline uses Adam optimizer.; LR Schedule: Not mentioned for bRSM. The LSTM baseline uses a constant learning rate. The use of trainable decay is also featured in the bRSM, but this is for decay of cell memory, not training.; Training: boosting (boosted k-winners algorithm), cell activity boosting scheme, top-k sparsity, early stopping (pausing RSM training); Attention: Not applicable; Regularization: L2 regularization of decoder; Special Algorithms: boosted k-winners algorithm, functional partitioning of memory; Initialization: Weights for embedding initialized with FastText. Initial RSM weights are not mentioned explicitly.; Other: predictive autoencoder structure, sparse activations, trainable decay for memory, word cache",,,,,,,,,,
160,LLaMA-33B (LoRA finetuned),"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023/05/23,2023,LLaMA: Open and Efficient Foundation Language Models,0.0,,https://arxiv.org/pdf/2305.14152.pdf,3.30E+10,,,1.09,,,0.0,1400000000000.0,3.02E+23,929000.0,1400000000000.0,,,,7.68,0.0,1.0,0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,,,,1,,32000,Architecture: Transformer; Optimizer: AdamW; LR Schedule: linear decay; Special Algorithms: Parameter-Efficient and Quantization-aware Adaptation (PEQA); Initialization: round-to-nearest (RTN) for the initialization method of quantized LLM.; Other: low-bit weight-only quantization,,,,,,,,,,
161,LLaMA-13B (LoRA finetuned),"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023/05/23,2023,LLaMA: Open and Efficient Foundation Language Models,0.0,,https://arxiv.org/pdf/2305.14152.pdf,1.30E+10,,,1.09,,,0.0,1000000000000.0,8.50E+22,929000.0,1000000000000.0,,,5.54,8.64,0.0,1.0,0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,,,,1,,32000,"Architecture: Transformer; Optimizer: AdamW [64]; LR Schedule: linear decay; Training: FP16 and BF16 training, weight-only quantization; Attention: Not explicitly specified; Special Algorithms: Parameter-Efficient and Quantization-aware Adaptation (PEQA): a method that fine-tunes only the quantization scales of quantized LLMs, keeping the integer matrix frozen.; Initialization: round-to-nearest (RTN) quantization is used for initialization of quantized LLM; Other: Low-bit weight-only quantization",,,,,,,,,,
162,LLaMA-33B,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023/02/27,2023,LLaMA: Open and Efficient Foundation Language Models,963.0,,https://arxiv.org/abs/2302.13971,3.30E+10,,,1.09,,,0.0,1400000000000.0,3.02E+23,0.0,1400000000000.0,,,6.9,,0.5,1.0,0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,,,,1,bytepair encoding (BPE) algorithm,32000,"Architecture: Transformer based architecture with pre-normalization, SwiGLU activation function and Rotary Embeddings.; Optimizer: AdamW optimizer with beta1=0.9, beta2=0.95; LR Schedule: Cosine learning rate schedule with the final learning rate equal to 10% of the maximal learning rate.; Training: Gradient clipping of 1.0, Checkpointing to reduce activation recomputation, Efficient implementation of the causal multi-head attention inspired by Rabe and Staats (2021); Attention: Causal multi-head attention; Regularization: Weight decay of 0.1; Other: RMSNorm normalizing function, Byte-pair encoding (BPE) algorithm for tokenization, 2,000 warmup steps",,,,,,,,,,
163,LLaMA-13B,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023/02/27,2023,LLaMA: Open and Efficient Foundation Language Models,963.0,,https://arxiv.org/abs/2302.13971,1.30E+10,,,1.09,,,1.0,1000000000000.0,8.50E+22,0.0,1000000000000.0,,,13.99,,0.5,1.0,0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,,,,1,,32000,"Architecture: Based on the transformer architecture. Leveraging various improvements that were subsequently proposed, and used in different models such as PaLM.; Optimizer: AdamW optimizer with β₁ = 0.9, β₂ = 0.95.; LR Schedule: Cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate.; Training: Pre-normalization (normalizing the input of each transformer sub-layer), Gradient clipping of 1.0, Activation checkpointing (saving the activations that are expensive to compute, such as the outputs of linear layers); Attention: Causal multi-head attention.; Regularization: Weight decay of 0.1; Initialization: Not mentioned; Other: SwiGLU activation function (replacing ReLU non-linearity), Rotary positional embeddings (RoPE)",,,,,,,,,,
164,LLaMA-7B,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023/02/27,2023,LLaMA: Open and Efficient Foundation Language Models,963.0,,https://arxiv.org/abs/2302.13971,7.00E+09,,,1.09,,,1.0,1000000000000.0,4.58E+22,0.0,1000000000000.0,,,9.49,,0.5,1.0,0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,,,,1,,32000,"Architecture: Transformer based architecture with modifications. These include: Pre-normalization using RMSNorm, SwiGLU activation function, Rotary Embeddings (RoPE).; Optimizer: AdamW (Loshchilov and Hutter, 2017); LR Schedule: Cosine learning rate schedule, such that the final learning rate is equal to 10% of the maximal learning rate.; Training: Gradient clipping of 1.0, Activation checkpointing (selective saving of activations during the backward pass), Efficient implementation of causal multi-head attention inspired by Rabe and Staats (2021) using the backward from Dao et al. (2022)., Model and sequence parallelism, Overlapping of activation computation and inter-GPU communication; Attention: Causal multi-head attention; Regularization: Weight decay of 0.1; Other: Numbers are split into individual digits during tokenization., Fallback to bytes to decompose unknown UTF-8 characters.",,,,,,,,,,
165,LLaMA-65B (LoRA finetuned),"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023/05/23,2023,LLaMA: Open and Efficient Foundation Language Models,0.0,,https://arxiv.org/pdf/2305.14152.pdf,6.52E+10,,,1.09,,,0.0,1400000000000.0,5.97E+23,2080000.0,1400000000000.0,,,4.27,,0.0,1.0,0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,,,,1,Probably Llama default tokenizer,32000,"Architecture: Transformer based language models (specifically the LLaMA family); Optimizer: AdamW; LR Schedule: Linear-decaying learning rate scheduler; Training: PEQA: Parameter-Efficient and Quantization-aware Adaptation, Weight quantization; Attention: Not explicitly specified, but inherited from the base Transformer architecture (likely multi-head self-attention); Special Algorithms: PEQA Algorithm, Quantization Algorithm for weight compression; Initialization: RTN (Round-to-Nearest) for initialization of quantized LLM when using PEQA; Other: Weight-only quantization (weights are quantized, activations remain at higher precision), Quantization scale fine-tuning (only quantization scales are updated, not the integer weight matrix)",,,,,,,,,,
166,LLaMA-65B,"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023/02/27,2023,LLaMA: Open and Efficient Foundation Language Models,963.0,,https://arxiv.org/abs/2302.13971,6.52E+10,,,1.09,,,0.0,1400000000000.0,5.97E+23,0.0,1400000000000.0,,,4.96,,0.5,1.0,0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,,,,1,,32000,"Architecture: Transformer based architecture with modifications: Pre-normalization using RMSNorm, SwiGLU activation function (dimension 4d), Rotary Embeddings (RoPE); Optimizer: AdamW (β₁ = 0.9, β₂ = 0.95); LR Schedule: Cosine learning rate schedule (final LR = 10% of max LR); Attention: Causal multi-head attention with efficient implementation (inspired by Rabe and Staats (2021) and using backward from Dao et al. (2022)); Regularization: Weight decay 0.1, Gradient clipping 1.0; Other: 2,000 warmup steps",,,,,,,,,,
167,LLaMA-7B (LoRA finetuned),"Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample",2023/05/23,2023,LLaMA: Open and Efficient Foundation Language Models,0.0,,https://arxiv.org/pdf/2305.14152.pdf,7.00E+09,,,1.09,,,0.0,1000000000000.0,4.58E+22,929000.0,1000000000000.0,,,6.19,9.69,0.0,1.0,0,0,Transformer,LLaMa,https://github.com/facebookresearch/llama,,,,1,Probably Llama default tokenizer,32000,"Architecture: Transformer-based Large Language Models (LLMs); Optimizer: AdamW; LR Schedule: Linear decay; Training: Quantization-aware training, Parameter-Efficient Fine-Tuning (PEFT); Attention: Not explicitly mentioned but Transformer implies multi-head self-attention; Regularization: Weight decay (likely, since AdamW optimizer is used; explicit value not given); Special Algorithms: Parameter-Efficient and Quantization-aware Adaptation (PEQA): Fine-tunes only the quantization scales of quantized LLMs while keeping the integer matrix frozen.; Initialization: Round-to-Nearest (RTN) is utilized for initialization method for quantized LLM in PEQA. Minimizing || Wo - Wo||, where Wo is the original pre-trained weight and Wo is the quantized weight.; Other: Low-bit weight-only quantization, Updates to Quantization scales, Group-wise Quantization",,,,,,,,,,
168,MemSizer,"Yizhe Zhang, Deng Cai",2022/03/23,2022,Linearizing Transformer with Key-Value Memory,0.0,0.0,https://web.archive.org/web/20220327055642/https://arxiv.org/pdf/2203.12644.pdf,3.57E+08,1.03E+08,7.3e+18,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,,20.8,,,0.0,1.0,0,0,Transformer,Transformer-XL,,,,,1,Word-level,268000,"Architecture: Key-value memory layer substituted into a vanilla Transformer, with a lightweight multi-head mechanism. It projects source sequences into lower dimension representations like Linformer, while enjoying efficient recurrent-style incremental computation similar to kernel-based transformers. 6 layers, 16 attention heads, 1024 model dimensions, and 4096 hidden dimensions for the large-sized transformer used for machine translation.; Optimizer: Adam with betas (0.9, 0.98); LR Schedule: Not explicitly mentioned, learning rate used is mentioned; Training: Layer Normalization, Mean-pooling for multi-head output aggregation, Incremental computation of recurrent matrix, Label smoothing; Attention: Lightweight multi-head computation.  Attention is achieved by computing the normalized similarities of query vector and key vectors. Attention weights are then used to calculate a weighted average of the value vectors.; Regularization: Dropout, Weight Decay; Special Algorithms: Key-value memory networks with an unbalanced key-value mechanism, Dynamic-length projection using a linear kernel (XTX), Fixed-sized memory representation for source sequences; Initialization: Xavier initialization; Other: Memory key is input-independent and shared across different instances (a learnable parametric matrix), The model can be made more expressive with multi-head specification, where they share V across r different heads but use a distinct K for each head.",,,,,,,,,,
169,Linear Transformer (large),"Imanol Schlag, Kazuki Irie, Jürgen Schmidhuber",2021/02/22,2021,Linear Transformers Are Secretly Fast Weight Programmers,78.0,,https://arxiv.org/pdf/2102.11174.pdf,9.00E+07,,,70,,,0.0,103000000.0,3.89E+18,,103000000.0,,31.5,,,0.0,1.0,0,0,Transformer,Linear Transformer,https://github.com/ischlag/fast-weight-transformers,,,,1,,268000,"Architecture: Linear Transformer, building on Fast Weight Programmers (FWPs); Optimizer: Adam with default hyperparameters; LR Schedule: Not explicitly stated, but implied to be using defaults of Adam, can adapt based on performance; Training: Mini-batch training, Backpropagation; Attention: Self-attention mechanism is the core, with a focus on linearizing the softmax function; Special Algorithms: Error-correcting delta rule inspired programming instruction to update the fast weights., Deterministic Parameter-Free Projection (DPFP) for linearizing the attention; Initialization: W(0) = 0 and z(0) = 0; Other: Introduction of a write-strength parameter beta to control interpolation of new and retrieved values in memory., Exploration of different kernel functions to linearize attention",,,,,,,,,,
170,Base LM + kNN LM + Continuous Cache,"Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis",2019/11/01,2019,Generalization through Memorization: Nearest Neighbor Language Models,410.0,,https://arxiv.org/abs/1911.00172,2.47E+08,,7.3e+18,200.00,,,0.0,103000000.0,3.05E+19,,103000000.0,WikiText-103,16.12,,,0.0,0.0,1,0,Transformer,Transformer-XL,https://github.com/urvashik/knnlm,IIT Delhi,Academia,,1,,,"Architecture: Decoder-only Transformer with 16 layers, 16 self-attention heads, 1024 dimensional hidden states, and 4096 dimensional feedforward layers. Adaptive inputs and an adaptive softmax are used for the WIKITEXT-103 experiments. On other datasets adaptive inputs or an adaptive softmax are not used.; Optimizer: Not explicitly mentioned, but inferred from the paper using the same architecture and optimization as described by Baevski & Auli (2019).; LR Schedule: Not explicitly mentioned, but inferred from the paper using the same architecture and optimization as described by Baevski & Auli (2019).; Attention: Multi-head self-attention with 16 heads; Special Algorithms: k-Nearest Neighbors Language Model (kNN-LM), Continuous Cache Model; Initialization: Not mentioned; Other: Interpolation parameter A used to combine kNN-LM and regular LM., Using FAISS for fast nearest neighbor retrieval in high dimensional spaces.",,,,,,,,,,
171,Linear Transformer (small),"Imanol Schlag, Kazuki Irie, Jürgen Schmidhuber",2021/02/22,2021,Linear Transformers Are Secretly Fast Weight Programmers,78.0,,https://arxiv.org/pdf/2102.11174.pdf,4.00E+07,,,120,,,0.0,103000000.0,2.97E+18,,103000000.0,,35.5,,,0.0,1.0,0,0,Transformer,Linear Transformer,https://github.com/ischlag/fast-weight-transformers,,,,1,,268000,"Architecture: Linear Transformer; Optimizer: Adam; LR Schedule: Not specified explicitly; Attention: Linearised self-attention mechanisms. Replaces softmax with linear kernels.; Special Algorithms: Fast Weight Programmers (FWPs): Used to program 'fast weights' in a network, improving dynamic interaction with memory content., Delta rule-like programming instruction for FWPs: An improved programming instruction akin to the error-correcting delta rule, allowing the FWP to learn to correct key-value mappings.; Initialization: W(0) = 0 and z(0) = 0, for fast weight update rule.; Other: New kernel function to linearize attention for simplicity and effectiveness., Introduction of Deterministic Parameter-Free Projection (DPFP) as an alternative attention function., Introduction of Sum Normalization, by dividing effective key and query vectors by the sum of its components.",,,,,,,,,,
172,DITTO,"Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, Jian Li",2022/06/06,2022,Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation,14.0,,https://web.archive.org/web/20221011104229/https://arxiv.org/pdf/2206.02369.pdf,7.50E+08,1.03E+08,1.1e+19,7.16,,,0.0,103000000.0,3.32E+18,0.0,103000000.0,,24.33,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/Jxu-Thu/DITTO,,,,1,?,?,"Architecture: 16-layer Transformer decoder with 8 attention heads, hidden size 1024 and fully-connected dimension 4096; Optimizer: Not explicitly stated in the main paper, but mentions Welleck et al. [35]'s implementation which they follow - a reasonable assumption is Adam; LR Schedule: Not explicitly stated, but mentions Welleck et al. [35]'s implementation which they follow; Training: Fine-tuning, Sentence-level repetition penalization; Attention: Multi-head attention with 8 heads; Special Algorithms: DITTO (PseuDo-RepetITion PenalizaTiOn); Initialization: Not mentioned",,,,,,,,,,
173,SCRN(Structurally Constrained Recurrent Network),"Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato",2014/12/24,2014,Learning Longer Memory in Recurrent Neural Networks,306.0,,https://arxiv.org/abs/1412.7753,2.65E+07,,,UNK,,,1.0,929000.0,#VALUE!,,929000.0,,,,115.0,0.0,1.0,0,0,Recurrent,RNN,https://github.com/facebookarchive/SCRNNs,,,,1,,10000,"Architecture: Structurally Constrained Recurrent Network (SCRN). It consists of a standard RNN architecture with a modified recurrent matrix. A part of the hidden units, called context units, are encouraged to change their state slowly by making part of the recurrent weight matrix close to identity. It also has a fast layer (hidden layer) with a fully connected recurrent matrix.; Optimizer: Stochastic Gradient Descent; LR Schedule: The learning rate is divided by 1.5 after each training epoch when the validation error does not decrease.; Training: Back-propagation through time; Attention: Not applicable; Regularization: Gradient renormalization (equivalent to gradient clipping); Special Algorithms: Structurally Constrained Recurrent Network: introduced a structural modification of the recurrent neural network architecture, encouraging some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity.; Initialization: Not mentioned; Other: Hierarchical Softmax: A simple hierarchy with two levels, by binning the tokens into √d clusters with same cumulative word frequency to reduce the complexity of computing the soft-max.",,,,,,,,,,
174,RNS-RNN,"Brian DuSell, David Chiang",2021/09/05,2021,Learning Hierarchical Structures with Differentiable Nondeterministic Stacks,5.0,,https://arxiv.org/pdf/2109.01982,5.66E+06,,,100,,,0.0,929000.0,3.15E+15,,929000.0,,,,117.56,0.0,1.0,0,1,Recurrent,RNN,https://github.com/bdusell/nondeterministic-stack-rnn,,,,1,?,?,"Architecture: Nondeterministic Stack RNN (NS-RNN) based on a differentiable data structure that simulates a nondeterministic PDA. It's based on an LSTM controller.; Optimizer: Simple Stochastic Gradient Descent (SGD); LR Schedule: Learning rate decayed by a factor of 1.5 if there is no improvement in validation perplexity for 2 epochs. Log-uniform distribution used to determine initial learning rate.; Training: truncated backpropagation through time (BPTT); Special Algorithms: Renormalizing NS-RNN (RNS-RNN): The two key changes are:

1) Stack actions have weights that do not necessarily form a probability distribution.
2) The RNS-RNN includes not only top stack symbols but also PDA states in this query.; Initialization: Random initialization uniformly from the interval [-0.05, 0.05].; Other: Stack actions have weights that do not necessarily form a probability distribution., Memory-Limited NS-RNN: Introducing the constraint that the stack WFA can only contain transitions where t-i does not exceed a hyperparameter D.",,,,,,,,,,
175,NLM,"Junxian He, Graham Neubig, Taylor Berg-Kirkpatrick",2021/09/09,2021,Efficient Nearest Neighbor Language Models,55.0,,https://arxiv.org/abs/2109.04212,5.15E+08,,7.36e+18,,,,0.0,103000000.0,0.00E+00,103000000.0,206000000.0,WikiText-103,18.66,,,0.0,0.0,0,0,Transformer,kNN-LM,https://github.com/jxhe/efficient-knnlm,Tianjin University; Microsoft Research; Beijing Institute of Technology,Industry - Academia Collaboration,,1,,,"Architecture: Transformer (decoder-only) for the base language model. The paper focuses on optimizing kNN-LM, which interpolates the Transformer output with a distribution from a datastore.; Optimizer: Adam (for training the retrieval adaptor); LR Schedule: Not explicitly mentioned for the base Transformer model. The retrieval adaptor uses a constant learning rate.; Training: Dropout 0.2 (for the retrieval adaptor), ReLU activations (for the retrieval adaptor), LogSoftmax (for the retrieval adaptor); Attention: Self-attentional (Vaswani et al., 2017); Regularization: L1 regularization on the weights of the retrieval adaptor to encourage sparsity. L1 regularizer is used to encourage learning sparse weights for PkNN.; Special Algorithms: k-Nearest Neighbors Language Model (kNN-LM), Adaptive Retrieval: A lightweight neural network is learned to automatically prune unnecessary retrieval operations., Datastore Pruning: Techniques such as random pruning, k-means pruning, greedy merging, and rank-based pruning are used to reduce the datastore size., Dimension Reduction: Principal Component Analysis (PCA) is employed to reduce the dimensionality of context vectors.; Initialization: Not mentioned in detail.; Other: Interpolation with a k-NN distribution calculated from the k nearest context-token pairs in the datastore., Key function maps context sequence to a fixed-size vector.",,,,,,,,,,
176,AWD-FWM (WT2),"Imanol Schlag, Tsendsuren Munkhdalai, Jürgen Schmidhuber",2020/11/16,2020,Learning Associative Inference Using Fast Weight Memory,29.0,,https://arxiv.org/abs/2011.07831,3.70E+07,,,1600,,,0.0,2080000.0,7.39E+17,,2080000.0,WikiText-2,,61.65,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,33000,"Architecture: LSTM augmented with Fast Weight Memory (FWM); Optimizer: Adam, then switched to Averaged Stochastic Gradient Descent (ASGD); LR Schedule: Switched from Adam to ASGD after a certain number of epochs; Attention: Not Applicable; Regularization: dropout (various forms applied to LSTM), AWD-style regularization (for Transformer-XL); Special Algorithms: Fast Weight Memory (FWM); Initialization: embedding layers were initialized randomly from a uniform distribution uniform(-0.25, 0.25)",,,,,,,,,,
177,AWD-FWM (PTB),"Imanol Schlag, Tsendsuren Munkhdalai, Jürgen Schmidhuber",2020/11/16,2020,Learning Associative Inference Using Fast Weight Memory,29.0,,https://arxiv.org/abs/2011.07831,2.40E+07,,,1000,,,0.0,929000.0,1.34E+17,,929000.0,Penn TreeBank,,,54.48,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,10000,"Architecture: LSTM augmented with Fast Weight Memory (FWM). The slow weights of the LSTM are regular NN weights updated by gradient descent. The fast weights of the FWM are updated by the LSTM at every step of the input sequence.; Optimizer: Adam (Truncated BPTT used); LR Schedule: Linear learning rate warm-up over the first 1000 steps; Training: Truncated backpropagation through time (BPTT); Regularization: Dropout (multiple types including token, embedding, recurrent weights, hidden representation dropout); Special Algorithms: Fast Weight Memory (FWM); Initialization: Embedding layers initialized randomly from a uniform distribution, uniform(-0.25, 0.25)",,,,,,,,,,
178,VD-LSTM+REAL Small,"Hakan Inan, Khashayar Khosravi, Richard Socher",2016/11/04,2016,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,397.0,,https://arxiv.org/abs/1611.01462,6.08E+06,,,60,,,1.0,2080000.0,4.55E+15,,2080000.0,WikiText-2,,98.9,,0.0,0.0,0,0,Recurrent,LSTM,,,,,1,,,"Architecture: 2-layer LSTM; Optimizer: Stochastic Gradient Descent; LR Schedule: Start with a learning rate of 1 and decay it with a constant rate after a certain epoch.; Training: Gradient clipping, Variational dropout; Attention: N/A; Regularization: Dropout; Special Algorithms: Augmented loss with KL-divergence between the model's prediction and an estimated target distribution based on word embeddings, Tying word vectors and word classifiers by reusing the input word embedding matrix as the output classification matrix.; Initialization: N/A; Other: Temperature parameter is used in the augmented loss calculation, Unrolling the network for 35 steps for backpropagation",,,,,,,,,,
179,GPT-2 (1542M),"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",2019/02/14,2019,Language Models are Unsupervised Multitask Learners,6654.0,,https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,1.50E+09,,1.5e+21,20,,,0.0,4000000000.0,7.20E+20,,4000000000.0,WebText,17.48,18.34,35.76,1.0,1.0,0,0,Transformer,GPT,https://github.com/openai/gpt-2,,,,1,Own,50257,,,,,,,,,,,
180,retrieval-quality-kNN-LMs,"Andrew Drozdov, Shufan Wang, Razieh Rahimi, Andrew McCallum, Hamed Zamani, Mohit Iyyer",2022/10/28,2022,"You can’t pick your neighbors, or can you? When and how to rely on retrieval in the kNN-LM",7.0,,https://arxiv.org/pdf/2210.15859.pdf,2.47E+08,,,,,,0.0,103000000.0,0.00E+00,,103000000.0,,15.5,,,0.0,0.0,0,0,,,https://stanfordnlp.github.io/stanza/,University of Texas at Austin,Academia,,1,,,Architecture: The base model used is the self-attentive adaptive input representation from Baevski and Auli (2019).; Optimizer: Not specified; LR Schedule: Not specified; Attention: Self-attention is used as part of the Baevski and Auli (2019) base model.; Special Algorithms: k-Nearest Neighbors Language Model (kNN-LM); Initialization: Not specified; Other: Adaptive interpolation coefficient based on retrieval quality (semantic similarity),,,,,,,,,,
181,GPT-2 (762M),"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",2019/02/14,2019,Language Models are Unsupervised Multitask Learners,6654.0,,https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,7.62E+08,,7.62e+20,100,,,0.0,4000000000.0,1.83E+21,,4000000000.0,WebText,22.05,,,1.0,1.0,0,0,Transformer,GPT,https://github.com/openai/gpt-3,,,,1,Own,50257,,,,,,,,,,,
182,GPT-2 (345M),"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",2019/02/14,2019,Language Models are Unsupervised Multitask Learners,6654.0,,https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,3.45E+08,,3.45e+20,100,,,0.0,4000000000.0,8.28E+20,,4000000000.0,WebText,26.37,,,1.0,1.0,0,0,Transformer,GPT,https://github.com/openai/gpt-4,,,,1,Own,50257,,,,,,,,,,,
183,GPT-2 (117M),"Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever",2019/02/14,2019,Language Models are Unsupervised Multitask Learners,6654.0,,https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf,1.17E+08,,1.17e+20,100,,,0.0,4000000000.0,2.81E+20,,4000000000.0,WebText,37.5,,,1.0,1.0,0,1,Transformer,GPT,https://github.com/openai/gpt-5,,,,1,Own,50257,,,,,,,,,,,
184,GPT-3 175B (davinci),"Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei",2020/05/28,2020,Language Models are Few-Shot Learners,13822.0,,https://arxiv.org/abs/2005.14165,1.75E+11,,3.14e+23,0.601,,,0.0,499000000000.0,3.15E+23,,499000000000.0,CommonCrawl; WebText2; Books1; Books2; Wikipedia,,,20.5,1.0,1.0,0,1,Transformer,GPT,https://github.com/openai/gpt-3/,,,,1,Own,50257,"Architecture: Transformer, using alternating dense and locally banded sparse attention patterns in the layers of the transformer; Optimizer: Adam (β₁ = 0.9, β₂ = 0.95, and € = 10⁻⁸); LR Schedule: cosine decay for learning rate down to 10% of its value, over 260 billion tokens. Linear LR warmup over the first 375 million tokens.; Training: weight decay of 0.1, gradient clipping at 1.0, data are sampled without replacement during training to minimize overfitting.; Attention: Alternating dense and locally banded sparse attention patterns, similar to the Sparse Transformer.; Regularization: weight decay of 0.1; Initialization: modified initialization as in GPT-2",,,,,,,,,,
185,GLM-2B,"Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",2021/03/18,2021,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,131.0,,https://arxiv.org/abs/2103.10360,2.00E+09,,,1,,,0.0,632000000000.0,7.58E+21,,632000000000.0,,11.65,14.9,33.31,1.0,0.0,0,0,Transformer,GLM,https://github.com/THUDM/GLM,,,,1,,,"Architecture: Transformer with several modifications: (1) rearrange layer normalization and residual connection, (2) use a single linear layer for output token prediction, (3) replace ReLU with GeLUs; Optimizer: Adam; LR Schedule: Cosine decay; Attention: Masked self-attention. Employs mixed attention masks among bidirectional, unidirectional, and cross attention; Regularization: Dropout, Weight decay, Gradient clipping; Special Algorithms: Autoregressive Blank Infilling, 2D positional encodings, Span Shuffling",,,,,,,,,,
186,SPN-4,"W. Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, K. M. A. Chai",2014/01/01,2014,Language modeling with sum-product networks,102.0,,https://spn.cs.washington.edu/papers/is14.pdf,5.00E+06,,,UNK,,,0.0,1010000.0,#VALUE!,,1010000.0,Penn TreeBank,,,100.0,0.0,1.0,0,0,,,https://github.com/stakok/lmspn,,,,1,Word-level,10000,,,,,,,,,,,
187,LaMemo,"Haozhe Ji, Rongsheng Zhang, Zhenyu Yang, Zhipeng Hu, Minlie Huang",2022/04/15,2022,LaMemo: Language Modeling with Look-Ahead Memory,2.0,1.0,https://web.archive.org/web/20220418055451/https://arxiv.org/pdf/2204.07341.pdf,1.51E+08,1.03E+08,,79.53,,,0.0,103000000.0,7.42E+18,0.0,103000000.0,,23.77,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/thu-coai/LaMemo,,,,1,,260000,"Architecture: Transformer with LaMemo (Look-Ahead Memory) which enhances the recurrence memory by incrementally attending to the right-side tokens, and interpolating with the old memory states to maintain long-term information in the history.  The underlying Transformer is a multi-layer identical block Transformer with multi-head self-attention, residual connections, and layer normalization.; Optimizer: Adam; LR Schedule: Cosine decay to 0 at the end of training, no warmup; Training: Stop-gradient on previous segments in memory, Mixed-precision training; Attention: Multi-head self-attention. LaMemo incrementally attends to the right and accumulate the weighted attention sum from previous segments to simulate the full attention.; Regularization: Dropout (0.1), Weight decay (implicit through Adam's regularization); Special Algorithms: LaMemo: Look-Ahead Memory that attends to right-side tokens and interpolates with old memory., Disentangled Relative Positional Encoding: modification based on Dai et al. (2019) that disentangles the bias of the relative distance and the attention direction., Memory Interpolation: Smoothly interpolates attention results from the future and past to provide bi-directional contextualization.; Initialization: Sinusoidal encoding matrix is used for relative position encoding.  Learned weights for content and position bias.; Other: Adaptive embeddings",,,,,,,,,,
188,Engin-Base (NE),"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",2021/12/11,2021,Show and Write: Entity-aware Article Generation with Image Information,0.0,0.5,https://arxiv.org/pdf/2112.05917,1.24E+08,,,3,,,0.0,,0.00E+00,,0.0,,,20.7,,0.0,1.0,0,0,,,,,,,0,,,"Architecture: GPT2 architecture with modifications to incorporate named entities; Optimizer: Likely Adam, (default for fine-tuning in the implementation details, Paszke et al., 2019); LR Schedule: Linear warm-up for 0.06 epochs, then constant learning rate. Maximum learning rate of 1e-4 for base and medium models and 0.5 * 1e-4 for XL.; Attention: Multi-head attention from GPT2; Special Algorithms: Entity-aware Mechanism, Named-entity extraction; Initialization: Pretrained GPT2 weights; Other: Top-p sampling for decoding, Introduction of special tokens <start-> and <end-T> to indicate the boundaries of fields",,,,,,,,,,
189,Amended-DARTS,"Kaifeng Bi, Changping Hu, Lingxi Xie, Xin Chen, Longhui Wei, Qi Tian",2019/10/25,2019,Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters,48.0,,https://arxiv.org/pdf/1910.11831,2.30E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,54.8,0.0,0.0,0,0,NAS,DARTS,,"Stanford University; SambaNova Systems; Peking University; Adobe; University at Buffalo, SUNY",Industry - Academia Collaboration,,1,,,"Architecture: Cell-based architecture similar to DARTS with normal and reduction cells. The cells contain N nodes, with each node taking inputs from previous nodes. Each connection between nodes involves a set of pre-defined operators.; Optimizer: Adam optimizer for updating architectural parameters during search stage, SGD with momentum during re-training.; LR Schedule: During the search stage, a learning rate of 0.0003 for the Adam optimizer is used. During re-training, an initial learning rate of 0.025 is used, decaying with cosine annealing, and arriving at 0 after 600 epochs.; Training: Differentiable architecture search, Amended Gradient Estimation; Attention: Not applicable; Regularization: Weight decay of 0.001 is used with the Adam optimizer during search, Weight decay is set to 0.0003 during re-training, During re-training, some regularisation techniques are used, including Cutout, Dropout and auxiliary loss.; Special Algorithms: Amended Gradient Estimation - A modified second-order gradient estimation method to improve the stability of the search process and prevent convergence to dummy architectures.; Initialization: Equal weights assigned to each operator on each edge; Other: The core of Amended DARTS involves an 'amending coefficient' (eta) which weights the amended second-order gradient. The value of eta greatly affects the network architecture., Fixed edge vs Searched Edge experiments, Progressive approach to search process where we first determine the best edges and then determine the best operators given those fixed edges",,,,,,,,,,
190,RNN+LSA+KN5+cache (model combination w/ linear extrapolation),"Tomas Mikolov, Geoffrey Zweig",2012/12/01,2012,Context dependent recurrent neural network language model,716.0,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,3.14E+06,,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,Penn TreeBank,,,72.9,0.0,0.0,1,0,Recurrent,RNN,,,,,1,,,,,,,,,,,,,
191,LSTM-3-layer+Gadam,"Diego Granziol, Xingchen Wan, Samuel Albanie, Stephen Roberts",2020/03/02,2020,Iterative Averaging in the Quest for Best Test Error,5.0,,https://arxiv.org/pdf/2003.01247,2.40E+07,,,200,,,0.0,929000.0,2.68E+16,,929000.0,,,,58.77,0.0,1.0,0,0,Recurrent,LSTM,,Eindhoven University of Technologyl; University of Twente,Academia,,1,Word-level,10000,"Architecture: 3-layer LSTM; Optimizer: Gadam, AdamW, Padam, ASGD, SGD; LR Schedule: Linear decay, piece-wise constant schedule, step decay, constant LR for some experiments, a linear decay mechanism leading up to iterate averaging activation; Training: Iterate Averaging (IA), Stochastic Weight Averaging (SWA); Attention: N/A; Regularization: Weight decay (L2 regularization), decoupled weight decay; Special Algorithms: Gadam, GadamX, Lookahead; Initialization: Not explicitly mentioned, but base optimizers use default initialization; Other: Averaging starting point (Tavg):  A manual trigger to start averaging at the 100th epoch for ASGD. Tune Tavg for Gadam. An automatic trigger was also explored. Strided Iterate Average, Polynomial-style averaging",,,,,,,,,,
192,2-layer-LSTM+Deep-Gradient-Compression,"Yujun Lin, Song Han, Huizi Mao, Yu Wang, William J. Dally",2017/12/05,2017,Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training,1270.0,,https://arxiv.org/pdf/1712.01887,6.02E+06,,,40,,,0.0,929000.0,1.34E+15,,929000.0,,,,72.24,0.0,0.0,0,0,Recurrent,LSTM,https://github.com/synxlin/deep-gradient-compression,University of Liverpool; USC Information Sciences Institute,Academia,,1,,,"Architecture: 2-layer LSTM; Optimizer: Vanilla SGD and Nesterov momentum SGD; LR Schedule: Learning rate decays when no improvement is made in validation loss, Warm-up training used in the early stages of training.; Training: local gradient clipping, warm-up training; Attention: not applicable; Regularization: gradient sparsification, Momentum factor masking; Special Algorithms: Deep Gradient Compression (DGC); Initialization: not mentioned; Other: Momentum correction, Gradient Sparsification",,,,,,,,,,
193,MPT-7B,MosaicML NLP Team,2023/05/05,2023,"Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs",,,https://www.mosaicml.com/blog/mpt-7b,7.00E+09,,,1,,,0.0,1000000000000.0,4.20E+22,,1000000000000.0,,,9.96,,0.5,1.0,0,0,Transformer,MPT,,,,,1,https://arxiv.org/abs/2204.06745,50432,,,,,,,,,,,
194,AWD-LSTM+WT+Cache+IOG (PTB),"Sho Takase, Jun Suzuki, Masaaki Nagata",2017/09/26,2017,Input-to-Output Gate to Improve RNN Language Models,7.0,,https://arxiv.org/pdf/1709.08907,3.00E+07,,,5,,,0.0,929000.0,8.36E+14,,929000.0,,,,53.0,0.0,1.0,1,0,Recurrent,LSTM,https://github.com/nttcslab-nlp/iog,,,,1,Word-level,10000,"Architecture: LSTM (2-layer with 650 or 1500 dimensions), Variational RHN (depth 8), AWD-LSTM; Optimizer: Adam; LR Schedule: Learning rate decay 1/Epoch; Training: dropout, variational inference based dropout; Regularization: weight dropping (for AWD-LSTM), DropConnect (for AWD-LSTM); Special Algorithms: Input-to-Output Gate (IOG), Averaged Stochastic Gradient Descent (for AWD-LSTM); Other: Ensemble averaging of output probability distributions",,,,,,,,,,
195,AWD-LSTM+WT+Cache+IOG (WT2),"Sho Takase, Jun Suzuki, Masaaki Nagata",2017/09/26,2017,Input-to-Output Gate to Improve RNN Language Models,7.0,,https://arxiv.org/pdf/1709.08907,5.30E+07,,,5,,,0.0,2080000.0,3.31E+15,,2080000.0,,,51.7,,0.0,1.0,1,0,Recurrent,LSTM,https://github.com/nttcslab-nlp/iog,,,,1,Word-level,33000,"Architecture: LSTM with Input-to-Output Gate (IOG). The base LSTM can be Elman network, LSTM, Recurrent Highway Network (RHN) or other RNN variants. IOG is an additional gate function in the output layer of the selected RNN language model.; Optimizer: Adam; LR Schedule: Learning rate decay: 1/Epoch; Regularization: Dropout rate 50%; Special Algorithms: Input-to-Output Gate (IOG)",,,,,,,,,,
196,LSTM+NeuralCache,"Lyan Verwimp, Joris Pelemans, Hugo Van hamme, Patrick Wambacq",2018/09/24,2018,Information-Weighted Neural Cache Language Models for ASR,3.0,1.0,https://arxiv.org/pdf/1809.08826,2.10E+06,,,39,,,0.0,2080000.0,1.02E+15,,2080000.0,,,66.2,,0.0,1.0,1,0,Recurrent,LSTM,,,,,1,Word-level,33000,"Architecture: 1-layer LSTM with 512 LSTM cells; Optimizer: Stochastic Gradient Descent; LR Schedule: Exponential decay: lr = a * lr_prev, a = 0.8 after first 6 epochs. Initial learning rate is 1.0; Training: Backpropagation through time (35 steps), Gradient clipping (norm clipped at 5); Regularization: Dropout (50%); Special Algorithms: Information-Weighted Neural Cache, Information-Weighted Interpolation, Information-Weighted Selective Cache; Initialization: Weights are initialized using a uniform distribution between -0.05 and 0.05; Other: Discourse level training (LSTM state is not reset at sentence boundaries)",,,,,,,,,,
197,Stack RNN,"Armand Joulin, Tomas Mikolov",2015/03/03,2015,Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets,440.0,,https://arxiv.org/abs/1503.01007,2.01E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,118.0,0.0,1.0,0,0,Recurrent,RNN,"https://github.com/facebook/Stack-RNN, ",,,,1,?,?,"Architecture: Stack-Augmented Recurrent Neural Network (Stack RNN): RNN with an external memory module, specifically a pushdown stack (or multiple stacks) or a doubly-linked list.  The memory is controlled by learnable gating mechanisms (multiplicative gating) which dictate operations like PUSH and POP. Two basic topologies of the structured memory: pushdown stack and a list.; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: Initial learning rate of 0.1, divided by 2 each time the entropy on the validation set is not decreasing.; Training: Backpropagation through time (BPTT) with 50 steps, Hard clipping of gradients to 15 to prevent gradient explosions; Attention: Not applicable.; Initialization: Not mentioned.; Other: Search-based procedure on top of SGD for complex tasks, using random restarts., Discretizing controllers at test time to address numerical issues., Use of multiplicative gating mechanisms as learnable controllers over the memory",,,,,,,,,,
198,NAS+ESS (156M),"Yinqiao Li, Chi Hu, Yuhao Zhang, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li",2020/05/06,2020,Learning Architectures from an Extended Search Space for Language Modeling,12.0,,https://arxiv.org/pdf/2005.02593,1.56E+08,,,30,,,0.0,103000000.0,2.89E+18,0.0,103000000.0,,29.2,,,0.0,0.0,0,0,Recurrent,RNN,,Salesforce Resarch,Industry,,1,,,"Architecture: Recurrent neural network (RNN) based system with Extended Search Space (ESS) method. The ESS method learns both intra-cell and inter-cell architectures simultaneously using differentiable architecture search (DARTS).; Optimizer: Gradient-based optimization (using DARTS, which is a differentiable architecture search method); LR Schedule: Learning rate was set as 3 × 10-3 for the intra-cell architecture and 1 × 10-3 for the inter-cell architecture for PTB dataset. For WikiText-103 the intra-cell and inter-cell learning rate to 1 × 10-3 and 1 × 10-4.; Training: Differentiable Architecture Search (DARTS), Joint learning of intra-cell and inter-cell architectures; Attention: Not applicable; Special Algorithms: Extended Search Space (ESS) method for neural architecture search; Initialization: Not mentioned; Other: Backpropagation Through Time (BPTT) with length 35 for PTB and 70 for WikiText-103.",,,,,,,,,,
199,Sandwich Transformer,"Ofir Press, Noah A. Smith, Omer Levy",2019/11/10,2019,Improving Transformer Models by Reordering their Sublayers,57.0,,https://arxiv.org/abs/1911.03864,2.09E+08,,,180,,,0.0,700000000.0,1.58E+20,0.0,700000000.0,Toronto Books Corpus,17.84,,,1.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/ofirpress/sandwich_transformer,,,,1,BERT,29000,"Architecture: Transformer. Explores different sublayer orderings (interleaved, sandwich, random). Sandwich Transformer reorders sublayers placing self-attention layers at the bottom and feedforward layers at the top.; Optimizer: Same hyperparameters as Baevski and Auli's original model, details not specified in the paper.; LR Schedule: Same hyperparameters as Baevski and Auli's original model, details not specified in the paper.; Attention: Multi-head attention (16 heads) in encoder and decoder when used in machine translation.; Initialization: Not mentioned",,,,,,,,,,
200,LSTM(large)+Sememe+cell,"Yujia Qin, Fanchao Qi, Sicong Ouyang, Zhiyuan Liu, Cheng Yang, Yasheng Wang, Qun Liu, Maosong Sun",2019/10/20,2019,Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes,19.0,,https://arxiv.org/pdf/1910.08910,4.80E+07,,,40,,,0.0,2080000.0,2.40E+16,,2080000.0,,,85.76,,0.0,1.0,0,1,Recurrent,LSTM,https://github.com/thunlp/SememeRNN,,,,1,Word-level,33378,"Architecture: LSTM, GRU and their bidirectional variants. Three methods of incorporating sememes into RNNs are proposed: simple concatenation (+concat), adding sememe output gate (+gate) and introducing sememe-RNN cell (+cell).; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: The learning rate would be divided by 4 if no improvement is observed on the validation set. Initial learning rate is 20 for LSTM and 10 for GRU.; Regularization: dropout, gradient norm clip boundary is 0.25; Initialization: Word and sememe embeddings are randomly initialized as real-valued vectors using a normal distribution with mean 0 and variance 0.05.",,,,,,,,,,
201,GPT-Neo-1.3B,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021/03/21,2021,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,1.30E+09,,,1,,,0.0,400000000000.0,3.12E+21,,400000000000.0,,,13.1,,1.0,0.0,0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,,,,1,,,,,,,,,,,,,
202,LSTM(medium)+Sememe+cell,"Yujia Qin, Fanchao Qi, Sicong Ouyang, Zhiyuan Liu, Cheng Yang, Yasheng Wang, Qun Liu, Maosong Sun",2019/10/20,2019,Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes,19.0,,https://arxiv.org/pdf/1910.08910,1.00E+07,,,40,,,0.0,2080000.0,5.00E+15,,2080000.0,,,89.16,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/thunlp/SememeRNN,,,,1,Word-level,33378,"Architecture: LSTM (medium and large), GRU (medium and large) and their bidirectional variants; Optimizer: SGD; LR Schedule: Initial learning rate is 20 for LSTM and 10 for GRU. The learning rate would be divided by 4 if no improvement is observed on the validation set.; Attention: Not applicable; Regularization: gradient norm clip boundary 0.25, dropout (0.5 for medium, 0.65 for large); Special Algorithms: Simple Concatenation(+concat), Adding Sememe output gate(+gate), Introducing Sememe-RNN cell(+cell); Initialization: word and sememe embeddings are randomly initialized as real-valued vectors using a normal distribution with mean 0 and variance 0.05",,,,,,,,,,
203,Neural cache model (size=2000) (300M),"Edouard Grave, Armand Joulin, Nicolas Usunier",2016/12/13,2016,Improving Neural Language Models with a Continuous Cache,302.0,,https://arxiv.org/abs/1612.04426,3.00E+08,,,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-103,40.8,,,0.0,1.0,1,0,Recurrent,LSTM,,,,,1,Word-level,260000,"Architecture: LSTM with 1024 units; Optimizer: Adagrad; LR Schedule: constant; Training: dropout, truncated backpropagation through time; Regularization: dropout (probability of dropping out units equals to 0.65, 0.45 and 0.25, depending on the datasets); Special Algorithms: Neural Cache Model; Initialization: initial weight uniformly sampled in the range [-0.05, 0.05]; Other: gradient clipping (norm of the gradient to 0.1), adaptive softmax (for text8 and wikitext103)",,,,,,,,,,
204,TCN (148M),"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018/02/15,2018,Convolutional Sequence Modeling Revisited,64.0,,https://openreview.net/forum?id=rk8wKk-R-,1.48E+08,,,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-103,45.19,,,0.0,0.0,0,1,Convolutional,TCN,,,,,1,,,,,,,,,,,,,
205,LSTM (WT103),"Edouard Grave, Armand Joulin, Nicolas Usunier",2016/12/13,2016,Improving Neural Language Models with a Continuous Cache,302.0,,https://arxiv.org/abs/1612.04426,1.10E+07,,,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-103,48.7,,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,260000,"Architecture: 1024 LSTM units; Optimizer: Adagrad; LR Schedule: Not specified explicitly, but a fixed learning rate is used.; Training: gradient clipping, dropout, truncated backpropagation through time; Regularization: dropout with probability 0.65 (text8 and wikitext2), 0.45 (text8), 0.25 (wikitext103), weight initialization sampled uniformly in the range [-0.05, 0.05]; Initialization: initial weight uniformly sampled in the range [-0.05, 0.05]; Other: adaptive softmax (used for text8 and wikitext103)",,,,,,,,,,
206,ENAS,"Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean",2018/02/09,2018,Efficient Neural Architecture Search via Parameter Sharing,2760.0,,https://arxiv.org/abs/1802.03268,2.40E+07,,,150,,,0.0,929000.0,2.01E+16,,929000.0,Penn TreeBank,,,55.8,0.0,0.0,0,0,NAS,NAS,,Peking University,Academia,,1,,,"Architecture: ENAS discovers neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is an LSTM with 100 hidden units.; Optimizer: Adam optimizer for training the controller parameters (θ) and SGD for training the shared parameters of the child models (ω).; LR Schedule: For Penn Treebank, SGD with a learning rate of 20.0, decayed by a factor of 0.96 after every epoch starting at epoch 15, for a total of 150 epochs. For CIFAR-10, Nesterov momentum is used with the learning rate following the cosine schedule with Imax = 0.05, Imin = 0.001, To = 10, and Tmul = 2.; Training: REINFORCE for training the controller with a moving average baseline, Highway connections are used to augment the simple transformations between nodes in the constructed recurrent cell., Tanh constant of 2.5 and a temperature of 5.0 for the sampling logits, Adding the controller's sample entropy to the reward, weighted by 0.0001 to prevent premature convergence; Attention: Not applicable; Regularization: l2 regularization weighted by 10^-7, variational dropout, tying word embeddings and softmax weights, gradient clipping at 0.25 (Penn Treebank) or normalizing the gradient norm (CIFAR-10), l2 weight decay of 10^-4 (CIFAR-10), KL divergence to enforce skip connection sparsity (CIFAR-10); Special Algorithms: Efficient Neural Architecture Search (ENAS): Parameter sharing among child models by searching for an optimal subgraph within a large computational graph,, Controller trained with policy gradient to select a subgraph that maximizes the expected reward on a validation set., Child model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss.; Initialization: Controller parameters (θ) initialized uniformly in [-0.1, 0.1]. Shared parameters (ω) initialized uniformly in [-0.025, 0.025] during architecture search, and [-0.04, 0.04] when training a fixed architecture. CIFAR-10 shared parameters use He initialization.; Other: tanh constant and temperature are used for the sampling logits, Entropy of the controller's sampling is added to the reward., For CIFAR-10, KL divergence between skip connection probabilities is added to the reward.",,,,,,,,,,
207,LSTM (PTB),"Edouard Grave, Armand Joulin, Nicolas Usunier",2016/12/13,2016,Improving Neural Language Models with a Continuous Cache,302.0,,https://arxiv.org/abs/1612.04426,3.28E+07,,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,Penn TreeBank,,,82.3,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,10000,"Architecture: LSTM with 1024 units; Optimizer: Adagrad; LR Schedule: Not explicitly mentioned, but a learning rate of 0.2 is used; Training: Truncated Backpropagation Through Time, Gradient Clipping, Dropout; Attention: Not applicable; Regularization: Dropout with probability 0.65 (for small datasets), Dropout with probability 0.45 (for text8), Dropout with probability 0.25 (for wikitext103); Special Algorithms: Neural Cache Model; Initialization: Initial weights uniformly sampled in the range [-0.05, 0.05]; Other: Adaptive Softmax is used for faster training with large vocabularies",,,,,,,,,,
208,AWD-LSTM + Phrase Induction + finetuning,"Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass",2019/06/04,2019,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",12.0,,https://arxiv.org/abs/1906.01702,2.40E+07,,,,,,0.0,,0.00E+00,929000.0,929000.0,,,,55.7,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/luohongyin/PILM,,,,1,Word-level,10000,"Architecture: AWD-LSTM with a phrase induction module, formulated as a multi-layer architecture. A two-layer LSTM is used as an example, where the first layer is a phrase generator and the last layer is a word generator. Can be extended to L layers with the first L1 layers generating phrases and the next L2 = L - L1 generating words.; Optimizer: Stochastic Gradient Descent (SGD) and Averaged SGD (ASGD); LR Schedule: Not explicitly mentioned but inferred to vary as ASGD is used; Training: Variational dropout for hidden states, Weight dropout, Negative sampling for context-phrase alignment; Attention: Head-finding attention mechanism is used to calculate the embedding of each phrase.; Regularization: Context-phrase alignment loss used as a regularization term with a coefficient γ; Special Algorithms: Syntactic Height and Phrase Induction algorithm based on temporal convolutional networks (TCN) to predict induced phrases; Initialization: Not explicitly mentioned; Other: Word embedding tying strategy, Phrase segmenting conditions (PSC) to determine the last word of an induced phrase, HardTanh function to calculate probabilities",,,,,,,,,,
209,Transformer-XL Large + Phrase Induction,"Hongyin Luo, Lan Jiang, Yonatan Belinkov, James Glass",2019/06/04,2019,"Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",12.0,,https://arxiv.org/abs/1906.01702,2.57E+08,,,1,,,0.0,103000000.0,1.59E+17,103000000.0,206000000.0,WikiText-103,17.4,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/luohongyin/PILM,,,,1,Word-level,268000,"Architecture: Transformer-XL Large. Utilizes both a recurrent architecture and a multi-head attention mechanism.; Optimizer: Stochastic Gradient Descent (SGD) and averaged SGD (ASGD); LR Schedule: Not explicitly stated in the context provided. The base model paper should be consulted.; Training: variational dropout for hidden states, weight dropout, negative sampling; Attention: Multi-head attention mechanism is used in the base Transformer-XL architecture. Head-finding attention is also used for phrase embedding.; Regularization: weight dropout 0.5, context-phrase alignment loss as a regularization term (coefficient not specified); Special Algorithms: Phrase Induction based on syntactic heights, Head-Finding Attention; Initialization: Not explicitly stated in the context provided.; Other: Word embedding tying strategy, Temporal Convolutional Network (TCN) for calculating syntactic heights",,,,,,,,,,
210,Transformer + Average Attention Network,"Jian Guo Zhang, Jian Ping Li, Huang Li",2019/01/01,2019,Language Modeling with Transformer,126.0,,https://ieeexplore.ieee.org/abstract/document/9067534,UNK,,,,,,1.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-103,22.13,,,0.0,0.0,0,0,Transformer,Transformer,,NVIDIA,Industry,,0,,,,,,,,,,,,,
211,AdvSoft + 4 layer QRNN + dynamic evaluation,"Dilin Wang, Chengyue Gong, Qiang Liu",2019/06/10,2019,Improving Neural Language Modeling via Adversarial Training,95.0,,https://arxiv.org/abs/1906.03805,2.60E+07,,3.6e+17,,,,0.0,103000000.0,0.00E+00,,103000000.0,WikiText-103,28.0,,,0.0,1.0,0,0,Recurrent/Convolutional,QRNN,https://github.com/ChengyueGongR/advsoft,,,,1,Word-level,268000,"Architecture: AWD-LSTM (3 layer LSTM), QRNN (4 layer QRNN), Transformer (6-layer encoder and decoder for Transformer-Base, 6-layer encoder and decoder for Transformer-Big for WMT2014 De->En; 4-layer encoder and decoder for Transformer-Small and Transformer-Base for IWSLT2014 De->En); Optimizer: SGD, Averaged SGD (ASGD), Adam; LR Schedule: Follow the learning rate warm-up strategy in Vaswani et al. (2017); Training: Adversarial training on output embedding vectors in softmax layer, Weight tying, Gaussian Noise, Dynamic evaluation (post-processing); Regularization: Adversarial regularization; Special Algorithms: Adversarial Softmax (AdvSoft), Minimax training strategy for regularization, MoS (mixture of softmax)",,,,,,,,,,
212,Compress-LSTM (4.6M),"Artem M. Grachev, Dmitry I. Ignatov, Andrey V. Savchenko",2019/02/06,2019,Compression of Recurrent Neural Networks for Efficient Language Modeling,37.0,,"https://arxiv.org/abs/1902.02380#:~:text=Compression%20of%20Recurrent%20Neural%20Networks%20for%20Efficient%20Language%20Modeling,-Artem%20M.&text=Recurrent%20neural%20networks%20have%20proved,real%2Dtime%20offline%20mobile%20applications.",4.64E+06,,,90,,,0.0,929000.0,2.33E+15,,929000.0,,,,117.66,0.0,0.0,0,0,Recurrent,LSTM,,,,,1,,,"Architecture: LSTM networks. They compress the output layer and LSTM-layers separately.  Emphasis on matrix factorization compression techniques and Tensor-Train decomposition.; Optimizer: Adam optimizer is used initially. Then, the optimizer is switched to SGD (Stochastic Gradient Descent); LR Schedule: Explicit learning rate schedule is not specified, but randomized search is performed over learning rate and learning rate schedule.; Training: fine-tuning after pruning, early stopping based on validation perplexity; Attention: Not applicable; Regularization: unspecified conventional regularization techniques used to prevent overfitting, dropout (implicitly mentioned as part of hyperparameter tuning); Special Algorithms: Low-rank factorization, Tensor Train (TT) decomposition; Initialization: Not mentioned",,,,,,,,,,
213,AWD-LSTM + MoS + Partial Shuffled,"Dilin Wang, Chengyue Gong, Qiang Liu",2019/06/10,2019,Improving Neural Language Modeling via Adversarial Training,95.0,,https://arxiv.org/abs/1906.03805,3.50E+07,,,750,,,0.0,2080000.0,3.28E+17,,2080000.0,WikiText-2,,38.07,,0.0,1.0,0,0,Recurrent/Convolutional,QRNN,https://github.com/ChengyueGongR/advsoft,,,,1,Word-level,33000,"Architecture: AWD-LSTM: a three-layer LSTM is used for PTB and WT2.
QRNN: Quasi-Recurrent neural networks is used for WT103 dataset. 
Transformer-Base and Transformer-Big architectures are used for the machine translation task. Transformer-Base consists of a 6-layer encoder and a 6-layer decoder with 512-dimensional and 1024-dimensional hidden units per layer, respectively.
Transformer-Small consists of a 4-layer encoder and a 4-layer decoder with 256-dimensional hidden units per layer; Optimizer: SGD and averaged SGD (ASGD); Adam for Machine Translation; LR Schedule: Learning rate warm-up strategy in Vaswani et al. (2017) for machine translation; Training: Adversarial training with perturbation on the output embedding vectors, Weight tying trick; Attention: N/A; Regularization: Adversarial regularization, Gaussian noise (annealed) in the input embedding layer; Special Algorithms: Minimax training strategy for regularization, involving injecting adversarial perturbation on the word embedding vectors in the softmax layer.; Initialization: N/A; Other: Mixture of softmax (MoS) is used in conjunction with AWD-LSTM to break the softmax bottleneck, Dynamic evaluation for perplexity calculation",,,,,,,,,,
214,4 layer QRNN + dynamic evaluation,"Dilin Wang, Chengyue Gong, Qiang Liu",2019/06/10,2019,Improving Neural Language Modeling via Adversarial Training,95.0,,https://arxiv.org/abs/1906.03805,2.60E+07,,3.6e+17,,,,0.0,103000000.0,0.00E+00,,103000000.0,WikiText-103,31.6,,,0.0,1.0,0,0,Recurrent/Convolutional,QRNN,https://github.com/ChengyueGongR/advsoft,,,,1,Word-level,268000,"Architecture: 4-layer QRNN for WT103, 3-layer LSTM (AWD-LSTM) for PTB and WT2; Optimizer: SGD and averaged SGD (ASGD) for AWD-LSTM, Adam for machine translation; LR Schedule: Learning rate warm-up strategy (for machine translation, Transformer); Training: Adversarial training, Weight tying (input and output embeddings); Attention: N/A; Regularization: Gaussian noise in the input embedding layer, Dropout, Activation Regularization, Layer Normalization; Special Algorithms: Adversarial MLE training; Initialization: N/A; Other: Mixture of Softmax (MoS)",,,,,,,,,,
215,Temporal Convolutional Attention-based Network(TCAN) (WT2),"Hongyan Hao, Yan Wang, Yudi Xia, Jian Zhao, Furao Shen",2020/02/28,2020,Temporal Convolutional Attention-based Network For Sequence Modeling,33.0,,https://arxiv.org/pdf/2002.12530,3.30E+07,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,,,6.66,,0.0,0.0,0,0,,,https://github.com/haohy/TCAN,,,,1,,,"Architecture: Temporal Convolutional Attention-based Network (TCAN) which combines temporal convolutional network and attention mechanism. It includes Temporal Attention (TA) and Enhanced Residual (ER) modules. TA captures relevant features inside the sequence, while ER extracts the shallow layer's important information and transfers it to deep layers. Dilated causal convolutions are used in the hidden layers. Skip connections are implemented through ER.; Optimizer: Adam; LR Schedule: Constant learning rate of 0.0001; Training: gradient clipping; Attention: Self-attention mechanism with modification to satisfy sequential nature/causality. Dot product attention. The input is mapped to queries, keys and values via linear transformation. The weights matrix excludes future time steps to prevent information leakage; Initialization: Not mentioned explicitly",,,,,,,,,,
216,SparseOPT-66B,"Elias Frantar, Dan Alistarh",2023/01/02,2023,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,8.0,,https://arxiv.org/abs/2301.00774,3.30E+10,,,1.666666667,,,1.0,180000000000.0,5.94E+22,0.0,180000000000.0,,9.32,,,1.0,0.0,0,0,Transformer,OPT,https://github.com/IST-DASLab/sparsegpt,,,,1,,,"Architecture: Transformer based language models (GPT family); Optimizer: Not mentioned (one-shot pruning, so no explicit optimizer is used); LR Schedule: Not mentioned (one-shot pruning, so no explicit learning rate schedule is used); Training: One-shot pruning (no retraining or fine-tuning); Attention: Not explicitly mentioned, but assumed to be standard multi-head attention from the base Transformer architecture; Regularization: Not mentioned; Special Algorithms: SparseGPT: A pruning method which reduces the pruning problem to a set of extremely large-scale instances of sparse regression. It then solves these instances via a new approximate sparse regression solver., Iterative OBS updates; Initialization: Weights are initialized according to the pre-trained base model (OPT); Other: Layer-wise pruning, Adaptive mask selection, Hessian synchronization, Weight freezing interpretation (column-wise greedy scheme), Extension to Semi-Structured Sparsity",,,,,,,,,,
217,Adversarial + AWD-LSTM-MoS + partial shuffled,"Dilin Wang, Chengyue Gong, Qiang Liu",2019/06/10,2019,Improving Neural Language Modeling via Adversarial Training,95.0,,https://arxiv.org/abs/1906.03805,2.20E+07,,,450,,,0.0,923000.0,5.48E+16,,923000.0,Penn TreeBank,,,46.01,0.0,1.0,0,0,Recurrent/Convolutional,QRNN,https://github.com/ChengyueGongR/advsoft,,,,1,?,?,"Architecture: AWD-LSTM (3-layer LSTM) for PTB and WT2, Quasi-Recurrent Neural Networks (QRNN) for WT103, Transformer for machine translation; Optimizer: SGD and averaged SGD (ASGD) for AWD-LSTM, Adam for Transformer; LR Schedule: Learning rate warm-up strategy for Transformer, Restarting ASGD for fine-tuning; Training: Minimax training strategy, Adversarial training by injecting adversarial perturbation on the word embedding vectors in the softmax layer; Attention: Not applicable, although transformer architecture is used for machine translation; Special Algorithms: Adversarial Softmax; Initialization: Not mentioned; Other: Weight-tying trick",,,,,,,,,,
218,TransformerXL + spectrum control,"Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, Quanquan Gu",2020/03/11,2020,Improving Neural Language Generation with Spectrum Control,55.0,,https://openreview.net/forum?id=ByxY8CNtvr,1.51E+08,,4.6e+17,250,,,0.0,103000000.0,2.33E+19,0.0,103000000.0,WikiText-103,23.2,,,0.0,1.0,0,0,Transformer,Transformer-XL,,,,,1,Word-level,260000,,,,,,,,,,,
219,4 layer Densely Connected LSTM,"Fréderic Godin, Joni Dambre, Wesley De Neve",2017/07/19,2017,Improving Language Modeling using Densely Connected Recurrent Neural Networks,7.0,,https://arxiv.org/pdf/1707.06130,1.40E+07,,,100,,,0.0,929000.0,7.80E+15,,929000.0,,,,76.8,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,10000,"Architecture: Densely connected LSTM with a variable number of LSTM layers (2-5). Skip connections are added between every pair of layers. Embedding layer followed by LSTM layers and a fully connected output layer.; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: Training for six epochs with a learning rate of one and then applying a decay factor of 0.95 every epoch.; Training: early stopping; Attention: N/A; Regularization: standard dropout on the output of every layer, gradient clipping (norm of the gradient constrained to three); Special Algorithms: Densely connected layers (skip connections between every pair of layers); Initialization: Weights initialized uniformly in the interval [-0.05;0.05]",,,,,,,,,,
220,Densely Connected LSTM + Var. Dropout,"Fréderic Godin, Joni Dambre, Wesley De Neve",2017/07/19,2017,Improving Language Modeling using Densely Connected Recurrent Neural Networks,7.0,,https://arxiv.org/pdf/1707.06130,2.30E+07,,,100,,,0.0,929000.0,1.28E+16,,929000.0,,,,78.3,0.0,1.0,0,0,Recurrent,,,,,,1,Word-level,10000,"Architecture: Densely connected LSTM network where every input of every layer is connected with every output of every other layer using skip connections. Baseline is a stacked LSTM.; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: Training for six epochs with a learning rate of one and then applying a decay factor of 0.95 every epoch.; Training: Early stopping, Batch size of 20, Sequence length of 35; Attention: N/A; Regularization: Dropout on the output of every layer (dropout probability of 0.6 for models with size 200 and 0.75 for models with hidden state size 650). Gradient clipping (norm constrained to three); Initialization: Weights initialized uniformly in the interval [-0.05;0.05]; Other: Skip connections",,,,,,,,,,
221,AWD-LSTM-MoS+PDR + dynamic evaluation (PTB),Siddhartha Brahma,2018/08/14,2018,Improved Language Modeling by Decoding the Past,5.0,,https://arxiv.org/abs/1808.05908,2.20E+07,,,1200,,,0.0,888000.0,1.41E+17,,888000.0,Penn TreeBank,,,47.3,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,10000,"Architecture: 3-layer LSTM with 1150, 1150, and 400 hidden dimensions for single softmax model and 960, 960 and 620 for mixture-of-softmaxes on PTB; 3-layer LSTM with 1150, 1150 and 650 for mixture-of-softmaxes on WT2. For PTBC, a 3-layer LSTM with 1000, 1000 and 200 hidden dimensions. For Enwik8, a LSTM with 1850, 1850 and 400 hidden dimensions.; Optimizer: Combination of SGD and NT-ASGD, followed by finetuning; LR Schedule: Adopted from original AWD-LSTM and AWD-LSTM-MoS; Attention: Not applicable; Regularization: Variational dropout on token embeddings, Dropout on hidden-to-hidden weights in LSTM, Norm regularization on the outputs of the LSTM, Classical dropout, Past Decode Regularization (PDR); Special Algorithms: Past Decode Regularization (PDR); Initialization: Not mentioned; Other: Weight tying",,,,,,,,,,
222,KnGPT2,"Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Partovi Nia, James J. Clark, Mehdi Rezagholizadeh",2021/10/15,2021,Kronecker Decomposition for GPT Compression,14.0,,https://web.archive.org/web/20221111092612/https://arxiv.org/pdf/2110.08152.pdf,8.30E+07,4.40E+09,1.24e+20,1,,,0.0,4000000000.0,1.99E+18,400000000.0,4400000000.0,,20.5,,,1.0,0.0,0,0,Transformer,GPT,,University of Lugano; King Abdullah University of Science and Technology,Academia,,1,,,"Architecture: Transformer-based GPT-2 model, with modifications to compress embedding and transformer layers using Kronecker decomposition.  The linear layers of multi-head attention (MHA) and feed-forward network (FFN) blocks are decomposed into Kronecker layers. In half of the layers, Kronecker Decomposition is not applied.; Optimizer: Not explicitly stated in the abstract or introduction. However, based on hyper-parameters used (Table 2), it is likely AdamW, but this needs to be confirmed within the rest of the paper.; LR Schedule: For pre-training and fine-tuning a constant learning rate is used, as can be seen from Table 2.; Training: Knowledge Distillation (KD), Intermediate Layer Knowledge Distillation (ILKD); Attention: Multi-head attention (MHA) within the Transformer architecture.; Special Algorithms: Kronecker decomposition for compressing linear layers; Initialization: Kronecker factors A and B are estimated from the corresponding weight matrix W in the original uncompressed pre-trained model using the solution to the nearest Kronecker problem by rank-1 singular value decomposition (SVD).",,,,,,,,,,
223,AWD-LSTM-MoS+PDR + dynamic evaluation (WT2),Siddhartha Brahma,2018/08/14,2018,Improved Language Modeling by Decoding the Past,5.0,,https://arxiv.org/abs/1808.05908,3.50E+07,,,,,,0.0,2050000.0,0.00E+00,,2050000.0,WikiText-2,,40.3,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,33000,"Architecture: 3-layer LSTM with varying hidden dimensions (1150, 1150, 400 for single softmax, and 960, 960, 620 or 1150, 1150, 650 for mixture-of-softmaxes); Optimizer: Combination of SGD and NT-ASGD, followed by fine-tuning; LR Schedule: Adopted from AWD-LSTM, details found in Merity et al. (2018a) and Yang et al. (2017).; Attention: Not applicable; Regularization: Variational dropout on token embeddings, Dropout on hidden-to-hidden weights in the LSTM, Norm regularization on the outputs of the LSTM, Classical dropout, Past Decode Regularization (PDR); Special Algorithms: Past Decode Regularization (PDR): a regularization method based on decoding the last token in the context using the predicted distribution of the next token.; Initialization: Not mentioned",,,,,,,,,,
224,Hyena-3-slim,"Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, Christopher Ré",2023/02/21,2023,Hyena Hierarchy: Towards Larger Convolutional Language Models,21.0,,https://arxiv.org/pdf/2302.10866,1.25E+08,,,UNK,,,0.0,103000000.0,#VALUE!,,103000000.0,,18.5,,,0.0,1.0,0,0,Transformer,Hyena,,,,,1,GPT2Tokenizer,50257,"Architecture: Hyena: a recurrence of implicit long convolutions and element-wise multiplicative gating. The convolutions are parametrized using a shallow feed-forward neural network (FFN) applied to a positional encoding of the time step. Order-N Hyena operator is defined by a recurrence with N learnable filters.; Optimizer: AdamW (β1, β2 = 0.9, 0.98); LR Schedule: cosine decay; Training: positional encoding, FFT convolution for efficiency, weight decay, dropout, data augmentations such as RandAugment, Mixup, and AugMix for ImageNet; Attention: N/A; Regularization: weight decay, dropout; Special Algorithms: implicit parameterization of long convolutions with shallow FFNs, Hyena recurrence; Initialization: Truncated normal for weights; Other: Causal convolutions, Window function applied to the filters",,,,,,,,,,
225,GPT-2 (fine-tuned with HYDRA),"Kabir Nagrecha, Arun Kumar",2021/10/16,2021,Hydra: A System for Large Multi-Model Deep Learning,4.0,0.0,https://arxiv.org/abs/2110.08633,1.54E+09,,,1,,,0.0,2080000.0,1.92E+16,,2080000.0,WikiText-2,,15.17,,0.0,1.0,0,0,Transformer,GPT,,,,,1,GPT2Tokenizer,50257,"Architecture: Transformer; Optimizer: Grid search used to compare different learning rates and optimizers.; LR Schedule: Grid search used to compare different learning rates and optimizers. Learning rates used {0.0003, 0.0001, 0.00005, 0.00006, 0.00001, 0.00002}; Training: Gradient checkpointing, Spilled execution scheme: model is sharded, and shards are promoted/demoted between GPU memory and DRAM, Shard Alternator Parallelism (SHARP); Special Algorithms: Shard Alternator Parallelism (SHARP), Sharded-LRTF",,,,,,,,,,
226,Hybrid H3-355M,"Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",2022/12/28,2022,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,27.0,,https://arxiv.org/pdf/2212.14052,3.55E+08,,,509.02,,,0.0,103000000.0,1.12E+20,,103000000.0,,16.9,,,0.0,1.0,0,0,State Space Model,H3,https://github.com/HazyResearch/H3,,,,1,GPT 2 tokenizer,50257,"Architecture: Hybrid H3-attention model with H3 layers interleaved with MLPs, connected by residual connections and layer norm (pre-norm architecture). H3 layer stacks two discrete SSMs with shift and diagonal matrices. The number of heads varies depending on the model size: 125M (12 heads), 355M (16 heads), 1.3B (16 heads), and 2.7B (20 heads); Optimizer: AdamW; LR Schedule: Cosine schedule with linear warmup (8000 steps), decay the learning rate to 10% by 300B tokens, then continue training at 10% learning rate for another 100B tokens (only for models trained on the Pile); Training: Mixed precision training (bf16 for MLPs and attention, fp32 for FFTConv), Gradient accumulation (used to fit into GPU memory); Attention: Self-attention layers (two layers retained in hybrid H3-attention model) at different positions depending on the model size. For the 125M hybrid model, the attention layers were at layers 1 and 7, the 355M and 1.3B hybrid models had attention layers at layers 1 and 13, and the 2.7B hybrid model had attention layers at layers 10 and 21.; Regularization: Weight decay 0.1, Residual dropout 0.0 (only used for models trained on the Pile), Embedding dropout 0.1 (only used for models trained on the Pile); Special Algorithms: FLASHCONV: Hierarchical algorithm for computing SSMs, inspired by IO-Aware attention, Block FFT, State-passing algorithm; Initialization: Diagonal version of HiPPO for the diagonal SSM (S4D initialization); Other: Discrete SSM (based on shift matrix), Multiplicative interaction (between output projections and inputs)",,,,,,,,,,
227,Hybrid H3-125M,"Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",2022/12/28,2022,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,27.0,,https://arxiv.org/pdf/2212.14052,1.25E+08,,,509.02,,,0.0,103000000.0,3.93E+19,,103000000.0,,23.7,,,0.0,1.0,0,0,State Space Model,H3,https://github.com/HazyResearch/H3,,,,1,GPT 2 tokenizer,50257,"Architecture: Hybrid H3-attention model. H3 stacks two discrete SSMs with shift and diagonal matrices, along with multiplicative operations against projections of the input. It interleaves H3 layers with MLPs, connected by residual connection and layer norm (pre-norm architecture).; Optimizer: AdamW; LR Schedule: cosine schedule with linear warmup, decay the learning rate to 10% by a certain number of tokens, then continue training at 10% learning rate for another a certain number of tokens.; Training: mixed-precision training, gradient accumulation, kernel fusion, block FFT, state passing algorithm, Layer Normalization, Residual Connection, mixed-precision training with bf16 for the MLPs and attention. fp32 for the FFTConv; Attention: Hybrid model retains two self-attention layers; Regularization: weight decay, embedding dropout, residual dropout, L2 regularization, dropout; Special Algorithms: FLASHCONV, State Passing Algorithm; Initialization: Diagonal SSM constrains A to be diagonal and initializes it from the diagonal version of HiPPO (S4D).; Other: H3 Layer: discrete SSM (based on shift matrix) and multiplicative interaction",,,,,,,,,,
228,Hybrid H3-2.7B,"Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",2022/12/28,2022,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,27.0,,https://arxiv.org/pdf/2212.14052,2.70E+09,,,509.02,,,0.0,103000000.0,8.49E+20,,103000000.0,,10.6,,,0.0,1.0,0,0,State Space Model,H3,https://github.com/HazyResearch/H3,,,,1,GPT 2 tokenizer,50257,"Architecture: Hybrid H3-attention model: H3 layers interleaved with MLPs, connected by residual connection and layer norm (i.e., pre-norm architecture). Two attention layers are retained in the hybrid model.; Optimizer: AdamW; LR Schedule: Cosine schedule with linear warmup.; Training: Mixed-precision training (bf16 for MLPs and attention, fp32 for FFTConv), Gradient accumulation, Kernel Fusion, Block FFT, State-passing algorithm; Attention: Self-attention layers (number of heads varies by model size); Regularization: Weight decay, Embedding dropout, Residual dropout, L2-regularisation; Special Algorithms: H3 layer: stacks two discrete SSMs with shift and diagonal matrices, with multiplicative interactions between their outputs and input projections.; Initialization: Diagonal version of HiPPO (S4D) for the diagonal SSM in the H3 layer.; Other: FLASHCONV: a hierarchical algorithm for computing SSMs inspired by IO-Aware attention.",,,,,,,,,,
229,Hybrid H3-1.3B,"Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher Ré",2022/12/28,2022,Hungry Hungry Hippos: Towards Language Modeling with State Space Models,27.0,,https://arxiv.org/pdf/2212.14052,1.30E+09,,,509.02,,,0.0,103000000.0,4.09E+20,,103000000.0,,12.5,,,0.0,1.0,0,0,State Space Model,H3,https://github.com/HazyResearch/H3,,,,1,GPT 2 tokenizer,50257,"Architecture: Hybrid H3-attention model. H3 stacks two discrete SSMs with shift and diagonal matrices, and uses multiplicative interactions between input projections and their outputs. Interleaved with MLPs, connected by residual connection and layer norm (i.e., pre-norm architecture). Keeps two self-attention layers in some configurations.; Optimizer: AdamW; LR Schedule: Cosine schedule with linear warmup and decay to 10% of the initial rate.; Training: Mixed-precision training (bf16 for MLPs and attention, fp32 for FFTConv), Gradient accumulation; Attention: Self-attention layers in hybrid models.; Regularization: Weight decay 0.1, Residual dropout 0.0, Embedding dropout 0.1 (used in some configurations), Dropout rate varied 0.1-0.3 for some fMRI data configurations; Special Algorithms: FLASHCONV: Hierarchical algorithm for computing SSMs. Uses fused block FFTConv inspired by IO-Aware attention. New state-passing algorithm over fused block FFTConv to increase hardware efficiency., Block FFT; Initialization: Diagonal version of HiPPO (S4D) for diagonal SSMs.; Other: Use of layer normalization (pre-norm architecture)",,,,,,,,,,
230,DOT(S)-RNN,"Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio",2013/12/20,2013,How to Construct Deep Recurrent Neural Networks,1255.0,,https://arxiv.org/pdf/1312.6026.pdf,6.16E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,107.5,0.0,1.0,0,0,Recurrent,RNN,,,,,1,Word-level,10000,"Architecture: Recurrent Neural Network (RNN) with deep variants: Deep Transition RNN (DT-RNN), Deep Transition RNN with shortcut connections (DT(S)-RNN), Deep Output, Deep Transition RNN (DOT-RNN), Stacked RNN (sRNN). The deep transition means that the hidden-to-hidden transition is modeled by an MLP.; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: Learning rate schedule tuned manually for each dataset, decreasing learning rate when the validation error starts increasing.  In language modeling, the learning rate is halved each time the validation cost does not decrease significantly.; Training: Backpropagation Through Time, Gradient Clipping; Attention: Not applicable; Regularization: Adding White Gaussian noise to each weight parameter., Sparse connections between hidden layers (20 non-zero connections per unit); Special Algorithms: Operator-based framework for building RNNs; Initialization: Weights initialized randomly from a white Gaussian distribution, with standard deviation 0.1 (or 0.01 depending on the layer), biases initialized to 0. Weights of connections between any pair of hidden layers are sparse, with unit largest singular value.; Other: Rescaling each weight matrix to have a unit largest singular value",,,,,,,,,,
231,Gated HORNN (3rd order),"Rohollah Soltani, Hui Jiang",2016/04/30,2016,Higher Order Recurrent Neural Networks,77.0,,https://arxiv.org/pdf/1605.00064,8.97E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,100.0,0.0,1.0,0,0,Recurrent,RNN,,,,,1,,10000,"Architecture: Higher Order Recurrent Neural Network (HORNN) with different orders (2nd, 3rd, 4th) and different pooling functions (Max-based, FOFE-based, Gated); combines multiple preceding RNN states as feedback; recurrently fed to the hidden layers as feedback through different weighted paths; N-th order HORNN updates its state recursively as: ht = f(Winxt + Σ(Whnht-n)); Optimizer: mini-batch Stochastic Gradient Descent (SGD); LR Schedule: Initial learning rate 0.5, halved at the end of each epoch if the cross-entropy function on the validation set does not decrease; The learning schedule used in (Mikolov et al., 2014): First initialize the learning rate to 0.5 and run 5 epochs using this learning rate; After that, the learning rate is halved at the end of every epoch; Training: back-propagation through time (BPTT); Attention: Soft-attention used in Gated HORNNs; Regularization: hard clipping to 5.0, weight decay, momentum, column normalization (Pachitariu & Sahani, 2013); Special Algorithms: Max-based pooling, FOFE-based pooling, Gated pooling (Gated HORNNs): sigmoid gates to compute combination weights to regulate information flowing from various feedback paths, gate signal rn = σ (Winxt + Wanht-n); Initialization: All model parameters (weight matrices in all layers) are randomly initialized based on a Gaussian distribution with zero mean and standard deviation of 0.1.",,,,,,,,,,
232,GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (PTB),"Ziv Aharoni, Gal Rattner, Haim Permuter",2017/08/29,2017,Gradual Learning of Recurrent Neural Networks,4.0,1.0,https://arxiv.org/abs/1708.08863,2.60E+07,,,1000,,,0.0,929000.0,1.45E+17,,929000.0,Penn TreeBank,,,46.34,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,10000,"Architecture: LSTM (3-layer used as reference model); Optimizer: Not specified, but reference is made to implementation details by Yang et al. [2017]; LR Schedule: Not explicitly described, but implied to follow settings similar to Yang et al. [2017] which is the baseline implementation; Training: Gradual Learning (GL) - incrementally training the network by adding layers, Layer-wise Gradient Clipping (LWGC) - clipping gradient norm layer-wise; Attention: Not applicable; Regularization: variational dropout, Weight Tying; Special Algorithms: Gradual Learning (GL), Layer-wise Gradient Clipping (LWGC); Initialization: Layers are initialized with weights learned in the previous phase of GL. New layers are initialized randomly.; Other: Dynamic evaluation (optional), Mixture of Softmaxes",,,,,,,,,,
233,GL-LWGC-AWD-MoS-LSTM + dynamic evaluation (WT2),"Ziv Aharoni, Gal Rattner, Haim Permuter",2017/08/29,2017,Gradual Learning of Recurrent Neural Networks,4.0,1.0,https://arxiv.org/abs/1708.08863,3.80E+07,,,1000,,,0.0,2080000.0,4.74E+17,,2080000.0,WikiText-2,,40.46,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,33000,"Architecture: AWD-MoS-LSTM (3-layer when final), where layers can be gradually added; Optimizer: Not explicitly stated, but Yang et al. [2017] (used as base) used SGD and Adam; LR Schedule: Not explicitly stated, uses the settings in the implementation of Yang et al. [2017]; Training: Gradual Learning (GL): training the network gradually by adding layers, Mixture of Softmaxes; Attention: Not applicable; Regularization: Layer-wise Gradient Clipping (LWGC), Weight Tying (WT), Variational Dropout; Special Algorithms: Dynamic evaluation; Initialization: Layers are initialized randomly, except for layers copied from previous phases in GL. Softmax layer weights can be initialized randomly or inherited from the previous training phase.",,,,,,,,,,
234,GPT-NeoX-20B,"Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, Samuel Weinbach",2022/04/14,2022,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,235.0,,https://arxiv.org/abs/2204.06745,2.00E+10,,,1,,,1.0,473000000000.0,5.67E+22,,473000000000.0,,,9.2,,1.0,1.0,0,0,Transformer,GPT,https://github.com/EleutherAI/gpt-neox,,,,1,Pretrained GPT2 tokenizer,50257,"Architecture: Autoregressive transformer decoder model largely following the architecture of GPT-3, with 44 layers, a hidden dimension size of 6144, and 64 heads.; Optimizer: AdamW optimizer with beta values of 0.9 and 0.95 respectively, and an epsilon of 1.0E-8. Extended with the ZeRO optimizer; LR Schedule: Cosine schedule, decaying the learning rate to 10% of its original value at the end of training, 150,000 steps; Training: tensor parallelism, pipeline parallelism; Attention: Multi-head attention, using rotary positional embeddings applied to the first 25% of embedding vector dimensions; Regularization: weight decay of 0.01, hidden dropout 0; Special Algorithms: ZeRO optimizer; Initialization: Used the initialization scheme introduced in Wang (2021) for Feed-Forward output layers before residuals; used small init scheme from Nguyen and Salazar (2019) for all other layers; Other: parallel computation of Attention and Feed-Forward layers, All dense layers",,,,,,,,,,
235,GPT-Neo-2.7B (finetuned on PTB),"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021/03/21,2021,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,2.70E+09,,,1,,,0.0,400000000000.0,6.48E+21,929000.0,400000000000.0,,,,14.7,0.0,1.0,0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,,,,1,,50257,,,,,,,,,,,
236,GPT-Neo-2.7B (finetuned),"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021/03/21,2021,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,2.70E+09,,,1,,,0.0,400000000000.0,6.48E+21,2080000.0,400000000000.0,,,10.78,,0.0,1.0,0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,,,,1,,50257,,,,,,,,,,,
237,GPT-Neo-125M(finetuned),"Michael Santacroce, Zixin Wen, Yelong Shen, Yuanzhi Li",2021/03/21,2021,What Matters In The Structured Pruning of Generative Language Models?,1.0,1.0,https://arxiv.org/pdf/2302.03773.pdf,1.25E+08,,,40,,,1.0,300000000000.0,9.00E+21,,300000000000.0,,16.14,,,0.0,0.0,0,0,Transformer,GPT-Neo,https://github.com/santacml/nn_pruning_uniqueness,,,,1,,,"Architecture: Transformer decoder-only; Optimizer: Adam; LR Schedule: Learning weight decayed linearly to 0; Training: Fine-pruning, Knowledge Distillation, L1 regularization on mask scores; Attention: Not specified in detail, but Transformer architecture implies multi-head self-attention; Regularization: Weight decay, L1 regularization on mask scores; Special Algorithms: Globally Unique Movement (GUM), Movement Pruning, Global Top; Initialization: Not explicitly mentioned, but assumed to be standard Transformer initialization; Other: Automated Gradual Pruning",,,,,,,,,,
238,GPT-Neo-125M(finetuned),"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021/03/21,2021,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,1.25E+08,,,1,,,0.0,400000000000.0,3.00E+20,2080000.0,400000000000.0,,,21.96,,0.0,1.0,0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,,,,1,,50257,,,,,,,,,,,
239,Neural Architecture Search with base 8 and shared embeddings,"Barret Zoph, Quoc V. Le",2016/11/05,2016,Neural Architecture Search with Reinforcement Learning,5473.0,,https://arxiv.org/abs/1611.01578,5.40E+07,,,35,,,0.0,929000.0,1.05E+16,,929000.0,Penn TreeBank,,,62.4,0.0,0.0,0,0,Recurrent,RNN,https://github.com/tensorflow/models,Baidu,Industry,,1,,,"Architecture: The controller is a 2-layer LSTM with 35 hidden units on each layer that generates the architectures of neural networks. The generated architectures are trained as ""child networks"". For language modeling, the child network has two layers, with number of hidden units adjusted to approximately match the size of the 'medium' baselines in Zaremba et al., 2014 and Gal, 2015.; Optimizer: ADAM with learning rate 0.0006 for the controller RNN; Momentum Optimizer with learning rate of 0.1, weight decay of 1e-4 and momentum of 0.9 for child networks.; LR Schedule: Schedule of increasing the number of layers in the child networks as training progresses. In the Penn Treebank experiment, the learning rate for controller RNN is 0.0005.; Training: REINFORCE policy gradient method, Asynchronous parameter updates, Distributed training; Attention: Set-selection type attention is used to enable skip connections (based on Bahdanau et al., 2015; Vinyals et al., 2015).  At layer N, an anchor point is added with N-1 content-based sigmoids to indicate the previous layers that need to be connected.; Regularization: Embedding dropout, Recurrent dropout, Weight decay of 1e-4 for CIFAR-10 child networks, Weight decay and Dropout rates (Penn Treebank); Special Algorithms: Neural Architecture Search (NAS) - uses a recurrent network (controller) trained with reinforcement learning (REINFORCE) to generate neural network architectures (child networks); Initialization: Weights of the controller are initialized uniformly between -0.08 and 0.08.",,,,,,,,,,
240,GPT-Neo-125M,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021/03/21,2021,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,1.25E+08,,,1,,,0.0,400000000000.0,3.00E+20,,400000000000.0,,,32.29,,1.0,1.0,0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,,,,1,,50257,,,,,,,,,,,
241,GPT-Neo-2.7B,"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021/03/21,2021,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,2.70E+09,,,1,,,1.0,400000000000.0,6.48E+21,,400000000000.0,,,11.39,,1.0,1.0,0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,,,,1,,50257,,,,,,,,,,,
242,GPT-Neo-1.3B (finetuned),"Sid Black, Leo Gao, Phil Wang, Connor Leahy, Stella Biderman",2021/03/21,2021,GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow,,,https://github.com/EleutherAI/gpt-neo,2.70E+09,,,1,,,0.0,400000000000.0,6.48E+21,2080000.0,400000000000.0,,,12.09,,0.0,1.0,0,0,Transformer,GPT-Neo,https://github.com/EleutherAI/gpt-neo,,,,1,,50257,,,,,,,,,,,
243,Pythia-70m,"Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal",2023/04/03,2023,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,73.0,,https://arxiv.org/abs/2304.01373,7.00E+07,,,1,,,0.0,300000000000.0,1.26E+20,,300000000000.0,,,57.04,,0.5,0.0,0,1,Transformer,Pythia,https://github.com/EleutherAI/pythia,,,,1,,,"Architecture: Transformer (decoder-only autoregressive); Optimizer: Adam with Zero Redundancy Optimizer (ZeRO); LR Schedule: cosine; Training: data parallelism, tensor parallelism, Flash Attention; Attention: parallel attention; Regularization: weight decay 0.01; Initialization: small-init; Other: rotary embeddings, untied embedding/unembedding matrices",,,,,,,,,,
244,TransfoRNN(d=1024)(2-layer) (PTB),"Tze Yuang Chong, Xuyang Wang, Lin Yang, Junjie Wang",2021/04/04,2021,TransfoRNN: Capturing the Sequential Information in Self-Attention Representations for Language Modeling,0.0,0.0,https://arxiv.org/pdf/2104.01572,4.99E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,82.1,0.0,0.0,0,0,Transformer/RNN,TransfoRNN,,,,,1,,,"Architecture: The TransfoRNN architecture consists of N layers of Transformers cascaded with M layers of RNNs (specifically LSTM). The Transformer layers use a residual connection followed by layer normalization around the self-attention component and a residual connection followed by layer normalization around the feed-forward component.; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: new-bob learning rate adjustment (Initial learning rate commonly fixed as 0.1); Training: Residual connections, Layer normalization; Attention: Multi-head attention mechanism used in Transformer layers with 8 heads; Initialization: Not explicitly mentioned; Other: Input embedding tied to the output embedding",,,,,,,,,,
245,BERT-Large-CAS (PTB+WT2+WT103),"Chenguang Wang, Mu Li, Alexander J. Smola",2019/04/20,2019,Language Models with Transformers,110.0,,https://arxiv.org/abs/1904.09408,3.95E+08,,,50,,,0.0,4400000000.0,5.21E+20,929000.0,4400000000.0,Penn TreeBank; WikiText-2; WikiText-103,,,31.34,0.0,0.0,0,0,Transformer,BERT,https://github.com/cgraywang/gluon-nlp-1/tree/lmtransformer/scripts/language_model,,,,1,,,"Architecture: Transformer with 12-layer Transformer decoder-only blocks (for GPT) or 12/24-layer bidirectional Transformer encoder blocks (for BERT). Adding LSTM layers either before or after all Transformer blocks is part of the architecture search.; Optimizer: Adam; LR Schedule: Not explicitly stated but a learning rate is set for the Adam optimizer.; Training: Truncated back-propagation through time; Attention: Self-attention with 12 heads (GPT-Base and BERT-Base) or 16 heads (BERT-Large). Masked self-attention to prevent leftward information flow.; Regularization: Dropout (0.1 for LSTM layers, 0.1 for final linear layer in GPT), L2 weight decay (0.01); Special Algorithms: Coordinate Architecture Search (CAS): Randomly generates variants of the Transformer architecture based on the current best found architecture., Mixture of Softmax (MoS) with 15 components to replace the standard softmax; Initialization: Random initialization for added linear layers; Other: FixSubset: Algorithm for freezing parameters of randomly selected Transformer blocks during fine-tuning",,,,,,,,,,
246,Compressive Transformers for Long-Range Sequence Modelling,"Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap",2019/11/13,2019,Compressive Transformers for Long-Range Sequence Modelling,330.0,,https://arxiv.org/abs/1911.05507,UNK,,1.6e+20,328.32,,,0.0,103000000.0,#VALUE!,,103000000.0,WikiText-103,17.1,,,0.0,1.0,0,0,Transformer,Transformer-XL,,Google Research,Industry,,0,,,"Architecture: Compressive Transformer, a variant of the Transformer architecture.; Optimizer: Adam; LR Schedule: Linear warmup from 1e-6 to 3e-4 and a cosine decay back down to 1e-6.; Training: Parameter updates every 4 steps after 60,000 iterations; Attention: Multi-head attention; Regularization: Gradient clipping to a norm of at most 0.1; Special Algorithms: Compression of past hidden activations into a compressed memory using linear algebra components such as convolutions or max/mean pooling., Attention-reconstruction loss (Algorithm 2); Other: Relative positional embedding scheme, Maintains a memory of past activations at each layer",,,,,,,,,,
247,GPT-J-6B,"Ben Wang, Aran Komatsuzaki",2021/06/09,2021,GPT-J-6B: 6B JAX-Based Transformer,,,https://huggingface.co/EleutherAI/gpt-j-6b,6.05E+09,,,1,,,1.0,402000000000.0,1.46E+22,,402000000000.0,,,10.88,,1.0,1.0,0,0,Transformer,GPT,https://github.com/kingoflolz/mesh-transformer-jax/,,,,1,,?,,,,,,,,,,,
248,Delta RNN (+ full context),"Kazuki Irie, Imanol Schlag, Róbert Csordás, Jürgen Schmidhuber",2021/06/11,2021,Going Beyond Linear Transformers with Recurrent Fast Weight Programmers,42.0,,https://proceedings.neurips.cc/paper/2021/file/3f9e3767ef3b10a0de4c256d7ef9805d-Paper.pdf,4.46E+07,,,40,,,0.0,103000000.0,1.10E+18,0.0,103000000.0,WikiText-103,32.8,,,0.0,1.0,0,0,Recurrent,RNN,https://github.com/IDSIA/recurrent-fwp,,,,1,,?,,,,,,,,,,,
249,TRIMELMext (247M),"Zexuan Zhong, Tao Lei, Danqi Chen",2022/05/25,2022,Training Language Models with Memory Augmentation,35.0,,https://arxiv.org/abs/2205.12674,2.47E+08,,,204.72,,,0.0,103000000.0,3.12E+19,103000000.0,206000000.0,WikiText-103,15.37,,,0.0,0.0,0,0,Transformer,Transformer-XL,https://github.com/princeton-nlp/TRIME,Michigan State University; TAL Education Group,Industry - Academia Collaboration,,1,,,"Architecture: Transformer; Optimizer: Adam; LR Schedule: Cosine decay, inverse square root; Training: gradient clipping, adaptive softmax, linear interpolation for output distributions when using Mext, Exclude the local memory from Mtrain with a probability p during training; Attention: Multi-head scaled dot-product attention; Special Algorithms: TRIME (Training with In-batch Memories), Data batching to improve memory construction, BM25 for packing segments with lexical overlap, contrastive learning objective; Initialization: Not explicitly mentioned",,,,,,,,,,
250,base LM+GNN+kNN,"Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li",2021/10/17,2021,GNN-LM: Language Modeling based on Global Contexts via GNN,22.0,,https://arxiv.org/abs/2110.08743,2.74E+08,,7.3e+18,,,,0.0,103000000.0,0.00E+00,103000000.0,206000000.0,WikiText-103,16.8,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/ShannonAI/GNN-LM,,,,1,,260000,"Architecture: Transformer-XL base model extended with a 3-layer self-attention augmented Graph Neural Network (GNN); Optimizer: Not mentioned; LR Schedule: Not mentioned; Training: Data Leakage Prevention, Feature Quantization; Attention: Self-attention augmented GNN; Regularization: Adding rintra edges between adjacent tokens (shallow GNN layers used), Product Quantization; Special Algorithms: GNN-LM, kNN-LM; Initialization: Input node representations of the graph neural network are generated by a pretrained neural language model",,,,,,,,,,
251,GLM-10B-bidirectional,"Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",2021/03/18,2021,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,131.0,,https://arxiv.org/abs/2103.10360,1.00E+10,,,1,,,0.0,632000000000.0,3.79E+22,,632000000000.0,,11.33,,,1.0,1.0,0,0,Transformer,GLM,https://github.com/THUDM/GLM,,,,1,BERT,30000,"Architecture: Transformer with several modifications: (1) Rearranged the order of layer normalization and the residual connection. (2) A single linear layer for output token prediction. (3) Replaced ReLU activation functions with GeLUs.; Optimizer: AdamW; LR Schedule: Cosine decay; Training: 2D positional encoding, blank infilling, span shuffling; Attention: Masked self-attention. Part A tokens can attend to themselves (blue frame) but not B. Part B tokens can attend to A and their antecedents in B (yellow and green frames correspond to the two spans).; Regularization: Dropout 0.1, Attention Dropout 0.1, Weight decay 0.1 (or 0.01); Special Algorithms: Autoregressive blank infilling, 2D positional encodings; Other: 2D positional encodings uses two positional IDs. The first represents the position in the corrupted text. The second represents the intra-span position. The two IDs are projected into vectors via learnable embedding tables, which are both added to the input token embeddings., Span shuffling: randomly permute the order of the spans, Gradient clipping with value 1.0",,,,,,,,,,
252,Fairseq + UID: variance,"Jason Wei, Clara Meister, Ryan Cotterell",2021/05/15,2021,A Cognitive Regularizer for Language Modeling,11.0,,https://web.archive.org/web/20221010230611/https://arxiv.org/pdf/2105.07144.pdf,UNK,1.03E+08,,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,,29.58,,,0.0,1.0,0,0,Transformer,Transformer,,University of Edinburgh,Academia,,0,,,"Architecture: Transformer with six decoder layers and eight attention heads; Optimizer: Not explicitly mentioned, likely the default used in the Fairseq library; LR Schedule: Not explicitly mentioned, likely the default used in the Fairseq library; Training: dropout, label smoothing (experimented but did not improve performance), data preprocessing scripts as recommended by Fairseq's language modeling module; Attention: Multi-head attention with 8 heads; Regularization: Variance Regularizer, Local Consistency Regularizer; Initialization: Not mentioned",,,,,,,,,,
253,Quantized ADMM,"Junhao Xu, Xie Chen, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng",2021/11/29,2021,Low-bit Quantization of Recurrent Neural Network Language Models Using Alternating Direction Methods of Multipliers,9.0,,https://arxiv.org/pdf/2111.14836,,,,50,,,0.0,929000.0,0.00E+00,,929000.0,,,,115.9,0.0,0.0,0,0,Recurrent,RNN,,,,,0,,,"Architecture: Single-layer LSTM with 200 hidden nodes.; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: Initial learning rate of 1, optionally within the ADMM based quantization; Special Algorithms: Alternating Direction Methods of Multipliers (ADMM) for quantization, Locally shared quantization tables; Initialization: Scaling factors are initialized to one.; Other: Extra-gradient method to improve convergence",,,,,,,,,,
254,EN^2AS with performance reward,"Miao Zhang, Huiqi Li, Shirui Pan, Taoping Liu, Steven Su",2019/07/22,2019,Efficient Novelty-Driven Neural Architecture Search,1.0,1.0,https://arxiv.org/pdf/1907.09109,2.30E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,57.36,0.0,0.0,0,0,NAS,NAS,,Lenovo Research,Industry,,1,,,"Architecture: Uses cells represented as directed acyclic graphs (DAG). CNN cell has 7 nodes (2 input, 4 operation, 1 output).  RNN cell contains 12 nodes (2 input, 1 adding, 8 operation, 1 output). Single-path supernet where only one path is activated.; Optimizer: SGD with momentum; LR Schedule: Cosine annealing (CNN). Averaged SGD (RNN); Training: Weight sharing based single-path model, Path dropout; Regularization: Weight decay, Dropout; Special Algorithms: Efficient Novelty-Driven Neural Architecture Search (EN^2AS), Novelty Search used for architecture sampling; Initialization: Randomly initialized weights in the supernet; Other: Architecture sampling based on novelty search mechanism, Archive-based novelty search",,,,,,,,,,
255,GLM-10B-unidirectional,"Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang",2021/03/18,2021,GLM: General Language Model Pretraining with Autoregressive Blank Infilling,131.0,,https://arxiv.org/abs/2103.10360,1.00E+10,,,1,,,0.0,632000000000.0,3.79E+22,,632000000000.0,,12.22,,,1.0,1.0,0,0,Transformer,GLM,https://github.com/THUDM/GLM,,,,1,BERT,30000,"Architecture: Single Transformer with several modifications: (1) rearrange the order of layer normalization and the residual connection, (2) use a single linear layer for the output token prediction, (3) replace ReLU activation functions with GeLUs; Optimizer: Adam; LR Schedule: Cosine Decay; Training: 2D positional encodings, Autoregressive Blank Infilling; Attention: Masked self-attention. Part A tokens can attend to themselves but not B. Part B tokens can attend to A and their antecedents in B.; Regularization: Dropout 0.1, Attention Dropout 0.1, Weight Decay 0.1 (0.01 for GLMROBERTA); Special Algorithms: Autoregressive blank infilling, 2D Positional Encoding; Initialization: Not explicitly mentioned; Other: Span shuffling (randomly permute the order of spans), 2D positional encoding represents inter- and intra-span positions",,,,,,,,,,
256,BERT-Large-CAS (WT103),"Chenguang Wang, Mu Li, Alexander J. Smola",2019/04/20,2019,Language Models with Transformers,110.0,,https://arxiv.org/abs/1904.09408,3.95E+08,,,50,,,0.0,4400000000.0,5.21E+20,103000000.0,4500000000.0,WikiText-103,20.42,,,0.0,0.0,0,0,Transformer,BERT,https://github.com/cgraywang/gluon-nlp-1/tree/lmtransformer/scripts/language_model,,,,1,,,"Architecture: Transformer-based architectures with additional LSTM layers after Transformer blocks; Optimizer: Adam; LR Schedule: Not explicitly mentioned, but a fixed learning rate of 6.25e-05 for GPT and 1e-04 for BERT during fine-tuning is used.; Training: Truncated back-propagation through time; Attention: Self-attention mechanisms as used in the original Transformer architecture (multi-head); Regularization: L2 weight decay 0.01, Dropout (0.1 for LSTM layers and output linear layer); Special Algorithms: Coordinate Architecture Search (CAS): An algorithm to search for effective architectures through iterative refinement of the model.; Initialization: Linear output layer parameters are randomly initialized.  Parameters for newly added LSTM layers are also randomly initialized.; Other: Mixture of Softmaxes (MoS) with 15 components to replace standard softmax",,,,,,,,,,
257,SparseOPT-30B,"Elias Frantar, Dan Alistarh",2023/01/02,2023,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,8.0,,https://arxiv.org/abs/2301.00774,1.50E+10,,,1.666666667,,,1.0,180000000000.0,2.70E+22,0.0,180000000000.0,,9.79,,,1.0,0.0,0,0,Transformer,OPT,https://github.com/IST-DASLab/sparsegpt,,,,1,,,"Architecture: Transformer; Optimizer: Not mentioned explicitly in the provided text. The special algorithm could have been used as an optimizer, or the models could have been optimized using a standard training procedure for OPT/BLOOM models.; LR Schedule: Not mentioned explicitly in the provided text. Presumed to be the default schedule for the OPT/BLOOM models.; Attention: Not mentioned explicitly in the provided text. Presumed to be multi-headed attention as it's the default for OPT/BLOOM models.; Special Algorithms: SparseGPT: an accurate one-shot pruning method using approximate sparse regression solver and OBS update; Initialization: Not mentioned explicitly in the provided text. Presumed to be the default initialization for the OPT/BLOOM models.",,,,,,,,,,
258,LSTM (WT2),"Edouard Grave, Armand Joulin, Nicolas Usunier",2016/12/13,2016,Improving Neural Language Models with a Continuous Cache,302.0,,https://arxiv.org/abs/1612.04426,3.28E+07,,,UNK,,,0.0,2080000.0,#VALUE!,,2080000.0,WikiText-2,,99.3,,0.0,0.0,0,0,Recurrent,LSTM,,,,,1,,,"Architecture: 1024 LSTM units; Optimizer: Adagrad with a learning rate of 0.2; LR Schedule: Constant; Training: gradient clipping (norm clipped to 0.1), dropout (probability of dropping out units equals to 0.65 for small scale experiments, 0.45 for text8, and 0.25 for wikitext103), truncated backpropagation through time algorithm (unroll the network for 30 steps); Attention: Not applicable; Regularization: dropout; Special Algorithms: Adaptive Softmax (for faster training with large vocabularies); Initialization: Initial weight uniformly sampled in the range [-0.05, 0.05]",,,,,,,,,,
259,RNN+weight noise+dynamic eval,Alex Graves,2013/08/04,2013,Generating Sequences With Recurrent Neural Networks,4734.0,,https://arxiv.org/abs/1308.0850,5.40E+07,,,14,,,0.0,929000.0,4.21E+15,,929000.0,,,,117.0,0.0,1.0,0,0,Recurrent,RNN,,,,,1,Word-level,10000,"Architecture: Deep RNN composed of stacked LSTM layers; Optimizer: Stochastic gradient descent, rmsprop for some experiments (Section 4.2); LR Schedule: Learn rate of 0.0001 used for Penn Treebank and Wikipedia experiments. Used adaptive weight noise as an alternative to early stopping.; Training: Backpropagation Through Time, truncated backpropagation (Wikipedia experiments), Mixture Density Outputs (handwriting experiments), Primed sampling (handwriting synthesis experiments), biased sampling (handwriting synthesis); Attention: Novel convolutional mechanism (a 'soft window' convolved with the text string is fed in as an extra input) allowing recurrent network to condition its predictions on an auxiliary annotation sequence. In the handwriting synthesis, a window layer is used to align the text with the pen trace.; Regularization: Weight noise (std dev 0.075), Adaptive weight noise (variance learned alongside weights); Initialization: With weight noise: initialized with the final weights of the unregularized network; when adaptive weight noise was used, the weights were initialised with those of the network trained with weight noise.; Other: Gradient clipping",,,,,,,,,,
260,DARTS,"Hanxiao Liu, Karen Simonyan, Yiming Yang",2018/06/24,2018,DARTS: Differentiable Architecture Search,3990.0,,https://arxiv.org/abs/1806.09055,3.30E+07,,1.1e+16,300,,,0.0,2080000.0,1.24E+17,,2080000.0,WikiText-2,,69.6,,0.0,0.0,0,0,NAS,DARTS,https://github.com/quark0/darts,,,,1,,,"Architecture: The architecture consists of computation cells represented as directed acyclic graphs. Each cell has N nodes (N=7 for CIFAR-10, N=12 for PTB). Each node is a latent representation, and edges are associated with operations. Convolutional cell building blocks are stacked to form a convolutional network, or recursively connected to form a recurrent network. Search space includes: 3x3 and 5x5 separable convolutions, 3x3 and 5x5 dilated separable convolutions, 3x3 max pooling, 3x3 average pooling, identity, and zero. For recurrent cells, the operation set includes linear transformations followed by tanh, relu, sigmoid activations, as well as identity mapping and the zero operation. Output is depthwise concatenation of intermediate nodes for convolutional cells and the average of intermediate nodes for recurrent networks.; Optimizer: Momentum SGD for training weights (w) and Adam for training architecture parameters (a). Averaged SGD (ASGD) is used for the last stage of PTB training.; LR Schedule: Cosine annealing without restart is used to anneal the learning rate of the weights to zero. For ASGD, constant learning rate is used.; Training: Batch normalization (but learnable affine parameters are disabled during search), Variational dropout (PTB), Cutout (CIFAR-10), Path dropout, Auxiliary towers; Attention: Implicit attention through the selection of operations based on learned architecture parameters.; Regularization: Weight decay (for both weights and architecture parameters), Variational dropout; Special Algorithms: Differentiable ARchiTecture Search (DARTS): a novel algorithm for differentiable network architecture search based on bilevel optimization; Initialization: Zero initialization for architecture variables (a).; Other: Bilevel optimization for jointly learning architecture and weights., Continuous relaxation of the architecture search space., Approximated architecture gradient for computational efficiency using a one-step forward model.",,,,,,,,,,
261,TF-LM-discourse LSTM (WT2),"Lyan Verwimp, Hugo Van hamme, Patrick Wambacq",2018/05/01,2018,TF-LM: TensorFlow-based Language Modeling Toolkit,7.0,,https://aclanthology.org/L18-1470.pdf,UNK,,,39,,,0.0,2080000.0,#VALUE!,,2080000.0,,,98.2,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/lverwimp/tf-lm,,,,0,,,,,,,,,,,,,
262,genCNN + dyn eval,"Mingxuan Wang, Zhengdong Lu, Hang Li, Wenbin Jiang, Qun Liu",2015/03/17,2015,genCNN: A Convolutional Architecture for Word Sequence Prediction,33.0,,https://aclanthology.org/P15-1151/,8.00E+06,,7.3e+16,,,,0.0,929000.0,0.00E+00,,929000.0,Penn TreeBank,,,106.3,0.0,1.0,0,0,,,,,,,1,,10000,,,,,,,,,,,
263,Transformer-XL+AdamGapAware(GA),"Saar Barkai, Ido Hakimi, Assaf Schuster",2019/09/24,2019,Gap Aware Mitigation of Gradient Staleness,12.0,,https://arxiv.org/pdf/1909.10802,2.57E+08,,,,,,0.0,103000000.0,0.00E+00,103000000.0,206000000.0,,26.48,,,0.0,1.0,0,0,Transformer,Transformer-XL,,,,,1,,?,"Architecture: Transformer-XL; Optimizer: Adam, NAG; LR Schedule: Cosine from 0.00025 to 0, gradual warm-up, the learning rate is divided by the number of workers N and ramped it up linearly until it reaches its original value after 5 epochs; Training: gradient clipping 0.25; Attention: Not Explicitly Mentioned, Using Transformer XL Architecture; Regularization: Dropout 0.1, Weight Decay; Special Algorithms: Gap-Aware (GA), DANA, DANA-Gap-Aware; Initialization: Not mentioned",,,,,,,,,,
264,GRU + p-tHSM (pretrain via Brown) (WT103),"Nan Jiang, Wenge Rong, Min Gao, Yikang Shen, Zhang Xiong",2017/08/19,2017,Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models,5.0,,https://www.researchgate.net/profile/Yikang-Shen-2/publication/318830618_Exploration_of_Tree-based_Hierarchical_Softmax_for_Recurrent_Language_Models/links/5b2c050aa6fdcc8506bc6f4a/Exploration-of-Tree-based-Hierarchical-Softmax-for-Recurrent-Language-Models.pdf,2.06E+08,,,UNK,,,0.0,103000000.0,#VALUE!,,103000000.0,WikiText-103,161.55,,,0.0,0.0,0,0,Recurrent,GRU,,DeepMind,Industry,,1,,,,,,,,,,,,,
265,Fraternal dropout + AWD-LSTM 3-layer (WT2),"Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio",2017/10/31,2017,Fraternal Dropout,55.0,,https://arxiv.org/abs/1711.00066,3.40E+07,,,520,,,0.0,2080000.0,2.21E+17,,2080000.0,WikiText-2,,64.1,,0.0,1.0,0,0,Recurrent,,,,,,1,Moses,?,"Architecture: AWD-LSTM 3-layer; Optimizer: non-monotonically triggered averaged SGD (NT-ASGD) or SGD; LR Schedule: The learning rate 30 (no momentum) which is multiplied by 0.1 whenever validation performance does not improve ever during 20 epochs. For Fraternal Dropout, models are trained longer using ordinary SGD optimizer.; Training: truncated back-propagation, Fraternal Dropout; Regularization: weight decay, dropout on the word vectors, dropout between LSTM layers, dropout of the final LSTM, embedding dropout, Activity Regularization, Temporal Activation Regularization; Special Algorithms: Fraternal Dropout; Other: Gradient clipping",,,,,,,,,,
266,Fraternal dropout + AWD-LSTM 3-layer (PTB),"Konrad Zolna, Devansh Arpit, Dendi Suhubdy, Yoshua Bengio",2017/10/31,2017,Fraternal Dropout,55.0,,https://arxiv.org/abs/1711.00066,2.40E+07,,,520,,,0.0,929000.0,6.96E+16,,929000.0,Penn TreeBank,,,56.8,0.0,1.0,0,0,Recurrent,,,,,,1,,?,"Architecture: AWD-LSTM 3-layer architecture; Optimizer: Non-monotonically triggered averaged SGD (NT-ASGD) or ordinary SGD. NT-ASGD is replaced by ordinary SGD for longer training.; LR Schedule: Learning rate is multiplied by 0.1 whenever validation performance does not improve ever during 20 epochs.; Attention: N/A; Regularization: Fraternal Dropout which adds a regularization term to the loss function which minimizes the difference between the pre-softmax predictions of two identical copies of the same LSTM with different dropout masks., Weight decay (1.2 × 10-6, input embedding size of 655); Special Algorithms: Fraternal Dropout; Initialization: A constant zero state is provided as the initial state with probability 0.01.; Other: Gradient clipping of 0.25, Truncated back-propagation with 35 time steps, embedding weights are tied",,,,,,,,,,
267,(ensemble): AWD-LSTM-DOC (fin) × 5 (PTB),"Sho Takase, Jun Suzuki, Masaaki Nagata",2018/08/30,2018,Direct Output Connection for a High-Rank Language Model,36.0,,https://arxiv.org/abs/1808.10143,1.14E+08,,,300,,,0.0,929000.0,1.91E+17,,929000.0,Penn TreeBank,,,47.17,0.0,0.0,0,0,Recurrent,LSTM,https://github.com/nttcslab-nlp/doc_lm,"Ruhr University Bochum, Germany; University of London; Technische Universität Dresden, Dresden, Germany",Academia,,1,,,"Architecture: 3-layer LSTM; Optimizer: averaged stochastic gradient descent; LR Schedule: Not explicitly mentioned; Training: weight dropping; Attention: Not applicable; Regularization: dropout, recurrent weight dropout; Special Algorithms: Direct Output Connection (DOC); Initialization: Not mentioned; Other: Mini-batch variation regularization",,,,,,,,,,
268,"AWD-LSTM-MoS + dynamic evaluation (WT2, 2018)","Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu",2018/09/18,2018,FRAGE: Frequency-Agnostic Word Representation,152.0,,https://arxiv.org/abs/1809.06858,3.50E+07,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,WikiText-2,,39.14,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/ChengyueGongR/Frequency-Agnostic,,,,1,,33278,"Architecture: AWD-LSTM (weight-dropped LSTM); Optimizer: NT-ASGD (a variant of the averaged stochastic gradient method); LR Schedule: Learning rate schedule as in Vaswani et al. (2017); Training: DropConnect on hidden-to-hidden weights (recurrent regularization), Adversarial training with a discriminator to classify popular/rare words; Attention: Not applicable; Regularization: V-dropout (word-level, embedding, hidden state, context vector), Recurrent weight dropout, Embedding V-dropout; Special Algorithms: Frequency-Agnostic Word Embedding (FRAGE) which is adversarial training that aims to remove frequency information from word embeddings by training a discriminator to classify rare/popular words; Initialization: Not mentioned; Other: Mixture of Softmaxes structure to the vanilla AWD-LSTM in the AWD-LSTM-MOS model",,,,,,,,,,
269,"AWD-LSTM-MoS + dynamic evaluation (PTB, 2018)","Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, Tie-Yan Liu",2018/09/18,2018,FRAGE: Frequency-Agnostic Word Representation,152.0,,https://arxiv.org/abs/1809.06858,2.40E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,Penn TreeBank,,,46.54,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/ChengyueGongR/Frequency-Agnostic,,,,1,,10000,"Architecture: AWD-LSTM and AWD-LSTM-MoS; Optimizer: Adam; LR Schedule: Learning rate schedule as in [42] for machine translation task, and NT-ASGD (variant of averaged stochastic gradient method) for language modelling; Training: weight dropping, DropConnect on hidden-to-hidden weights as a means of recurrent regularization; Regularization: embedding V-dropout, hidden state V-dropout, recurrent weight dropout, context vector V-dropout, weight decay (l2 regularization on embeddings, mentioned in additional comparisons); Special Algorithms: FRAGE (Frequency-Agnostic Word Representation) using adversarial training. A discriminator classifies word embeddings into popular/rare, while the LM fools the discriminator and minimizes task loss.; Other: Mixture of Softmaxes structure for AWD-LSTM-MOS, NT-ASGD, which is a variant of the averaged stochastic gradient method",,,,,,,,,,
270,FNetAR Medium,"Tim Lou, Michael Park, Mohammad Ramezanali, Vincent Tang",2021/07/22,2021,FNetAR: Mixing Tokens with Autoregressive Fourier Transforms,2.0,0.0,https://arxiv.org/abs/2107.10932,3.43E+07,,,,,,0.0,103000000.0,0.00E+00,0.0,103000000.0,WikiText-103,25.81,,,0.0,1.0,0,0,Transformer,Transformer-XL,,,,,1,?,?,"Architecture: FNetAR is an autoregressive generalization of FNet. It uses a 2-layer ResNet architecture with one Fourier-mixing layer followed by one position-wise feedforward layer, instead of self-attention layers in standard Transformer architecture. The paper also explores combining FNetAR with recurrent Transformer architectures such as Transformer-XL.; Optimizer: Not explicitly mentioned; LR Schedule: Not explicitly mentioned; Attention: Self-attention layers from the standard Transformer architecture are substituted with a trivial sparse-uniform sampling procedure based on Fourier transforms. The Fourier-mixing layer mixes a sequence element by sampling its complementary elements with discrete wave-form coefficients whose frequencies decay as a function of distance.; Special Algorithms: Autoregressive Fourier Transform; Initialization: Not explicitly mentioned; Other: Residual connection, Causal masking to create autoregressive transformers, Padding a Fourier transform matrix with zeros and performing a roll over the rows",,,,,,,,,,
271,PAR Transformer Large,"Swetha Mandava, Szymon Migacz, Alex Fit Florea",2020/09/09,2020,Pay Attention when Required,11.0,,https://arxiv.org/abs/2009.04534,,,,,,,0.0,103000000.0,0.00E+00,0.0,103000000.0,WikiText-103,18.4,,,0.0,0.0,0,0,Transformer,Transformer-XL,,Carnegie Mellon University; University of California San Diego,Academia,,0,,,"Architecture: Transformer-XL base architecture. The model architectures consists of interleaving self attention and feed forward blocks. The number of self-attention blocks is reduced by replacing them with feed-forward blocks to improve computation time.; Optimizer: NVLamb (used for pretraining BERT); LR Schedule: Not explicitly mentioned, but architecture update learning rate is 1e-2.; Training: Differential Neural Architecture Search (similar to FBNet algorithm), Gumbel Softmax function for computing the output of each layer, Architecture search with 3 options for each layer: Self Attention, Feed Forward, or Identity; Attention: Self-attention; Regularization: Weight decay (5e-4 for architecture parameters, 1e-4 for block weights); Special Algorithms: Differential Neural Architecture Search; Initialization: Architecture parameters are initialized uniformly; Other: Explores the trade-off and ordering of self-attention and feed-forward blocks, Uses a Super Net as a linear combination of search blocks, PAR Transformer based on two design rules: self-attention layers are only needed among the former two-third layers of the network, total number of layers to self-attention layers ratio of p:1 is sufficient",,,,,,,,,,
272,FMMformer (2-kernel fast weight + Band20),"Tan M. Nguyen, Vai Suliafu, Stanley J. Osher, Long Chen, Bao Wang",2021/08/05,2021,FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention,15.0,,https://web.archive.org/web/20220803154831/https://proceedings.neurips.cc/paper/2021/file/f621585df244e9596dc70a39b579efb1-Paper.pdf,4.00E+07,1.03E+08,4.3e+17,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,,34.71,,,0.0,1.0,0,1,Transformer,FMMformer,,,,,1,?,?,,,,,,,,,,,
273,VD-LSTM+REAL Medium,"Hakan Inan, Khashayar Khosravi, Richard Socher",2016/11/04,2016,Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling,397.0,,https://arxiv.org/abs/1611.01462,2.04E+07,,,,,,1.0,2080000.0,0.00E+00,,2080000.0,WikiText-2,,87.0,,0.0,0.0,0,0,Recurrent,LSTM,,,,,1,,,"Architecture: 2-layer LSTM; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: Start with a learning rate of 1 and decay it with a constant rate after a certain epoch. The decay rate is 0.9 or 0.97.; Training: Gradient clipping, Dropout; Attention: N/A; Regularization: Dropout (variational dropout method); Special Algorithms: Augmented Loss with Kullback-Leibler divergence between model prediction and estimated target distribution, Tying word vectors and word classifiers by reusing the input word embedding matrix as the output classification matrix; Initialization: N/A; Other: Temperature parameter in augmented loss",,,,,,,,,,
274,"MicroNet (Adaptive, Cache)","Zhongxia Yan, Hanrui Wang, Demi Guo, Song Han",2020/01/01,2020,MicroNet for Efficient Language Modeling,7.0,,http://proceedings.mlr.press/v123/yan20a.html?ref=https://githubhelp.com,8.30E+06,,,,,,0.0,103000000.0,0.00E+00,0.0,103000000.0,WikiText-103,35.0,,,0.0,0.0,1,0,Transformer,Transformer-XL,,Google Brain; Carnegie Mellon University,Industry - Academia Collaboration,,1,,,,,,,,,,,,,
275,Decaying Fast Weights Transformer,Huanru Henry Mao,2022/10/09,2022,Fine-Tuning Pre-trained Transformers into Decaying Fast Weights,0.0,1.0,https://arxiv.org/pdf/2210.04243.pdf,2.42E+08,,1.3e+19,192.12,,,0.0,103000000.0,2.87E+19,103000000.0,206000000.0,,20.5,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/jenni-ai/T2FW,,,,1,,?,"Architecture: Fine-tuned GPT-2 Small (Radford et al., 2019) into a recurrent formulation with decaying fast weights. The self-attention mechanism in Transformers is replaced with decaying fast weights (Ba et al., 2016) that evolves with linear dynamics.; Optimizer: Adam; LR Schedule: Cosine schedule to 2e-6 after 4000 warm-up steps; Training: Mixed precision training (except for computing normalizer), Gradient clipping; Attention: Self-attention replaced with decaying fast weights; Special Algorithms: Decay Update Rule:  introduces a low-rank decay matrix Gt ∈ (0,1)dxm to forget information in the additive update rule; Initialization: Uniform Gate Initialization (Gu et al., 2020), then re-scale pre-trained weight W by 1-σ(bz); Other: Linear projection feature map",,,,,,,,,,
276,Fine-tuned-AWD-LSTM-DOC(fin),"Vadim Popov, Mikhail Kudinov",2018/11/12,2018,Fine-tuning of Language Models with Discriminator,2.0,0.0,https://arxiv.org/pdf/1811.04623,2.30E+07,,,15,,,0.0,929000.0,1.92E+15,,929000.0,,,,52.12,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,100000,Architecture: LSTM. The architecture of discriminator is the same as that of the language model. Discriminator applies sigmoid function to logits.; Optimizer: Stochastic Gradient Descent (SGD). Switched to Averaged Stochastic Gradient Descent (ASGD) after initial training; LR Schedule: Multiplied learning rates by 0.1 each time validation perplexity showed no improvement; Special Algorithms: KL-divergence discriminator fine-tuning; Other: Noise Contrastive Estimation (NCE) as an alternative way to get estimation formula (8),,,,,,,,,,
277,"LSTM (Hebbian, Cache, MbPA)","Jack W Rae, Chris Dyer, Peter Dayan, Timothy P Lillicrap",2018/03/27,2018,Fast Parametric Learning with Activation Memorization,44.0,,https://arxiv.org/abs/1803.10049,4.52E+07,,2.4e+19,90.00,,,0.0,103000000.0,2.51E+18,0.0,103000000.0,WikiText-103,29.2,,,0.0,1.0,1,0,Recurrent,LSTM,,,,,1,,267735,"Architecture: LSTM (The specific number of layers is mentioned as 1 or 2 in the experiments, with hidden sizes ranging from 1024, 2048, 4096); Optimizer: Adaptive optimizers, such as RMSProp, used for gradient descent updates; LR Schedule: The Hebbian Softmax update involves an annealing function (At) that gradually reduces the influence of Hebbian learning in favor of gradient descent as class occurrences increase. The decay of contribution of activations into the weights is weighted geometrically (if cache models are considered); Training: Minibatches, Layer Normalization, Dropout; Attention: Attention mechanisms are used over a cache of previous activations to model long-range dependencies.; Regularization: L2 regularization, Embedding dropout; Special Algorithms: Hebbian Softmax, Neural Cache, Memory-based Parameter Adaptation (MbPA); Initialization: Glorot initialization for the softmax layer; Other: A two-speed dynamic: full network optimized slowly by gradient descent, sparse parameters change rapidly, The learning rule transitions from Hebbian learning to gradient descent.",,,,,,,,,,
278,ONLSTM-SYD,"Wenyu Du, Zhouhan Lin, Yikang Shen, Timothy J. O'Donnell, Yoshua Bengio, Yue Zhang",2020/05/12,2020,Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach,15.0,,https://arxiv.org/pdf/2005.05864,2.50E+07,,,1000,,,0.0,929000.0,1.39E+17,,929000.0,,,,55.7,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/wenyudu/SDLM,,,,1,,10000,"Architecture: 3-layer Ordered-Neuron LSTM (ON-LSTM); Optimizer: Not explicitly stated; LR Schedule: Not explicitly stated, but the model is trained and finetuned, suggesting a possible change in LR; Training: Embedding dropout, dropout between LSTM layers, Weight dropout; Attention: Not applicable; Regularization: dropout rates are 0.5, 0.45, 0.3, 0.45 for the word vectors, LSTM weight matrices, outputs between LSTM layers and the output of the last layer, respectively. The embedding dropout ratio is 0.125; Special Algorithms: Cumax, Syntactic Distance Approach; Initialization: Not mentioned; Other: Multi-task learning with a ranking loss between learned syntactic distance and ground truth distances, Uses syntactic distances calculated from parse trees as additional input",,,,,,,,,,
279,ERNIE-Doc (151M),"Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",2020/12/31,2020,ERNIE-Doc: A Retrospective Long-Document Modeling Transformer,35.0,,https://arxiv.org/pdf/2012.15688,1.51E+08,,,190.88,,,0.0,103000000.0,1.78E+19,,103000000.0,,21.0,,,1.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/PaddlePaddle/ERNIE/,,,,1,RoBERTa wordpieces tokenizer,50000,"Architecture: Memory-enhanced Transformer-XL using enhanced recurrence mechanism and retrospective feed mechanism.; Optimizer: Adam; LR Schedule: Linear decay after a warmup period; Training: Segment reordering objective, Relative positional embedding; Attention: Memory-enhanced self-attention; Regularization: Weight decay, Dropout; Special Algorithms: Retrospective feed mechanism, Enhanced recurrence mechanism, Segment-reordering objective",,,,,,,,,,
280,ERNIE-Doc (247M),"Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang",2020/12/31,2020,ERNIE-Doc: A Retrospective Long-Document Modeling Transformer,35.0,,https://arxiv.org/pdf/2012.15688,2.47E+08,,,190.88,,,0.0,103000000.0,2.91E+19,,103000000.0,,16.8,,,1.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/PaddlePaddle/ERNIE/,,,,1,RoBERTa wordpieces tokenizer,50000,"Architecture: Recurrence Transformer based on Transformer-XL, with modifications including retrospective feed mechanism and enhanced recurrence mechanism (same-layer recurrence instead of shifting-one-layer-downwards recurrence); Optimizer: Adam; LR Schedule: linear decay after a warmup period; Attention: Standard self-attention mechanism within the Transformer architecture; no specific modifications mentioned beyond the recurrence aspects.; Regularization: weight decay, dropout; Special Algorithms: Retrospective feed mechanism, Enhanced recurrence mechanism, Segment-reordering objective; Initialization: The remaining pretraining hyperparameters were the same as those of ROBERTa; Other: Relative positional embedding",,,,,,,,,,
281,S4,"Albert Gu, Karan Goel, Christopher Ré",2021/10/31,2021,Efficiently Modeling Long Sequences with Structured State Spaces,171.0,,https://arxiv.org/abs/2111.00396,2.49E+08,"8 A100 GPUs, 800k steps of training",,509.02,,,0.0,103000000.0,7.83E+19,0.0,103000000.0,WikiText-103,20.95,,,0.0,1.0,0,0,State Space Model,S4,https://github.com/HazyResearch/state-spaces,,,,1,,?,"Architecture: Structured State Space sequence model (S4) based on a new parameterization for the State Space Model (SSM). The state matrices A are reparameterized by decomposing them as the sum of a low-rank and normal term.; Optimizer: AdamW; LR Schedule: cosine learning rate cycle, constant learning rate schedule with decay on validation plateau; Training: truncated generating function in frequency space, Woodbury Identity; Regularization: dropout; Special Algorithms: Normal Plus Low-Rank (NPLR) parameterization; Initialization: A initialized to the HiPPO matrix (HiPPO theory of continuous-time memorization)",,,,,,,,,,
282,Routing Transformer,"Aurko Roy, Mohammad Saffar, Ashish Vaswani, David Grangier",2020/03/12,2020,Efficient Content-Based Sparse Attention with Routing Transformers,349.0,,https://arxiv.org/abs/2003.05997,7.95E+07,,,,,,0.0,103000000.0,0.00E+00,,103000000.0,WikiText-103,15.8,,,0.0,1.0,0,0,Transformer,Local transformer,https://github.com/google-research/google-research/tree/master/routing_transformer,,,,1,,?,"Architecture: Transformer architecture with sparse routing module. Combines content-based sparse attention with local attention.; Optimizer: Adam (except for PG-19 which uses Adafactor); LR Schedule: Follows the learning rate schedule described in Vaswani et al. (2017). For PG-19: constant learning rate with linear warmup followed by rsqrt_normalized_decay; Training: Relative position encoding, Layer normalization, Residual connections; Attention: Sparse routing module based on online k-means clustering. Queries are routed to keys belonging to the same cluster. Half the attention heads do local attention and the other half route attention (except for PG-19 which uses only routing heads).; Regularization: Attention dropout, ReLU dropout, Weight decay (not used on PG-19); Special Algorithms: Mini-batch k-means for centroid training, Spherical k-means; Initialization: Weights are projected onto the unit ball using Layer Normalization before computing dot product attention.",,,,,,,,,,
283,EI-REHN-1200D,"Hyunsin Park, Chang D. Yoo",2017/08/14,2017,Early Improving Recurrent Elastic Highway Network,6.0,,https://arxiv.org/pdf/1708.04116,1.20E+07,,,100,,,0.0,929000.0,6.69E+15,,929000.0,,,,66.2,0.0,1.0,0,0,Recurrent,RHN,,,,,1,,?,"Architecture: Early Improving Recurrent Elastic Highway Network (EI-REHN). It varies recurrence depth between input intervals. The recurrence depth is extended by intermediate hidden state units. Weights determining these units are dynamically calculated based on an elastic gate.; Optimizer: Adam; LR Schedule: Constant learning rate of 0.01 and 0.0025 for synthetic and HAR experiments respectively. No learning rate schedule mentioned.; Special Algorithms: Adaptive recurrence depth using an elastic gate, Dynamic weight matrix using a hypernetwork to generate weights for different recurrence layers; Other: Gated weight update",,,,,,,,,,
284,GCNN-14,"Yann N. Dauphin, Angela Fan, Michael Auli, David Grangier",2016/12/23,2016,Language Modeling with Gated Convolutional Networks,2176.0,,https://arxiv.org/abs/1612.08083,UNK,,,35.00,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-103,37.2,,,0.0,1.0,0,0,Recurrent,GCNN,,Stanford University; Salesforce Research,Industry - Academia Collaboration,,0,,,"Architecture: Gated Convolutional Networks (GCNN) with stacked convolutions. Bottleneck structure for computational efficiency with up to 5 layers per block. Pre-activation residual blocks are used.; Optimizer: Nesterov's momentum with momentum set to 0.99; LR Schedule: Learning rate sampled uniformly in the interval [1., 2.]; Training: Gradient clipping (value 0.1), Weight normalization, Shifting the convolutional inputs to prevent kernels from seeing future context (zero-padding); Attention: Not applicable; Regularization: Weight normalization; Special Algorithms: Gated Linear Units (GLU): h₁(X) = (X * W + b) ⊗ σ(X * V + c), Adaptive Softmax; Initialization: Kaiming initialization (He et al., 2015b); Other: Parallelization over the individual words of a sentence by convolving the inputs with a function., Convolution and Gated Linear Unit is wrapped in a pre-activation residual block.",,,,,,,,,,
285,EI-REHN-1000D,"Hyunsin Park, Chang D. Yoo",2017/08/14,2017,Early Improving Recurrent Elastic Highway Network,6.0,,https://arxiv.org/pdf/1708.04116,1.90E+07,,,100,,,0.0,929000.0,1.06E+16,,929000.0,,,,68.7,0.0,1.0,0,0,Recurrent,RHN,,,,,1,,?,"Architecture: Early Improving Recurrent Elastic Highway Network (EI-REHN). Recurrent network capable of varying and adjusting the recurrence depth between input intervals. The recurrence depth is extended by several intermediate hidden state units.; Optimizer: Adam; LR Schedule: constant learning rate of 0.01 or 0.0025; Special Algorithms: Adaptive recurrence depth, Dynamic weight matrix",,,,,,,,,,
286,Transformer-XL + RMS dynamic eval,"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",2019/04/17,2019,Dynamic Evaluation of Transformer Language Models,40.0,,https://arxiv.org/abs/1904.08378,2.57E+08,,,,,,0.0,103000000.0,0.00E+00,103000000.0,206000000.0,WikiText-103,16.4,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/benkrause/dynamiceval-transformer,,,,1,,268000,Architecture: Transformer-XL: stacked self-attention layers and position-wise feedforward operations with segment-level attention recurrence and a relative positional encoding mechanism; Optimizer: Stochastic Gradient Descent (SGD) with a fixed learning rate and RMSprop-related optimizer with gradient statistics computed from the training data and weights decayed back to the original parameters learned during training (dynamic evaluation optimizer); LR Schedule: fixed learning rate for SGD and tuned hyperparameters for dynamic evaluation optimizer; Attention: self-attention mechanism; Special Algorithms: Dynamic evaluation: a gradient descent based adaptation method that can be applied to auto-regressive sequence modeling problems,,,,,,,,,,
287,AWD-LSTM + dynamic eval (PTB),"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",2017/09/21,2017,Dynamic Evaluation of Neural Sequence Models,130.0,,https://arxiv.org/abs/1709.07432,2.40E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,Penn TreeBank,,,51.1,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/benkrause/dynamic-evaluation,,,,1,,10000,"Architecture: LSTM, AWD-LSTM (vanilla LSTM that combines drop-connect on recurrent weights and averaged stochastic gradient descent), mLSTM; Optimizer: SGD, Averaged Stochastic Gradient Descent (ASGD), Adam; LR Schedule: Not specified directly, but uses averaged stochastic gradient descent.; Training: DropConnect, Weight Normalization, Variational Dropout, Recurrent Dropout, Truncated Backpropagation Through Time; Attention: Not applicable; Regularization: Weight Drop, Global Decay Prior, RMS global prior; Special Algorithms: Dynamic evaluation, Sparse Dynamic Evaluation, RMSProp derived update rule; Initialization: Not mentioned; Other: Tied input and output embeddings, Uses an adaptation matrix to update a smaller number of parameters",,,,,,,,,,
288,LSTM + dynamic eval,"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",2017/09/21,2017,Dynamic Evaluation of Neural Sequence Models,130.0,,https://arxiv.org/abs/1709.07432,5.00E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,Penn TreeBank,,59.8,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/benkrause/dynamic-evaluation,,,,1,,33000,"Architecture: 2-layer LSTM with 650 units, and 3-layer AWD-LSTM; Optimizer: SGD (for standard LSTM), Averaged Stochastic Gradient Descent (AWD-LSTM); LR Schedule: Not explicitly mentioned, but dynamic evaluation adapts model parameters during evaluation using gradient descent.; Training: recurrent dropout (for standard LSTM), DropConnect (AWD-LSTM), Weight Drop (AWD-LSTM); Attention: Not applicable; Regularization: recurrent dropout, weight normalization (for mLSTM), variational dropout (for mLSTM); Special Algorithms: Dynamic Evaluation (gradient descent based adaptation to recent history), RMS with a global prior (variant of RMSprop), Sparse dynamic evaluation (adapting only a subset of the parameters); Initialization: Not mentioned; Other: Truncated Backpropagation Through Time (BPTT), Tied input and output embeddings (AWD-LSTM)",,,,,,,,,,
289,TransformerXL + PowerSGD + L-Greco,"Mohammadreza Alimohammadi, Ilia Markov, Elias Frantar, Dan Alistarh",2022/10/31,2022,L-GreCo: An Efficient and General Framework for Layerwise-Adaptive Gradient Compression,3.0,0.0,https://web.archive.org/web/20221101102609/https://arxiv.org/pdf/2210.17357.pdf,UNK,1.03E+08,4.14e+17,UNK,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,,24.08,,,0.0,0.0,0,0,Transformer,Transformer-XL,https://github.com/LGrCo/L-GreCo,DeepMind; University of Oxford,Industry - Academia Collaboration,,0,,,"Architecture: Transformer-XL, Transformer-LM (both are Transformer architectures); Optimizer: Adam (for Transformer-LM/XL), SGD with momentum (for ResNet); LR Schedule: Linear warmup followed by cosine decay or inverse sqrt; Attention: Not specified, assuming standard Transformer attention mechanism; Regularization: Weight decay, Label Smoothing (for ResNet); Special Algorithms: L-GreCo: Layer-wise parametrization of gradient compression schemes. This is the main novelty of the paper and involves dynamically adjusting compression parameters across layers during training based on a theoretically-justified measure of layer compression sensitivity., PowerSGD (for low-rank approximation), QSGD (for quantization), TopK (for sparsification); Initialization: Not mentioned",,,,,,,,,,
290,AWD-LSTM + dynamic eval (WT2),"Ben Krause, Emmanuel Kahembwe, Iain Murray, Steve Renals",2017/09/21,2017,Dynamic Evaluation of Neural Sequence Models,130.0,,https://arxiv.org/abs/1709.07432,3.30E+07,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,WikiText-2,,44.3,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/benkrause/dynamic-evaluation,,,,1,,33000,"Architecture: LSTM. Standard LSTM has 2 layers with 650 units. AWD-LSTM has 3 layers and tied input and output embeddings. mLSTM has 2800 hidden units and an embedding layer of 400 units.; Optimizer: SGD (for standard LSTM), Averaged SGD (AWD-LSTM), ADAM (mLSTM); LR Schedule: Not specified, but the use of averaged SGD implies a specific learning rate schedule; Training: Truncated Backpropagation Through Time, Gradient Clipping (Implied by RMSProp), Weight Normalization (mLSTM), Variational Dropout (mLSTM), Drop-connect (AWD-LSTM), Recurrent Dropout (Standard LSTM); Regularization: Recurrent Dropout (Zaremba et al., 2014), Global Decay Prior, RMS Gradient Scaling; Special Algorithms: Dynamic evaluation, Sparse dynamic evaluation; Other: RMSProp, Update rule methodology for dynamic evaluation, Adaptation Matrix (Sparse Dynamic Evaluation)",,,,,,,,,,
291,4-gram + 8 DENN,"Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran",2014/12/22,2014,Diverse Embedding Neural Network Language Models,1.0,0.0,https://arxiv.org/pdf/1412.7063,1.61E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,109.32,0.0,1.0,0,0,N-gram,,,,,,1,,10000,"Architecture: Feed-forward Neural Network Language Model (NNLM) and Diverse Embedding Neural Network (DENN). The DENN architecture learns multiple diverse low-dimensional representations of the input words using multiple NNLMs, instead of a single representation.; Optimizer: Root Mean Square Propagation (RMSProp); LR Schedule: Not explicitly mentioned, but hyperparameters were tuned on a held-out set.; Training: Back-propagation, Stochastic gradient descent (using batches), Gradient clipping (mentioned in hyperparameter tuning section), Scaling of bias updates (mentioned in hyperparameter tuning section); Attention: Not applicable; Regularization: L1 or L2 regularization penalties (mentioned in section 3 and 5); Special Algorithms: Diversity promoting loss function. The DENNLM loss function is augmented with terms to promote diversity of word representations. The loss function contains a mixture model loss, a discriminative loss, and a representational diversity term.; Initialization: Random Initialization of NNLMs; Other: The diversity between two NNLM embeddings is captured by the negative correlation between cosine angles of pairwise points in the continuous space representation., The number of NNLMs is adjustable, e.g., 4 DENN, 8 DENN. The hidden layer size is also configurable.",,,,,,,,,,
292,AWD-LSTM-DOC (fin) (37M),"Sho Takase, Jun Suzuki, Masaaki Nagata",2018/08/30,2018,Direct Output Connection for a High-Rank Language Model,36.0,,https://arxiv.org/abs/1808.10143,3.70E+07,,,300,,,0.0,2080000.0,1.39E+17,,2080000.0,WikiText-2,,58.03,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/nttcslab-nlp/doc_lm,,,,1,,33278,"Architecture: 3-layer LSTM; Optimizer: Averaged Stochastic Gradient Descent (AWD-LSTM); LR Schedule: Not explicitly stated, but the training involved a fine-tuning stage implying a reduction in the learning rate; Training: Weight Dropping, Averaging, Fine-tuning; Attention: Not applicable for base language model but used with DOC in an encoder-decoder model; Regularization: Weight Dropping, Dropout, Coefficient of variation in each mini-batch is used as a regularization term; Special Algorithms: Direct Output Connection (DOC); Initialization: Not mentioned",,,,,,,,,,
293,AWD-LSTM-DOC (fin) (23M),"Sho Takase, Jun Suzuki, Masaaki Nagata",2018/08/30,2018,Direct Output Connection for a High-Rank Language Model,36.0,,https://arxiv.org/abs/1808.10143,2.30E+07,,,300,,,0.0,2080000.0,8.61E+16,,2080000.0,WikiText-2,,,52.38,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/nttcslab-nlp/doc_lm,,,,1,,10000,"Architecture: 3-layer AWD-LSTM; Optimizer: Averaged Stochastic Gradient Descent (SGD); LR Schedule: Not explicitly mentioned, but the text indicates the use of an approach that reduces the learning rate during training (averaged SGD). They also use a non-monotone interval.; Training: Averaged SGD, Using the middle layers (including word embeddings) to compute the probability distributions (DOC) to weaken the vanishing gradient problem during backpropagation, Fine-tuning the model by retraining until convergence; Attention: Not applicable; Regularization: Weight Drop, Dropout (various rates for input, hidden states, and the k vector); Special Algorithms: Direct Output Connection (DOC): Computes probability distributions from the middle layers in addition to the final layer., Coefficient of variation regularization:  Compute the coefficient of variation of Equation 10 in each mini-batch as a regularization term., Word Tying (using the output embedding to improve language models, from related work Inan et al. (2017) ); Initialization: Not explicitly mentioned",,,,,,,,,,
294,DiffQ Transformer (16L),"Alexandre Défossez, Yossi Adi, Gabriel Synnaeve",2021/04/20,2021,Differentiable Model Compression via Pseudo Quantization Noise,20.0,,https://arxiv.org/abs/2104.09987,2.57E+08,,,,,,0.0,103000000.0,0.00E+00,,103000000.0,WikiText-103,18.1,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/facebookresearch/diffq,,,,1,,?,"Architecture: 16 layer Transformer; Optimizer: Adam (separate optimizer for the logit parameters controlling the number of bits used); LR Schedule: Default learning rate α= 1e-3 for all tasks, except language modeling where α= 1e-2; Training: Pseudo Quantization Noise (PQN), differentiable method for model compression for quantizing model parameters without gradient approximations; Attention: Not explicitly mentioned, assumed to be standard transformer attention.; Special Algorithms: DIFFQ: Differentiable Model Compression via Pseudo Quantization Noise; Initialization: trainable parameter l is initialized so that b = binit = 8",,,,,,,,,,
295,DeLight,"Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, Hannaneh Hajishirzi",2020/08/03,2020,DeLighT: Deep and Light-weight Transformer,98.0,,https://arxiv.org/abs/2008.00623,9.90E+07,,2.4e+19,62.14,,,0.0,103000000.0,3.80E+18,0.0,103000000.0,WikiText-103,24.14,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/sacmehta/delight,,,,1,,260000,"Architecture: Transformer-based architecture with DeLighT transformations and block-wise scaling. The architecture stacks DeLighT blocks in both encoder and decoder, with each block containing three sub-layers: a DeLighT transformation, a single-head attention, and a position-wise light-weight feed-forward network.; Optimizer: Adam; LR Schedule: Not explicitly mentioned; Training: Residual connections; Attention: Single-head attention; Regularization: Label smoothing, Dropout (explicitly mentioned as lower in DeLighT than baseline transformers), Input mixer connection; Special Algorithms: DeLighT transformation, Block-wise scaling, Group linear transformations (GLTs), Feature shuffling; Initialization: Sinusoidal positional encodings; Other: Adaptive input and output embeddings, Tied input and output weights",,,,,,,,,,
296,T2R + Random Init,"Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",2021/03/24,2021,Finetuning Pretrained Transformers into RNNs,30.0,,https://arxiv.org/abs/2103.13076,4.50E+08,,6.1e+19,205.48,,,1.0,103000000.0,5.71E+19,,103000000.0,,20.8,,,0.0,0.0,0,0,Transformer,ELU,https://github.com/jungokasai/T2R/,University of Washington; Allen Institute for AI,Industry - Academia Collaboration,,1,,,"Architecture: Transformer architecture converted to an efficient RNN (Transformer-to-RNN or T2R). The transformer consists of multihead attention, feedforward networks, and layer normalization modules. The attention mechanism is modified by replacing the dot-then-exponential similarity function with a single-layer MLP feature mapping.; Optimizer: Adam, with beta values (0.9, 0.98); LR Schedule: The learning rate is linearly warmed up and then annealed using a cosine learning rate schedule with cycles for the language modeling task. For machine translation, inverse square root schedule.; Training: Teacher forcing, Mixed precision training, Distributed training; Attention: Multi-head attention. The original softmax attention is replaced by a linear complexity recurrent alternative using MLP feature mapping.  Different MLP parameters are used for different attention heads.; Regularization: Dropout (0.2 for language modeling, 0.3 for machine translation), Layer dropout (0.2 for language modeling), Weight decay (0.01 for machine translation), Label smoothing (0.1 for machine translation); Special Algorithms: Transformer-to-RNN (T2R): converts a pretrained transformer into an efficient RNN of linear time and constant space complexity.; Initialization: Initialized with a pretrained transformer model, then finetuned.  Random initialization is also performed for comparison.; Other: Swap-then-finetune procedure: The attention mechanism of a pre-trained transformer is modified, and then the model is finetuned with the task objective.",,,,,,,,,,
297,top-down frozen classifier,"Shucong Zhang, Cong-Thanh Do, Rama Doddipatla, Erfan Loweimi, Peter Bell, Steve Renals",2021/02/09,2021,Train your classifier first: Cascade Neural Networks Training from upper layers to lower layers,2.0,0.0,https://arxiv.org/pdf/2102.04697,UNK,,,UNK,,,0.0,,#VALUE!,,0.0,,,65.2,,0.0,0.0,0,0,,,,,,,0,,,"Architecture: AWD-LSTM; Optimizer: Not explicitly mentioned, likely the same as used in original AWD-LSTM paper; LR Schedule: Not explicitly mentioned, likely the same as used in original AWD-LSTM paper; Training: Top-down layer-wise training, Freezing layers, Reinitializing layers; Attention: Not applicable; Regularization: Implicit overlap with regularization techniques used in the training of AWD-LSTM; Special Algorithms: Greedy Layer-wise Top-down Training (Algorithm 1); Initialization: Using the same initial weights as baseline models (as indicated by 'same initial weights as the baseline' when retraining); Other: Tied input/output embedding",,,,,,,,,,
298,Transformer-XL DeFINE (107M),"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",2019/11/27,2019,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,21.0,,https://arxiv.org/abs/1911.12385,1.07E+08,,5.2e+18,20,,,0.0,103000000.0,1.33E+18,,103000000.0,WikiText-103; Penn Treebank,25.72,,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,260000,"Architecture: Uses a hierarchical structure with novel skip-connections for learning deep token embeddings (DeFINE) efficiently. DeFINE replaces standard embedding layers. The DeFINE unit is composed of HGT transformations. DeFINE approximates complicated token embedding function with fewer parameters using a Map-Expand-Reduce (MER) principle. The Expand step applies a Hierarchical Group Transformation (HGT) with sparse and dense connections. Includes a split layer and a mixer.; Optimizer: SGD; LR Schedule: LR reduction with factor of 10, and step [15]; Regularization: Weight decay; Special Algorithms: Hierarchical Group Transformation (HGT), Map-Expand-Reduce principle (MER); Other: Weight tying between the adaptive inputs and outputs",,,,,,,,,,
299,Adaptive LSTM + DeFINE,"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",2019/11/27,2019,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,21.0,,https://arxiv.org/abs/1911.12385,4.87E+07,,6.2e+18,20,,,0.0,103000000.0,6.02E+17,,103000000.0,WikiText-103; Penn Treebank,35.94,,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,260000,"Architecture: Adaptive LSTM (when used as basemodel) augmented with DeFINE which replaces standard embedding layers. DeFINE leverages a hierarchical group transformation (HGT) with novel skip-connections.; Optimizer: SGD (for LSTM-based language models, see Table 9); LR Schedule: Unspecified, but LR reduction is used (for LSTM-based language models, see Table 9) with factor 10 at step 15; Attention: Not applicable for the Adaptive LSTM itself, but applicable for the Transformer model that DeFINE can be applied to.; Regularization: Weight decay (for LSTM-based language models, see Table 9), Dropout (for LSTM-based language models, see Table 9); Special Algorithms: Hierarchical Group Transformation (HGT) for deep token representations, Map-Expand-Reduce (MER) principle, Novel skip-connections that establish a direct link with the input layer at every level of its hierarchy; Initialization: Not mentioned",,,,,,,,,,
300,Base LM + kNN LM + Continuous Cache,"Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis",2019/11/01,2019,Generalization through Memorization: Nearest Neighbor Language Models,410.0,,https://arxiv.org/abs/1911.00172,2.47E+08,,7.3e+18,200.00,,,0.0,103000000.0,3.05E+19,,103000000.0,WikiText-103,15.79,,,0.0,0.0,1,0,Transformer,Transformer-XL,https://github.com/urvashik/knnlm,DeepMind; University of Oxford,,,1,,,"Architecture: Decoder-only Transformer with 16 layers, each with 16 self-attention heads, 1024 dimensional hidden states, and 4096 dimensional feedforward layers.; Optimizer: Same optimization as described by Baevski & Auli (2019); LR Schedule: Following Baevski & Auli (2019); Attention: Multi-head self-attention with 16 heads.; Special Algorithms: k-Nearest Neighbors Language Model (kNN-LM), Continuous Cache (Grave et al., 2017c); Other: Adaptive inputs and adaptive softmax (for WIKITEXT-103 experiments). Weight tying (Press & Wolf, 2017) for the WIKITEXT-103 experiments. Squared L2 distance as a distance function d(,) for KNN.",,,,,,,,,,
301,Transformer-XL DeFINE (141M),"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",2019/11/27,2019,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,21.0,,https://arxiv.org/abs/1911.12385,1.41E+08,,6.2e+18,20,,,0.0,103000000.0,1.75E+18,,103000000.0,WikiText-103; Penn Treebank,24.17,,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,260000,"Architecture: Uses a hierarchical structure with novel skip-connections, Deep Factorized Input token Embeddings (DeFINE). DeFINE replaces standard embedding layers, leaving the rest of the model untouched.; Optimizer: SGD (for LSTM-based language models - based on Table 9); LR Schedule: LR reduction (factor, steps) 10, [15] (for LSTM-based language models - based on Table 9); Attention: If integrating with Transformer-XL, uses the attention mechanism inherent to that model.; Regularization: Weight decay 0 (for LSTM-based language models - based on Table 9), Dropout (Same as Merity et al. (2018a) - for LSTM based language models based on Table 9); Special Algorithms: Hierarchical Group Transformation (HGT), Map-Expand-Reduce (MER) principle; Initialization: Not Mentioned; Other: Adaptive inputs and adaptive softmax (when used with RNN-based sequence models), Weight tying between adaptive inputs and outputs (when used with RNN-based sequence models), Novel skip-connection that establishes a direct link with the input layer at every level of its hierarchy, Group transformation splits the input into g groups, each of which is processed independently using a linear transformation",,,,,,,,,,
302,AWD-LSTM + DeFINE,"Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi",2019/11/27,2019,DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence Modeling,21.0,,https://arxiv.org/abs/1911.12385,2.00E+07,,,20,,,0.0,929000.0,2.23E+15,,929000.0,WikiText-103; Penn Treebank,,,54.2,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,10000,"Architecture: AWD-LSTM with DeFINE. DeFINE uses a hierarchical structure with novel skip-connections. DeFINE replaces standard embedding layers in existing sequence models, leaving the rest of the model untouched.; Optimizer: SGD; LR Schedule: LR reduction with factor of 10 at specified steps; Regularization: Weight decay, Dropout; Special Algorithms: Hierarchical Group Transformation (HGT), Map-Expand-Reduce (MER) principle, Novel skip-connections",,,,,,,,,,
303,AWD-LSTM-DRILL + dynamic evaluation† (PTB),"Nikolaos Pappas, James Henderson",2019/05/14,2019,Deep Residual Output Layers for Neural Language Generation,7.0,,https://arxiv.org/abs/1905.05513,2.40E+07,,,1000,,,0.0,929000.0,1.34E+17,,929000.0,Penn TreeBank,,,49.4,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/idiap/drill,,,,1,?,10000,"Architecture: Deep residual nonlinear output mapping from word embeddings to the joint input-output space. Uses a deep neural network with k layers which takes the label embedding E as input and outputs its deep label mapping at the last layer. The general form is the dual nonlinear mapping of Eq. 4: p(yty-1) x exp(gout(E)gin(ht) + b).; Optimizer: Not explicitly stated, but comparable setups use Adam as described in dependency papers; LR Schedule: Not explicitly stated, but comparable setups use learning rate decay as described in dependency papers; Training: Dropout between layers of the deep residual output mapping, Residual connections to the word embeddings, Optional residual connections to the outputs of previous layers, Variational Dropout; Attention: Not applicable, focus is on output layer; Regularization: Dropout (standard or variational); Special Algorithms: Deep Residual Output Layers (DRILL); Initialization: Uniform weight initialization in the interval [-0.1, 0.1]; Other: Output layer parameterisation which has the same general form as the one with the dual nonlinear mapping. It preserves the property of being a generalization of output layers based on bilinear mapping and weight tying.",,,,,,,,,,
304,AWD-LSTM-DRILL + dynamic evaluation† (WT2),"Nikolaos Pappas, James Henderson",2019/05/14,2019,Deep Residual Output Layers for Neural Language Generation,7.0,,https://arxiv.org/abs/1905.05513,3.40E+07,,,1000,,,0.0,2080000.0,4.24E+17,,2080000.0,WikiText-2,,42.0,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/idiap/drill,,,,1,?,33278,"Architecture: AWD-LSTM - 3-layer LSTM with 400-dimensional embeddings and 1150-dimensional hidden states. The paper introduces a deep residual nonlinear output mapping from word embeddings to the joint input-output space.; Optimizer: Not explicitly mentioned, but the same optimization techniques as Merity et al., 2017 were used.; LR Schedule: Not explicitly mentioned, but the same LR schedule as Merity et al., 2017 were used.; Attention: Not applicable; Regularization: Dropout between the k layers of the deep residual output mapping. Both standard and variational dropout were explored., Residual connections to the word embeddings and previous output layers in the deep residual output layer, L1 and L2 weight regularization (same as Merity et al., 2017), Weight decay; Special Algorithms: Deep Residual Output Layers (DRILL) - a novel deep residual output mapping for language generation.; Initialization: Uniform weight initialization in the interval [-0.1, 0.1]",,,,,,,,,,
305,True-Regularization+Finetune,"Yangyang Shi, Mei-Yuh Hwang, Xin Lei, Haoyu Sheng",2019/04/08,2019,Knowledge Distillation For Recurrent Neural Network Language Modeling With Trust Regularization,24.0,,https://arxiv.org/pdf/1904.04163,7.00E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,54.0,0.0,0.0,0,0,Recurrent,RNN,,,,,1,,,"Architecture: Teacher: High-rank RNNLM with Mixture of Softmaxes (MoS). Student: LSTM; Optimizer: Not explicitly mentioned in the paper, but the teacher model used techniques from Merity et al. (2017) [24, 11], which use SGD.; LR Schedule: Based on [24,10], it is likely that the teacher model used a learning rate scheduler, though it is not explicitly mentioned. The student model training procedure doesn't mention a learning rate schedule.; Training: Knowledge Distillation, Trust Regularization; Attention: Not applicable; Regularization: Teacher: Three different dropouts (DropConnect, Variational dropout, Embedding dropout), Activation Regularization, Student: Trust Regularization; Special Algorithms: Trust Regularization; Initialization: Based on [24,10], the teacher model uses an initialization method that is not explicitly mentioned in this paper. The student model does not appear to use a specific initialization method.; Other: Mixture of Softmaxes",,,,,,,,,,
306,WeNet (PTB),"Zhiheng Huang, Bing Xiang",2019/04/08,2019,WeNet: Weighted Networks for Recurrent Network Architecture Search,5.0,,https://arxiv.org/pdf/1904.03819,2.30E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,54.8,0.0,0.0,0,0,NAS,WeNet,,"Stanford University; SambaNova Systems; Peking University; Adobe; University at Buffalo, SUNY",Industry - Academia Collaboration,,1,,,"Architecture: Recurrent networks with cells consisting of an ordered sequence of L (number of levels) nodes. Each node can be connected to a previous node through an activation function (tanh, relu, sigmoid, or identity). WeNets consist of a collection of such candidate networks, each with a weight.; Optimizer: Adam; LR Schedule: Not explicitly mentioned, but the paper mentions starting with SGD and triggering ASGD using the same protocol as in (Yang et al., 2017; Merity et al., 2017) for the recurrent architecture evaluation.; Training: Mini-batch training, Joint training of candidate networks via WeNet structure, BPTT length 35 during architecture search, Averaged SGD (ASGD) during recurrent architecture evaluation; Attention: Not applicable; Regularization: Weight decay 5 during architecture search, 8 × 10-7 during recurrent architecture evaluation, Variational dropout (Gal & Ghahramani, 2016) of 0.2 to word embeddings, 0.75 to the cell input, and 0.25 to all the hidden nodes. A dropout of 0.75 is also applied to the output layer during architecture search; Special Algorithms: WeNet - Weighted Networks for architecture search: An algorithm that combines multiple network architectures, each assigned a weight, and updates these weights during training, Random Network Generalization, Architecture Search Procedure to search for optimal architectures; Initialization: Random network initialization; Other: Network Batching: Similar to a mini-batch, but processes multiple networks at the same time for architecture search.",,,,,,,,,,
307,dense-IndRNN+dynamic eval,"Shuai Li, Wanqing Li, Chris Cook, Yanbo Gao",2019/10/11,2019,Deep Independently Recurrent Neural Network (IndRNN),45.0,,https://arxiv.org/abs/1910.06251,4.41E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,Penn TreeBank,,,49.95,0.0,1.0,0,0,Recurrent,,,,,,1,?,?,"Architecture: Independently Recurrent Neural Network (IndRNN). Different architectures explored: basic stacked IndRNN, residual IndRNN (res-IndRNN), densely connected IndRNN (dense-IndRNN).; Optimizer: Adam; LR Schedule: Learning rate is reduced by a factor of 5 when the accuracy (or loss) on the validation data no longer improves (drops) (with patience set to 100).; Training: Batch normalization, Dropout, Weight decay; Attention: Not applicable; Regularization: Weight decay of 10^-4 is used for the weight parameters (without applying to the recurrent weight and bias)., Dropout used after each layer.; Special Algorithms: Hadamard product for recurrent connections, Skip-connection in residual IndRNN, Concatenation in densely connected IndRNN, CUDA optimization for faster computation; Initialization: For tasks with output at every timestep such as the language modeling problem, the recurrent weights of all layers were initialized uniformly in the range [0,1]. For tasks such as classification where only the output at the last time step in the last layer is used, the recurrent weights of the last layer were initialized uniformly in the range [(epsilon/(T-t))^0.5, 1/(epsilon * (T-t))^0.5] as described in Subsection 3.2.; Other: Recurrent weights are constrained to be in the range of |un| <= 1/(T-t)^0.5 for IndRNN with ReLU as analyzed in Subsection 3.1., Weight tying used in word-level Penn Treebank experiments.",,,,,,,,,,
308,DEQ-TrellisNet,"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2019/09/03,2019,Deep Equilibrium Models,496.0,,https://arxiv.org/abs/1909.01377,1.10E+08,,,12,,,0.0,103000000.0,8.16E+17,0.0,103000000.0,Penn TreeBank,,,57.1,0.0,1.0,0,0,Transformer,DEQ,https://github.com/locuslab/deq,,,,1,?,10000,"Architecture: DEQ is applied to self-attention transformers and trellis networks. Uses weight-tied temporal convolutions and weight-tied multi-head self-attention.; Optimizer: Not specified; LR Schedule: Not specified; Training: Constant memory during training and prediction, Analytical backpropagation through the equilibrium point, Weight tying, Quasi-Newton Methods; Attention: Memory-augmented self-attention is used in the transformer variant, and relative positional embeddings are added to the self-attention operation.; Regularization: weight-tying, dropout (RNN variational dropout); Special Algorithms: Deep Equilibrium Model (DEQ), Broyden's method; Initialization: Parameters of fe (nonlinear transformation function) are initialized by sampling from N(0, 0.05).; Other: Employs Broyden's method to approximate the inverse Jacobian. Broyden iterations stopped when norm of function falls below a tolerance or max iterations reached",,,,,,,,,,
309,base LM+GNN+kNN,"Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, Jiwei Li",2021/10/17,2021,GNN-LM: Language Modeling based on Global Contexts via GNN,22.0,,https://arxiv.org/abs/2110.08743,2.74E+08,,7.3e+18,,,,0.0,103000000.0,0.00E+00,103000000.0,206000000.0,WikiText-103,14.8,,,0.0,0.0,0,0,Transformer,Transformer-XL,https://github.com/ShannonAI/GNN-LM,IBM Research,Industry,,1,,,"Architecture: GNN-LM. Extends a base language model with a Graph Neural Network (GNN). The base LM is a Transformer-XL or deep Transformer language model with adaptive embeddings. The GNN is a self-attention augmented GNN with 3 layers that sits on top of the pretrained base LM.; Optimizer: Not explicitly mentioned in the provided context.; LR Schedule: Not explicitly mentioned in the provided context.; Training: Data Leakage Prevention: Ignores neighboring nodes within a certain interval during graph construction to prevent leakage., Feature Quantization: Uses product quantization to compress the high-dimensional representation of each token for caching; Attention: Self-attention augmented GNN, extending the self-attention mechanism to inter-context edges.; Regularization: Adding intra edges between adjacent tokens can alleviate overfitting; Special Algorithms: k-Nearest Neighbors (kNN) for retrieving similar contexts., The retrieval step uses Faiss for fast approximate kNN search.; Initialization: The node representations of the GNN are initialized by a pretrained language model.; Other: Graph Construction: Constructs a directed heterogeneous graph where nodes are tokens and edges represent connections between tokens. Two types of nodes: original nodes from input context and neighbor nodes from extracted contexts; two types of edges: inter-context and intra-context edges., The architecture is designed to allow the language model to reference similar contexts in the entire training corpus., The GNN aggregates information from the input and the extracted contexts.",,,,,,,,,,
310,"DEQ-Transformer (Medium, Adaptive Embedding)","Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2019/09/03,2019,Deep Equilibrium Models,496.0,,https://arxiv.org/abs/1909.01377,1.10E+08,,,12,,,0.0,103000000.0,8.16E+17,0.0,103000000.0,WikiText-103,23.2,,,0.0,1.0,0,0,Transformer,DEQ,https://github.com/locuslab/deq,,,,1,?,260000,"Architecture: DEQ instantiated with two feedforward sequence models: trellis networks (weight-tied temporal convolutions) and memory-augmented universal transformers (weight-tied multi-head self-attention); Optimizer: Not explicitly mentioned, but SGD updates are referenced in equation 9.; LR Schedule: Not explicitly mentioned.; Training: Broyden's method is used to approximate inverse Jacobian, Implicit differentiation; Attention: Memory-augmented transformer with multi-head self-attention; Regularization: Weight tying is used as regularization that stabilizes training and supports generalization., Layer normalization; Special Algorithms: Deep Equilibrium Model (DEQ); Initialization: Parameters of f_theta are initialized by sampling from N(0, 0.05).; Other: Trellis networks use LSTM gated activation for nonlinearity., Broyden iterations are stopped when either the norm of go falls below a tolerance or when the maximum number of iterations is reached., The authors adapted RNN variational dropout scheme to feedforward networks by applying the same mask at all levels.",,,,,,,,,,
311,"GPT-2 (117M, SLW 110K)","Conglong Li, Minjia Zhang, Yuxiong He",2021/08/13,2021,Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training,20.0,,https://arxiv.org/abs/2108.06084,1.17E+08,,1.3e+20,1.10,,,0.0,157000000000.0,1.21E+20,,157000000000.0,Wikipedia; CC-Stories; RealNews; OpenWebtext,26.03,,,1.0,1.0,0,1,Transformer,GPT,,,,,1,?,?,"Architecture: Transformer with 12 layers, 768 hidden size, 12 attention heads for 117M model, and 48 layers, 1600 hidden size, 25 attention heads for 1.5B model; Optimizer: Adam (β₁ = 0.9, β₂ = 0.999, ε = 1e-08); LR Schedule: Linear warmup of 3K steps and a single cycle cosine decay (min. learning rate 1e-05). Token-wise cosine decay schedule introduced when using sequence length warmup.; Training: Mixed precision/FP16 training, Gradient clipping at 1.0; Attention: Multi-head attention. 12 attention heads for 117M model and 25 attention heads for 1.5B model.; Regularization: Weight decay 0.01; Special Algorithms: Sequence Length Warmup (SLW): starts training with short sequences and gradually increases the length; Initialization: Same random seed; Other: Truncation-based implementation for sequence length warmup, Step-wise linear function for pacing function of sequence length warmup",,,,,,,,,,
312,"GPT-2 (1.5B, Curriculum Learning 45K)","Conglong Li, Minjia Zhang, Yuxiong He",2021/08/13,2021,Curriculum Learning: A Regularization Method for Efficient and Stable Billion-Scale GPT Model Pre-Training,20.0,,https://arxiv.org/abs/2108.06084,1.50E+09,,6e+20,2.20,,,0.0,157000000000.0,3.11E+21,,157000000000.0,Wikipedia; CC-Stories; RealNews; OpenWebtext,13.72,,,1.0,1.0,0,0,Transformer,GPT,,,,,1,?,?,"Architecture: Transformer with 12 layers, 768 hidden size, 12 attention heads (for 117M parameter model) and 48 layers, 1600 hidden size, 25 attention heads (for 1.5B parameter model); Optimizer: Adam (β₁ = 0.9, β2 = 0.999, ε = 1 × 10−8); LR Schedule: Linear warmup followed by single cycle cosine decay; Training: Mixed precision/FP16 training, Sequence Length Warmup (SLW), Truncation-based implementation for variable sequence length; Attention: Multi-head attention; Regularization: Weight decay 0.01, Gradient clipping at 1.0; Special Algorithms: Sequence Length Warmup (SLW); Initialization: Same random seed",,,,,,,,,,
313,RETRO-7B,"Sebastian Borgeaud†, Arthur Mensch†, Jordan Hoffmann†, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero,Karen Simonyan, Jack W. Rae‡, Erich Elsen‡ and Laurent Sifre",2021/12/08,2021,Improving language models by retrieving from trillions of tokens,315.0,,https://arxiv.org/abs/2112.04426,7.50E+09,,7.5e+21,,,,0.0,2000000000000.0,0.00E+00,,2000000000000.0,MassiveText,21.53,,,1.0,0.0,0,0,Transformer,GPT,,OpenAI,Industry,,1,,,"Architecture: Encoder-decoder transformer architecture with chunked cross-attention. The retrieved tokens are fed into an encoder Transformer, which computes the encoded neighbours set. The transformer decoder then interleaves RETRO-blocks and standard Transformer blocks. The retrieval encoder is a bi-directional transformer conditioned on the activations of the retrieving chunk through cross-attention layers.; Optimizer: AdamW; LR Schedule: Cosine cycle length matching the total number of training tokens, with a linear increase for the first 750 steps, and a weight decay.; Training: ZeRO to shard the optimizer state; Attention: Chunked cross-attention, multi-head attention; Regularization: Weight decay 0.1, Dropout; Other: Relative positional encodings in self-attention and chunked cross-attention, Chunking of the input sequence and retrieval of k-nearest neighbor chunks based on BERT embeddings, Frozen BERT embeddings to compute keys for retrieval",,,,,,,,,,
314,GPT2+CoreLM+Fine-Tuning,"Nikolaos Stylianou, Ioannis Vlahavas",2021/11/04,2021,CoreLM: Coreference-aware Language Model Fine-Tuning,2.0,1.0,https://arxiv.org/pdf/2111.02687,1.32E+08,,,10,,,0.0,4000000.0,3.17E+16,,4000000.0,,29.51,31.8,,1.0,1.0,0,0,Transformer,GPT,,,,,1,GPT2 tokenizer,?,"Architecture: GPT2 with N stacked Transformer decoder blocks, each consisting of a multi-headed attention layer and a position-wise feedforward layer with residual connections and layer normalizations, extended with an Entity-Gating layer after the Transformer decoders.; Optimizer: Rectified Adam; LR Schedule: linearly decaying learning rate with 100 steps of warm up; Training: Fine-tuning; Attention: Multi-headed attention in the Transformer decoder blocks, masked multi-headed self attention layer in the decoder to prevent leftward information flow, masked multi-headed entity attention layer with 12 heads in the Entity-Gating layer; Regularization: Dropout of 10% between layers in the Entity-Gating layer; Special Algorithms: Entity-Gating layer with a learnable gating mechanism to control the flow of information; Initialization: Entity representations are initialized as a vector of ones; Other: Gradients applied only to the input layers (We and Wp), output layers (the language modeling head) and the Entity-Gating layer. All 12 GPT2 Transformer layers are frozen.",,,,,,,,,,
315,RNNLM + Dynamic KL Regularization,"Thanapon Noraset, David Demeter, Doug Downey",2018/01/01,2018,Controlling Global Statistics in Recurrent Neural Network Text Generation,6.0,,https://ojs.aaai.org/index.php/AAAI/article/view/11993,2.77E+07,,,20,,,0.0,929000.0,3.09E+15,,929000.0,Penn Treebank,,,77.8,0.0,1.0,0,0,Recurrent,RNN,,"Northeastern University, Shenyang, China; NiuTrans Research, Shenyang, China; Chinese Academy of Sciences; Kingsoft AI Lab, Beijing, China",Industry - Academia Collaboration,,1,?,?,,,,,,,,,,,
316,E-SPA,"Bobby He, James Martens, Guodong Zhang, Aleksandar Botev, Andrew Brock, Samuel L Smith, Yee Whye Teh",2023/02/20,2023,Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation,3.0,1.0,https://arxiv.org/pdf/2302.10322.pdf,2.43E+08,,,UNK,,,0.0,103000000.0,#VALUE!,,103000000.0,,19.7,,,0.0,0.0,0,0,Transformer,Transformer,,,,,1,,,"Architecture: Transformer models, specifically decoder-only transformers. Focus on skipless transformers (no skip connections, \alpha = 0 and \beta = 1) and vanilla transformers (skipless and no normalisation layers). Attention-only models (transformers without MLP blocks) are analysed for simplicity.; Optimizer: Adam optimizer with global gradient clipping of 0.1; LR Schedule: Linear learning rate warm-up period of 4000 steps, which increases from 0 to a maximum value, followed by a one-cycle cosine learning rate schedule which reaches 0 at the final iteration.; Training: Gradient Clipping; Attention: Multi-head attention; Special Algorithms: Exponential Signal Preserving Attention (E-SPA), Uniform Signal Preserving Attention (U-SPA), Value-SkipInit; Initialization: Orthogonal initialisation (scaled-corrected uniform orthogonal initialization)",,,,,,,,,,
317,RNNLM + Dynamic KL Regularization (WT2),"Thanapon Noraset, David Demeter, Doug Downey",2018/01/01,2018,Controlling Global Statistics in Recurrent Neural Network Text Generation,6.0,,https://ojs.aaai.org/index.php/AAAI/article/view/11993,8.76E+07,,,20,,,0.0,2080000.0,2.19E+16,,2080000.0,,,86.8,,0.0,1.0,0,0,Recurrent,RNN,,,,,1,?,?,,,,,,,,,,,
318,Neural cache model (size=2000),"Edouard Grave, Armand Joulin, Nicolas Usunier",2016/12/13,2016,Improving Neural Language Models with a Continuous Cache,302.0,,https://arxiv.org/abs/1612.04426,UNK,,,,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-103,,68.9,,0.0,1.0,1,0,Recurrent,LSTM,,,,,0,,,"Architecture: LSTM (with 1024 units in medium scale experiments); Optimizer: Adagrad (with learning rate 0.2); LR Schedule: constant; Training: gradient clipping (norm 0.1), truncated backpropagation through time, dropout; Regularization: dropout (probability varies from 0.65 for small datasets to 0.45 and 0.25 for larger datasets); Special Algorithms: Neural Cache Model; Initialization: uniform sampling in the range [-0.05, 0.05]; Other: Adaptive Softmax for faster training in larger datasets (text8, wikitext103)",,,,,,,,,,
319,CT-MoS (PTB),"Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",2020/12/25,2020,Contextual Temperature for Language Modeling,6.0,,https://arxiv.org/pdf/2012.13575,2.40E+07,,,1000,,,0.0,929000.0,1.34E+17,,929000.0,,,,54.69,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,10000,"Architecture: 3-layer LSTM; Optimizer: Not explicitly mentioned but the code links to a MoS implementation, likely Adam or SGD with momentum as typical for LSTM models.; LR Schedule: Not explicitly mentioned, likely the code repo for MoS would give details; Training: Activation Regularization, Temporal Activation Regularization, Weight Decay; Attention: Not applicable; Regularization: Weight Decay, Activation Regularization, Temporal Activation Regularization; Special Algorithms: Contextual Temperature, Loss Scaling; Initialization: Not mentioned",,,,,,,,,,
320,CT-MoS + DynamicEval (WT2),"Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",2020/12/25,2020,Contextual Temperature for Language Modeling,6.0,,https://arxiv.org/pdf/2012.13575,4.50E+07,,,1000,,,0.0,2080000.0,5.62E+17,,2080000.0,,,40.96,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,33000,"Architecture: 3-layer LSTM with embedding sizes of 960-960-620 (PTB) or 1150-1150-650 (WT2). Used AWD-LSTM to parameterize contextual temperature.; Optimizer: Not explicitly stated in this paper, but inferred to be related to the MoS paper, which likely used SGD with Nesterov momentum (see Merity et al., 2018).; LR Schedule: Not explicitly stated, but dynamic evaluation is mentioned as an approach.; Training: Cross Entropy Loss, Activation Regularization (AR), Temporal Activation Regularization (TAR), Weight Decay (WD); Attention: Not applicable; Regularization: Activation Regularization (AR), Temporal Activation Regularization (TAR), Weight Decay (WD); Special Algorithms: Contextual Temperature, Mixture of Softmaxes (MoS) with contextual temperature (CT-MoS), Loss Scaling (LS) to balance cross entropy with regularization terms.; Initialization: Not explicitly stated",,,,,,,,,,
321,CT-MoS (WT2),"Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",2020/12/25,2020,Contextual Temperature for Language Modeling,6.0,,https://arxiv.org/pdf/2012.13575,4.50E+07,,,1000,,,0.0,2080000.0,5.62E+17,,2080000.0,,,62.21,,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,,33000,"Architecture: 3-layer LSTM with embedding sizes of 960-960-620 (PTB) or 1150-1150-650 (WT2); Optimizer: Not explicitly mentioned, but they follow training configurations in MoS paper and their github (https://github.com/zihangdai/mos).; LR Schedule: Not explicitly mentioned, but they follow training configurations in MoS paper and their github (https://github.com/zihangdai/mos).; Regularization: Activation Regularization (AR), Temporal Activation Regularization (TAR), Weight Decay (WD); Special Algorithms: Contextual Temperature; Initialization: Not mentioned; Other: Mixture of Softmaxes (MoS) with number of mixtures M=15, Loss Scaling",,,,,,,,,,
322,CT-MoS + DynamicEval (PTB),"Pei-Hsin Wang, Sheng-Iou Hsieh, Shih-Chieh Chang, Yu-Ting Chen, Jia-Yu Pan, Wei Wei, Da-Chang Juan",2020/12/25,2020,Contextual Temperature for Language Modeling,6.0,,https://arxiv.org/pdf/2012.13575,2.40E+07,,,1000,,,0.0,929000.0,1.34E+17,,929000.0,,,,47.42,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,?,?,"Architecture: 3-layer LSTM with embedding sizes of 960-960-620 (PTB) or 1150-1150-650 (WT2); Optimizer: Not explicitly mentioned; LR Schedule: Not explicitly mentioned; Training: Activation Regularization (AR), Temporal Activation Regularization (TAR), Weight Decay (WD), Loss Scaling (LS); Attention: Not applicable; Regularization: Activation Regularization (AR), Temporal Activation Regularization (TAR), Weight Decay (WD); Special Algorithms: Contextual Temperature, Contextual Temperature Mixture of Softmaxes (CT-MoS); Initialization: Not explicitly mentioned; Other: Softmax layer for probability distribution, Mixture of Softmaxes (MoS)",,,,,,,,,,
323,RNN,"Tomas Mikolov, Geoffrey Zweig",2012/12/01,2012,Context dependent recurrent neural network language model,716.0,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,6.00E+06,,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,Penn TreeBank,,,124.7,0.0,1.0,0,0,Recurrent,RNN,,,,,1,?,?,,,,,,,,,,,
324,RNN+LDA+KN5+cache,"Tomas Mikolov, Geoffrey Zweig",2012/12/01,2012,Context dependent recurrent neural network language model,716.0,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,9.00E+06,,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,Penn TreeBank,,,92.0,0.0,1.0,1,0,Recurrent,RNN,,,,,1,?,?,,,,,,,,,,,
325,RNN+LDA,"Tomas Mikolov, Geoffrey Zweig",2012/12/01,2012,Context dependent recurrent neural network language model,716.0,,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf,7.00E+06,227,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,Penn TreeBank,,,113.7,0.0,1.0,0,0,Recurrent,RNN,,,,,1,?,?,,,,,,,,,,,
326,Transformer + GFM,"Hao Yu, Jianxin Wu",2022/12/01,2022,"Compressing Transformers: Features Are Low-Rank, but Weights Are Not",0.0,1.0,https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf,1.85E+08,1.03E+08,8.04e+18,UNK,,,0.0,103000000.0,#VALUE!,,103000000.0,,20.05,,,0.0,1.0,0,0,Transformer,Transformer-XL,,,,,1,Word-level,260000,,,,,,,,,,,
327,1-layer-LSTM,"H. T. Kung, Bradley McDanel, Sai Qian Zhang",2020/07/13,2020,Term Revealing: Furthering Quantization at Run Time on Quantized DNNs,9.0,,https://arxiv.org/pdf/2007.06389,8.65E+07,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,,,86.85,,0.0,0.0,0,0,Recurrent,LSTM,,,,,1,,,"Architecture: 1-layer LSTM with 650 hidden units and word embedding of length 650.; LR Schedule: A word embedding of length 650, and a dropout rate of 0.5, following the PyTorch word language model examples.; Training: dropout 0.5; Special Algorithms: Term Revealing (TR), Hybrid Encoding for Shortened Expressions (HESE)",,,,,,,,,,
328,Mogrifier RLSTM (PTB),Gábor Melis,2022/11/03,2022,Circling Back to Recurrent Models of Language,0.0,1.0,https://arxiv.org/pdf/2211.01848,2.40E+07,,,400,,,0.0,929000.0,5.35E+16,,929000.0,,,,42.9,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,?,?,"Architecture: 2-layer Mogrifier RLSTM with residual connections. Cell output is dropped out before being added to the residual branch.; Optimizer: Rectified Adam (RAdam); LR Schedule: Learning rate is multiplied by 0.9 if training diverges and weights are reset to the previous best checkpoint.; Training: Backpropagation Through Time (BPTT), Multiple dropout samples for averaging model predictions in objective function, Early Stopping; Attention: None; Regularization: Dropout on cell output, Input, cell, state and output dropout masks; Special Algorithms: Rewired LSTM (RLSTM): a tweaked LSTM cell with modifications to the forget gate, output gate, and input gate capping., Mogrifier: a function used to process input and hidden state., Two-Tailed Averaging (2TA) for weight averaging; Initialization: Forget gates are initialized with Chrono init (bf ~ lnU(1, Tmax – 1)); Other: Dynamic Evaluation: weights depend on the context during evaluation only. Fast weights are eschewed at training time, using only slow weights.",,,,,,,,,,
329,WeNet (WT2),"Zhiheng Huang, Bing Xiang",2019/04/08,2019,WeNet: Weighted Networks for Recurrent Network Architecture Search,5.0,,https://arxiv.org/pdf/1904.03819,3.30E+07,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,,,66.6,,0.0,0.0,0,0,NAS,WeNet,,,,,1,,,"Architecture: Weighted Networks (WeNets), a specific instance of mixture of experts. It consists of a number of networks, with each assigned a weight, connecting to the same input and output layer. The cell is a directed acyclic graph consisting of an ordered sequence of L nodes with tanh, relu, sigmoid, identity activation functions.; Optimizer: Adam; LR Schedule: Not mentioned; Training: BPTT, batch normalization, variational dropout; Attention: Not applicable; Regularization: weight decay 5, variational dropout of 0.2 to word embeddings, 0.75 to the cell input, and 0.25 to all the hidden nodes. A dropout of 0.75 is also applied to the output layer.; Special Algorithms: Architecture Search via Weighted Networks; Initialization: Not mentioned; Other: Stochastic Gradient Descent (SGD) is used to update both the model parameters and model weights., Averaged SGD (ASGD) is used for the evaluation of the recurrent architecture with the discovered cell.",,,,,,,,,,
330,AFP+FPI (WT2),"Zhengxiong Wang, Anton Ragni",2021/06/04,2021,Approximate Fixed-Points in Recurrent Neural Networks,1.0,0.0,https://arxiv.org/pdf/2106.02417,8.02E+04,,,40,,,0.0,2080000.0,4.00E+13,,2080000.0,WikiText-2,,149.35,,0.0,0.0,0,0,Recurrent,AFP,,,,,1,,,"Architecture: Elman Networks consisting of an input layer, a hidden or history layer, and an output layer. The hidden layer computes a history representation at time t using a recursive process.; Optimizer: Adam; LR Schedule: Not mentioned explicitly in the paper, but they train for a fixed number of epochs (20 for PTB and 40 for WikiText-2).; Attention: Not applicable; Special Algorithms: Approximate Fixed-Points (AFP), Fixed-point iteration (FPI) algorithm; Initialization: Initial history states of all recurrent LMs were set to zeros. All other parameters were initialised randomly using the uniform distribution U(-0.05, 0.05).",,,,,,,,,,
331,Mogrifier RLSTM (WT2),Gábor Melis,2022/11/03,2022,Circling Back to Recurrent Models of Language,0.0,1.0,https://arxiv.org/pdf/2211.01848,3.50E+07,,,250,,,0.0,2080000.0,1.09E+17,,2080000.0,,,38.0,,1.0,1.0,0,0,Recurrent,LSTM,,,,,1,?,?,"Architecture: 2-layer Mogrifier RLSTM (Rewired LSTM) with residual connections. State dropout is applied. Mogrification is used for recurrent layers.; Optimizer: Rectified Adam; LR Schedule: If training diverges, weights and optimization state are reset to the previous best checkpoint and learning rate is multiplied by 0.9.; Training: BPTT, Multiple dropout samples, Dynamic evaluation (fast weights used at evaluation time but not at training time).; Attention: Attention mechanisms not explicitly used within the model architecture, but related to dynamic evaluation.; Regularization: Weight resetting when training diverges, Dropout (input, cell, state and output); Special Algorithms: Two-Tailed Averaging (2TA); Initialization: Forget gates are initialized with Chrono init.; Other: Capping input gate to ensure cell state values stay within a reasonable range (min(i, 1-f))",,,,,,,,,,
332,LSTM-Char-Large,"Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush",2015/08/26,2015,Character-Aware Neural Language Models,2033.0,,https://arxiv.org/abs/1508.06615,1.90E+07,,,25,,,0.0,929000.0,2.65E+15,,929000.0,Penn TreeBank,,,78.9,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/yoonkim/lstm-char-cnn,,,,1,Word-level,10000,"Architecture: Character-level CNN with max-over-time pooling -> Highway Network -> LSTM. The CNN has multiple filters of varying widths. The LSTM can be two-layered.; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: The learning rate is initially set to 1.0 and halved if the perplexity does not decrease by more than 1.0 on the validation set after an epoch.; Training: Truncated backpropagation through time, Gradient clipping (norm of the gradients is constrained to be below 5); Regularization: Dropout (probability 0.5 on LSTM input-to-hidden layers, except on the initial Highway to LSTM layer, and hidden-to-output softmax layer); Special Algorithms: Hierarchical Softmax (for large datasets DATA-L); Initialization: Parameters are randomly initialized over a uniform distribution with support [-0.05, 0.05]. by in highway layers is initialized to a small interval around -2.; Other: Max-over-time pooling, Highway Network (1 or 2 layers), Hierarchical Softmax uses random clusters",,,,,,,,,,
333,RNN Baseline,"Sho Takase, Jun Suzuki, Masaaki Nagata",2019/07/14,2019,Character n-Gram Embeddings to Improve RNN Language Models,26.0,,https://ojs.aaai.org/index.php/AAAI/article/view/4437,1.53E+08,,,UNK,,,0.0,103000000.0,#VALUE!,,103000000.0,,32.19,,,0.0,1.0,0,0,Recurrent/Convolutional,QRNN,,,,,1,Word-level,267735,,,,,,,,,,,
334,RNN + char3-MS-vec,"Sho Takase, Jun Suzuki, Masaaki Nagata",2019/07/16,2019,Character n-Gram Embeddings to Improve RNN Language Models,26.0,,https://ojs.aaai.org/index.php/AAAI/article/view/4439,1.75E+08,,,UNK,,,0.0,103000000.0,#VALUE!,,103000000.0,,31.81,,,0.0,1.0,0,0,Recurrent/Convolutional,QRNN,,,,,1,Word-level,267735,,,,,,,,,,,
335,RNN + char4-MS-vec,"Sho Takase, Jun Suzuki, Masaaki Nagata",2019/07/17,2019,Character n-Gram Embeddings to Improve RNN Language Models,26.0,,https://ojs.aaai.org/index.php/AAAI/article/view/4440,2.26E+08,,,UNK,,,0.0,103000000.0,#VALUE!,,103000000.0,WikiText-103,32.21,,,0.0,1.0,0,0,Recurrent/Convolutional,QRNN,,,,,1,Word-level,267735,,,,,,,,,,,
336,RNN + char2-MS-vec,"Sho Takase, Jun Suzuki, Masaaki Nagata",2019/07/15,2019,Character n-Gram Embeddings to Improve RNN Language Models,26.0,,https://ojs.aaai.org/index.php/AAAI/article/view/4438,1.58E+08,,,UNK,,,0.0,103000000.0,#VALUE!,,103000000.0,,31.92,,,0.0,1.0,0,0,Recurrent/Convolutional,QRNN,,,,,1,Word-level,267735,,,,,,,,,,,
337,CODA,"Lin Zheng, Zhiyong Wu, Lingpeng Kong",2021/05/31,2021,Cascaded Head-colliding Attention,2.0,1.0,https://arxiv.org/pdf/2105.14850,2.47E+08,,,,,,0.0,103000000.0,0.00E+00,,103000000.0,,18.48,,,0.0,1.0,0,0,Transformer,CODA,"https://github.com/LZhengisme/CODA, ",,,,1,Word-level,267744,"Architecture: Transformer-based architecture with Cascaded Head-colliding Attention (CODA), which replaces all vanilla MHA blocks with the cascaded head-colliding attention. CODA adopts a hierarchical variational distribution.; Optimizer: Adam for machine translation, Nesterov's accelerated gradient (NAG) for language modeling.; LR Schedule: Inverse square root scheduling for machine translation, cosine learning rate schedule for language modeling.; Training: Label smoothing (rate 0.1) for machine translation, Adaptive input embeddings (language modeling), Gradient norm clipping, Activation dropout (rate 0.1), Variational Inference; Attention: Multi-head attention (MHA) mechanism; Cascaded Head-colliding Attention (CODA) which explicitly models the interactions between attention heads through a hierarchical variational distribution.; Regularization: Dropout (0.1 for machine translation, 0.3 for language modeling); Special Algorithms: Cascaded Head-colliding Attention (CODA): Formulates MHA as a probabilistic model where each attention head is represented by a latent variable and all of them collide into the observed sequence data. Uses a hierarchical variational distribution to model interactions between attention heads.; Other: Adaptive input embeddings (language modeling)",,,,,,,,,,
338,LSTM-MemoryAug (PTB),"Ke Li, Daniel Povey, Sanjeev Khudanpur",2020/09/29,2020,Neural Language Modeling With Implicit Cache Pointers,4.0,1.0,https://arxiv.org/pdf/2009.13774,1.33E+07,,,,,,1.0,929000.0,0.00E+00,,929000.0,,,,67.8,0.0,0.0,0,0,Recurrent,LSTM,,,,,1,,,"Architecture: LSTM-MemoryAug (PTB). Base model is LSTM, with a memory-augmentation unit and an extended output layer to represent preceding words in history. The pre-softmax activation of these units is computed by a linear transformation of the hidden state.; Optimizer: SGD (for both LSTM and Transformer based LMs). For Transformer LMs, they also tried Adam, but failed to get better performance than SGD.; LR Schedule: Not explicitly specified, but assumed to be tuned in the experiments since they mentioned trying Adam with the learning rate schedule proposed in the original Transformer paper.; Training: Truncated Back-Propagation Through Time (BPTT), Dropout, Tie the embedding and output matrices; Attention: The work proposes a simpler alternative to attention-based pointer mechanism.  No explicit attention mechanism is used in the core approach.; Regularization: Dropout; Special Algorithms: Implicit cache pointer mechanism where the output is extended by a predefined size L to represent L preceding words in history., Memory Augmented Pointer Component which introduces an additional unit to capture the probability that a word may be a self-trigger; Initialization: Not explicitly mentioned.; Other: Supervision vector is set to have additional ones in history positions where the word is the same as the predicted one. (at-least-one-hot vector), For ASR experiments on SWBD and WSJ, to rescore each of the N hypotheses for an utterance, we find it useful to initialize the initial LM state with the last LM state of the best hypothesis for the previous utterance.",,,,,,,,,,
339,ISS,"Wei Wen, Yuxiong He, Samyam Rajbhandari, Minjia Zhang, Wenhan Wang, Fang Liu, Bin Hu, Yiran Chen, Hai Li",2017/09/15,2017,Learning Intrinsic Sparse Structures within Long Short-Term Memory,146.0,,https://arxiv.org/pdf/1709.05027,1.11E+07,,,55,,,0.0,929000.0,3.40E+15,,929000.0,,,,65.4,0.0,0.0,0,0,Recurrent,LSTM,https://github.com/wenwei202/iss-rnns,National Tsing Hua University; Google,Industry - Academia Collaboration,,1,,,"Architecture: Two-layer stacked LSTM; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: Not explicitly mentioned, but SGD is used which implies a learning rate is being used.; Training: dropout, thresholding to stabilize sparsity; Attention: Not applicable for the core LSTM experiments. BiDAF model for question answering uses a bidirectional attention flow mechanism.; Regularization: group Lasso regularization, l1-norm regularization (Appendix A), weight decay (l1-norm regularization in Appendix A); Special Algorithms: Intrinsic Sparse Structures (ISS); Initialization: Not mentioned",,,,,,,,,,
340,Alleviated TOI 10 (WT2),"Noémien Kocher, Christian Scuito, Lorenzo Tarantino, Alexandros Lazaridis, Andreas Fischer, Claudiu Musat",2019/09/18,2019,Alleviating Sequence Information Loss with Data Overlapping and Prime Batch Sizes,0.0,1.0,https://arxiv.org/abs/1909.08700,UNK,,,1000,,,1.0,103000000.0,#VALUE!,0.0,103000000.0,WikiText-2,,64.73,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/nkcr/overlap-ml,,,,0,,,"Architecture: LSTM, AWD-LSTM, Mixture of Softmaxes (MoS), Transformer; Optimizer: SGD; LR Schedule: Not explicitly mentioned, but implied to be controlled by the optimizers' parameters and the number of training epochs.; Attention: Mention of a local attention-based BiLSTM is cited from prior work (Mirsamadi et al., 2017) for speech emotion recognition.; Regularization: Weight Dropping (for AWD-LSTM); Special Algorithms: Alleviated Token Order Imbalance (Alleviated TOI) - iterative overlapping of token composition of data points, Prime Batch Sizes; Initialization: Not mentioned; Other: Maximum Over Softmax (MoS) technique",,,,,,,,,,
341,"AWD-LSTM-MoS + dynamic evaluation (WT2, 2017)","Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen",2017/11/10,2017,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,358.0,,https://arxiv.org/abs/1711.03953,3.50E+07,,,1000,,,0.0,2080000.0,4.37E+17,,2080000.0,WikiText-2,,40.68,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/zihangdai/mos,,,,1,Top-words,100000,"Architecture: 2-layer LSTM; Optimizer: Followed techniques in Merity et al. 2017.; LR Schedule: Followed techniques in Merity et al. 2017.; Attention: Not applicable; Regularization: F, o, l, l, o, w, e, d,  , t, e, c, h, n, i, q, u, e, s,  , i, n,  , M, e, r, i, t, y,  , e, t,  , a, l, .,  , 2, 0, 1, 7, .; Special Algorithms: Mixture of Softmaxes (MoS); Initialization: Not mentioned",,,,,,,,,,
342,"AWD-LSTM-MoS + dynamic evaluation (PTB, 2017)","Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen",2017/11/10,2017,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,358.0,,https://arxiv.org/abs/1711.03953,2.20E+07,,,1000,,,0.0,929000.0,1.23E+17,,929000.0,Penn TreeBank,,,47.69,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/zihangdai/mos,,,,1,Word-level,10000,"Architecture: AWD-LSTM with Mixture of Softmaxes (MoS); Optimizer: Not explicitly mentioned. Likely SGD or Adam based on AWD-LSTM paper.; LR Schedule: Not explicitly mentioned, but it likely leverages the AWD-LSTM's schedule (based on related work from Merity et al. (2017)); Training: Variational Dropout (V-dropout), Mixture of Softmaxes; Attention: Not applicable; Regularization: Recurrent weight dropout; Special Algorithms: Mixture of Softmaxes (MoS); Initialization: Not mentioned; Other: Dynamic Evaluation",,,,,,,,,,
343,aLSTM(depth-2)+RecurrentPolicy (PTB),"Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot",2018/05/22,2018,Breaking the Activation Function Bottleneck through Adaptive Parameterization,12.0,,https://arxiv.org/pdf/1805.08574,2.40E+07,,,180,,,0.0,929000.0,2.41E+16,,929000.0,,,,55.3,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/flennerhag/alstm,,,,1,Word-level,10000,"Architecture: adaptive LSTM (aLSTM) with 2 layers. The LSTM uses IO-adaptation, and the adaptation policies are parameterized by neural networks which are jointly trained with the main model. Two latent variable models are tested: static and recurrent. The recurrent model leverages a standard LSTM.; Optimizer: Adam, unless otherwise stated; LR Schedule: Initial learning rate of 0.003 with decay rates beta1=0 and beta2=0.999. The learning rate is cut after epochs 100 and 160 by a factor of 10.; Training: Variable truncated backpropagation through time centered at 70; Attention: Not applicable; Regularization: Weight decay 10e-6, Variational dropout; Special Algorithms: Adaptive parameterization: parameters in the affine transformation adapt to a given input; Initialization: Not mentioned; Other: The input summary variable for a multi-layer architecture is a hybrid of a standard LSTM and a Recurrent Highway Network (RHN)., Tied embedding weights",,,,,,,,,,
344,aLSTM(depth-2)+RecurrentPolicy (WT2),"Sebastian Flennerhag, Hujun Yin, John Keane, Mark Elliot",2018/05/22,2018,Breaking the Activation Function Bottleneck through Adaptive Parameterization,12.0,,https://arxiv.org/pdf/1805.08574,3.20E+07,,,190,,,0.0,2080000.0,7.59E+16,,2080000.0,,,64.5,,0.0,1.0,0,0,Recurrent,LSTM,https://github.com/flennerhag/alstm,,,,1,Word-level,267735,"Architecture: 2-layer adaptive LSTM (aLSTM). Adaptive feed-forward layers with IO-adaptation.; Optimizer: Adam; LR Schedule: Initial learning rate 0.003, cut after epochs 100 and 160 by a factor of 10; Training: Variable truncated backpropagation through time centered at 70; Regularization: weight decay 10-6, variational dropout; Special Algorithms: Adaptive parameterization; Initialization: Weight matrices initialized as semi-orthogonal (Saxe et al., 2013), but orthogonality not enforced after initialization; Other: Recurrent Highway Network (RHN) used in combination with LSTM for adaption policy., IO-adaptation: Integration of input- and output-adaptation into a jointly learned adaptation policy., Input summary variable is conditioned not only by the latent variable in its own layer, but also on that of the preceding layer",,,,,,,,,,
345,Stacked-LSTM+Pruning,"Liangjian Wen, Xuanyang Zhang, Haoli Bai, Zenglin Xu",2019/06/17,2019,Structured Pruning of Recurrent Neural Networks through Neuron Selection,34.0,,https://arxiv.org/pdf/1906.06847,6.16E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,78.08,0.0,0.0,0,0,Recurrent,LSTM,,UC Berkeley,Academia,,1,,,"Architecture: Stacked LSTM (two-layer in the baselines), binary gates are introduced on the input and hidden neurons; Optimizer: NT-ASGD; LR Schedule: Initial learning rate is equal to 20.0. The learning rate is divided by a factor of 1.02 at every epoch after it reaches 35 (in the Recurrent Highway Networks experiment); Training: gradient clipping (set to 0.25), Hard-sigmoid rectification of continuous random variables; Attention: N/A; Regularization: L0 norm regularization, L2 regularization on model parameters; Special Algorithms: Structured pruning through neuron selection (introducing binary gates on recurrent and input units to generate sparse masks), An efficient Lo inference algorithm for inferring the binary gate variables, motivated from the work of pruning DNN weights; Initialization: Default initialization strategy provided in PyTorch for the input and output word embedding",,,,,,,,,,
346,BLOOM-1.7B,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022/07/05,2022,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,404.0,,"https://huggingface.co/bigscience/bloom-3b#:~:text=Started%2011th%20March%2C%202022%2011,Ended%205th%20July%2C%202022",1.72E+09,,,1,,,0.0,350000000000.0,3.62E+21,,350000000000.0,,,20.17,,0.5,1.0,0,0,Transformer,Megatron-LM GPT2,,,,,1,BLOOM tokenizer,250684,,,,,,,,,,,
347,BLOOM-1B,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022/07/05,2022,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,404.0,,"https://huggingface.co/bigscience/bloom-3b#:~:text=Started%2011th%20March%2C%202022%2011,Ended%205th%20July%2C%202022",1.07E+09,,,1,,,0.0,350000000000.0,2.24E+21,,350000000000.0,,,23.7,,0.5,1.0,0,0,Transformer,Megatron-LM GPT2,,,,,1,BLOOM tokenizer,250684,,,,,,,,,,,
348,BLOOM-560M,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022/07/05,2022,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,404.0,,"https://huggingface.co/bigscience/bloom-3b#:~:text=Started%2011th%20March%2C%202022%2011,Ended%205th%20July%2C%202022",5.60E+08,,,1,,,0.0,350000000000.0,1.18E+21,,350000000000.0,,,30.05,,0.5,1.0,0,0,Transformer,Megatron-LM GPT2,,,,,1,BLOOM tokenizer,250684,,,,,,,,,,,
349,BLOOM-3B,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022/07/05,2022,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,404.0,,"https://huggingface.co/bigscience/bloom-3b#:~:text=Started%2011th%20March%2C%202022%2011,Ended%205th%20July%2C%202022",3.00E+09,,,1,,,0.0,350000000000.0,6.30E+21,,350000000000.0,,,17.57,,0.5,1.0,0,0,Transformer,Megatron-LM GPT2,,,,,1,BLOOM tokenizer,250684,,,,,,,,,,,
350,mini-GPT-2+Active-AdamW,"Davood Wadi, Marc Fredette, Sylvain Senecal",2023/01/24,2023,Read the Signs Towards Invariance to Gradient Descent’s Hyperparameter Initialization,0.0,0.0,https://arxiv.org/pdf/2301.10133.pdf,2.98E+06,,,200,,,0.0,103000000.0,3.68E+17,,103000000.0,,9.5,,,0.0,0.0,0,0,Transformer,GPT,,Facebook AI Research,Industry,,1,,,"Architecture: Transformer; Optimizer: AdamW; LR Schedule: ActiveLR meta algorithm adapts the learning rate locally for each parameter at each epoch based on the sign changes of the cumulative gradients; Training: ActiveLR, Mini-batch training; Attention: Not specified; Regularization: Weight decay; Special Algorithms: ActiveLR; Initialization: Not specified",,,,,,,,,,
351,BLOOM-7.1B,"Margaret Mitchell, Giada Pistilli, Yacine Jernite, Ezinwanne Ozoani, Marissa Gerchick, Nazneen Rajani, Sasha Luccioni, Irene Solaiman, Maraim Masoud, Somaieh Nikpoor, Carlos Muñoz Ferrandis, Stas Bekman, Christopher Akiki, Danish Contractor, David Lansky, Angelina McMillan-Major, Tristan Thrush, Suzana Ilić, Gérard Dupont, Shayne Longpre, Manan Dey, Stella Biderman, Douwe Kiela, Emi Baylor, Teven Le Scao, Aaron Gokaslan, Julien Launay, Niklas Muennighoff",2022/07/05,2022,BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model,404.0,,"https://huggingface.co/bigscience/bloom-3b#:~:text=Started%2011th%20March%2C%202022%2011,Ended%205th%20July%2C%202022",7.07E+09,,,1,,,0.0,350000000000.0,1.48E+22,,350000000000.0,,,14.72,,0.5,1.0,0,0,Transformer,Megatron-LM GPT2,,,,,1,BLOOM tokenizer,250684,,,,,,,,,,,
352,CD-GraB (WT103),"A. Feder Cooper, Wentao Guo, Khiem Pham, Tiancheng Yuan, Charlie F. Ruan, Yucheng Lu, Christopher De Sa",2023/02/02,2023,CD-GraB: Coordinating Distributed Example Orders for Provably Accelerated Training,0.0,0.0,https://arxiv.org/pdf/2302.00845.pdf,UNK,,,30,,,0.0,103000000.0,#VALUE!,,103000000.0,WikiText-103,66.11,,,0.0,0.0,0,0,Transformer,GPT,,Facebook AI Research,Industry,,0,,,"Architecture: Transformer; Optimizer: SGD with momentum and AdamW in some experiments; LR Schedule: Decays by 0.1 per 10 epochs in some experiments, linear learning rate scheduler in some fine-tuning experiments; Attention: Not specified; Regularization: weight decay, dropout (disabled in some experiments); Special Algorithms: Coordinated Distributed Gradient Balancing (CD-GraB), PairBalance Algorithm; Initialization: Random initialization with different random seeds; Other: Data-parallel training",,,,,,,,,,
353,"Segatron-XL large, M=384 + HCP","He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",2022/03/21,2022,Better Language Model with Hypernym Class Prediction,3.0,1.0,https://arxiv.org/abs/2203.10692,2.57E+08,,,167.02,,,0.0,103000000.0,2.65E+19,,103000000.0,WikiText-103,17.0,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/richardbaihe/robustLM,,,,1,,48935,"Architecture: Transformer-XL base (various sizes) and Segatron-XL base (various sizes). Different sizes refer to layers, heads, and hidden size, as described in the main text; Optimizer: Not specified; LR Schedule: Curriculum learning: Gradually anneal class prediction to token prediction during LM training. Constant and Linear decay are used. Hypernym Class Prediction (HCP); Training: mixed vocabulary, substitution rate annealing; Attention: Multi-head attention (number of heads varies with model size); Special Algorithms: Hypernym Class Prediction (HCP); Initialization: Not specified",,,,,,,,,,
354,Compress-LSTM (66M),"Artem M. Grachev, Dmitry I. Ignatov, Andrey V. Savchenko",2019/02/06,2019,Compression of Recurrent Neural Networks for Efficient Language Modeling,37.0,,"https://arxiv.org/abs/1902.02380#:~:text=Compression%20of%20Recurrent%20Neural%20Networks%20for%20Efficient%20Language%20Modeling,-Artem%20M.&text=Recurrent%20neural%20networks%20have%20proved,real%2Dtime%20offline%20mobile%20applications.",6.60E+07,,,90,,,0.0,929000.0,3.31E+16,,929000.0,,,,78.29,0.0,0.0,0,0,Recurrent,LSTM,,UC Berkeley,Academia,,1,,,"Architecture: LSTM with 2 hidden layers and sizes 200, 650, and 1500 are used. Also includes different types of recurrent cells like RNN and GRU. Compression is done on the output layer and LSTM layers separately.; Optimizer: Adam, SGD (Stochastic Gradient Descent); LR Schedule: Learning rate schedule is mentioned as a hyperparameter during randomized search, with stopping criteria when validation perplexity increases.; Training: Fine-tuning, stopping criteria when validation perplexity increases; Attention: Not applicable; Regularization: Conventional regularization techniques to prevent overfitting, mentioned stopping criteria when validation perplexity starts to increase., Variational dropout; Special Algorithms: Matrix factorization compression techniques, Tensor Train decomposition (TT-decomposition); Initialization: Not mentioned; Other: Pruning, Quantization, Low-rank factorization",,,,,,,,,,
355,T2R + Pretrain,"Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, Noah A. Smith",2021/03/24,2021,Finetuning Pretrained Transformers into RNNs,30.0,,https://arxiv.org/abs/2103.13076,4.50E+08,,2.03e+19,34.47,,,1.0,103000000.0,9.59E+18,103000000.0,206000000.0,WikiText-103,19.6,,,0.0,0.0,0,0,Transformer,ELU,https://github.com/jungokasai/T2R/,University of Washington; Facebook AI Research; Allen Institute for AI,Industry - Academia Collaboration,,1,,,"Architecture: Transformer encoder-decoder architecture where softmax attention is replaced with linear-complexity recurrent attention alternative and then finetuned. This alternative includes single-layer MLP feature mapping.; Optimizer: Adam (for machine translation); Nag (for language modeling); LR Schedule: cosine learning rate schedule with cycles (for language modeling); inverse square root (for machine translation); Training: teacher forcing, mixed precision training, gradient accumulation, BPE encoding; Attention: Multi-head attention with 'r' attention heads, where r is 8 for Language Modeling and 16 for machine translation.; Regularization: layer normalization, dropout 0.2 (for language modeling), 0.3 (for machine translation), label smoothing epsilon=0.1 (for machine translation), weight decay 0.01 (for machine translation); Special Algorithms: Transformer-to-RNN (T2R): converts a pretrained transformer into an RNN inference model, swap-then-finetune procedure; Initialization: Pretrained transformer weights are used to initialize the model, including the new MLP layers. For training models from scratch, the paper leverages nag optimizer with cosine lr-schedule for language modelling; Other: Encoder-Decoder Architecture, Beam search with size 5 and length penalty 0.6 for machine translation inference, MLP Feature mapping: relu (Wx+b)",,,,,,,,,,
356,Transformer Large + HCP,"He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",2022/03/21,2022,Better Language Model with Hypernym Class Prediction,3.0,1.0,https://arxiv.org/abs/2203.10692,2.57E+08,,,38.18,,,0.0,103000000.0,6.06E+18,,103000000.0,WikiText-103,25.3,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/richardbaihe/robustLM,,,,1,,48935,"Architecture: Transformer-XL based language model with varying number of layers (12, 16, 18), number of attention heads (10, 16), hidden size (300, 410, 1024) and batch size (64, 128, 256). Also used segment-aware Transformer (Segatron); Optimizer: Not explicitly mentioned in this excerpt.; LR Schedule: Gradually annealing from hypernym prediction to token prediction during training (Curriculum Learning). Implemented using constant and linear decay pacing functions.; Attention: Multi-head attention (number of heads varies with model size); Special Algorithms: Hypernym Class Prediction (HCP); Initialization: Not explicitly mentioned in this excerpt.",,,,,,,,,,
357,GRU + p-tHSM (pretrain via Brown) (WT2),"Nan Jiang, Wenge Rong, Min Gao, Yikang Shen, Zhang Xiong",2017/08/19,2017,Exploration of Tree-based Hierarchical Softmax for Recurrent Language Models,5.0,,https://www.researchgate.net/profile/Yikang-Shen-2/publication/318830618_Exploration_of_Tree-based_Hierarchical_Softmax_for_Recurrent_Language_Models/links/5b2c050aa6fdcc8506bc6f4a/Exploration-of-Tree-based-Hierarchical-Softmax-for-Recurrent-Language-Models.pdf,2.06E+08,,,,,,0.0,2000000.0,0.00E+00,,2000000.0,1.50E+06,,189.58,,0.0,0.0,0,0,Recurrent,GRU,,,,,1,,,,,,,,,,,,,
358,"Segatron -XL base, M=150 + HCP","He Bai, Tong Wang, Alessandro Sordoni, Peng Shi",2022/03/21,2022,Better Language Model with Hypernym Class Prediction,3.0,1.0,https://arxiv.org/abs/2203.10692,1.51E+08,,,18.64,,,0.0,103000000.0,1.74E+18,,103000000.0,WikiText-103,22.1,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/richardbaihe/robustLM,,,,1,,48935,"Architecture: Transformer-XL based architecture (Segatron-XL). The Transformer-XL architecture is extended with a memory segment.; Optimizer: Not explicitly mentioned in the provided context.; LR Schedule: Curriculum learning is used to gradually anneal from hypernym class prediction to token prediction.; Training: Mixed vocabulary training composed of hypernyms and tokens, Curriculum Learning; Attention: Based on Transformer and Transformer-XL architectures, multi-head self-attention is likely used, but details are not in this context; Special Algorithms: Hypernym Class Prediction (HCP); Initialization: Not explicitly mentioned in the provided context.; Other: Class-based language model, The tokens are mapped to their WordNet hypernyms, Masked language model",,,,,,,,,,
359,LSTM-Medium+Behaviorial-Gating,"Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan",2019/08/31,2019,Behavior Gated Language Models,3.0,0.0,https://arxiv.org/pdf/1909.00107,2.00E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,78.75,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,?,?,"Architecture: LSTM-based RNN model. The language model uses a vanilla unidirectional LSTM. The Behavior Gated Language Model consists of a multi-layered RNN over the input sequence of words. The first recurrent layer of the behavior model is initialized with pre-trained weights from a behavior classification model and fixed during language modeling training. The outputs of the pre-trained model are fed into a time-synced RNN, which is subsequently used for gating the RNNLM predictions.; Optimizer: Not explicitly mentioned in the provided paper.; LR Schedule: Hyperparameters such as learning rate are optimized through grid search.; Attention: Not applicable; Regularization: dropout (optimized via grid search); Special Algorithms: Behavior Gating: Using behavioral information to gate the outputs of the language model.; Initialization: The first recurrent layer of the behavior model is initialized with pre-trained weights from the behavior classification model.",,,,,,,,,,
360,AWD-LSTM+Behaviorial-Gating,"Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan",2019/08/31,2019,Behavior Gated Language Models,3.0,0.0,https://arxiv.org/pdf/1909.00107,2.70E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,56.92,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,?,?,"Architecture: AWD-LSTM, which augments a standard LSTM with a behavior-gating mechanism. Behavior model is implemented with a multi-layered RNN.; Optimizer: Hyperparameters (including learning rate) were optimized with grid search.; LR Schedule: Not explicitly mentioned, but optimized with a grid search.; Attention: Not applicable; Regularization: dropout (optimized using a grid search); Special Algorithms: Behavior Gating: augmenting a language model with a module which analyzes the behavioral state of the current context and gates the outputs of the language model before the final word prediction output.; Initialization: The first recurrent layer of the behavior model is initialized with pre-trained weights from a separate behavior classification model.; Other: Behavior model is pre-trained to identify the presence of each behavior from a sequence of words using a multi-label classification scheme.",,,,,,,,,,
361,TransformerXL-LayerFusion-CA,"James O' Neill, Greg Ver Steeg, Aram Galstyan",2020/07/29,2020,Compressing Deep Neural Networks via Layer Fusion,5.0,,https://arxiv.org/pdf/2007.14917,UNK,,,,,,0.0,103000000.0,#VALUE!,2080000.0,105000000.0,,,11.13,,0.0,0.0,0,0,Transformer,Transformer-XL,,,,,0,,,"Architecture: Transformer-XL; Optimizer: Not mentioned; LR Schedule: Exponential curriculum schedule to allocate the percentage of compression at each compression step.; Attention: Multi-head attention; Special Algorithms: Layer Fusion (LF), Wasserstein distance metric, Covariance Alignment; Initialization: Pretrained; Other: Freezing layers, Averaging layers, Random mixing layers, Truncated SVD, Denoising Autoencoders",,,,,,,,,,
362,LSTM-Large+Behaviorial-Gating,"Prashanth Gurunath Shivakumar, Shao-Yen Tseng, Panayiotis Georgiou, Shrikanth Narayanan",2019/08/31,2019,Behavior Gated Language Models,3.0,0.0,https://arxiv.org/pdf/1909.00107,6.70E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,75.8,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,?,?,"Architecture: LSTM-based RNN with behavioral gating. The language model is a unidirectional LSTM. A separate multi-layered RNN acts as the behavior model, which is pre-trained and its first recurrent layer's weights are fixed.  Outputs of the behavior model are fed into a time-synced RNN which then gates the outputs of the language model.; Optimizer: Not explicitly mentioned, but hyperparameters are optimized via grid search.; LR Schedule: Not explicitly mentioned, but hyperparameters are optimized via grid search, implying a potential search over various learning rates and schedules.; Attention: Not applicable; Regularization: dropout; Special Algorithms: Behavioral Gating Mechanism; Initialization: First recurrent layer of behavior model is initialized with pre-trained weights from a separate behavior classification model.",,,,,,,,,,
363,Scatterbrain,"Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, Christopher Ré",2021/10/28,2021,Scatterbrain: Unifying Sparse and Low-rank Attention Approximation,8.0,,https://web.archive.org/web/20220808053741/https://arxiv.org/pdf/2110.15343.pdf,UNK,1.03E+08,,30,,,0.0,103000000.0,#VALUE!,0.0,103000000.0,,26.72,,,0.0,1.0,0,0,Transformer,Transformer,https://github.com/HazyResearch/scatterbrain,Salesforce Resarch,Industry,,0,,,"Architecture: Transformer; Optimizer: Not specified, but experiments fine-tune BERT base model.; LR Schedule: Not specified, but experiments fine-tune BERT base model with a learning rate of 3e-5.; Attention: Sparse and low-rank attention approximation is unified using Scatterbrain (novel algorithm); Special Algorithms: Scatterbrain: An algorithm to unify sparse (Locality Sensitive Hashing) and low-rank (Kernel Feature Map) attention; Initialization: Not specified",,,,,,,,,,
364,BERT-Large-CAS (WT2),"Chenguang Wang, Mu Li, Alexander J. Smola",2019/04/20,2019,Language Models with Transformers,110.0,,https://arxiv.org/abs/1904.09408,3.95E+08,,,50,,,0.0,4400000000.0,5.21E+20,2080000.0,4400000000.0,WikiText-2,,34.11,,0.0,0.0,0,0,Transformer,BERT,https://github.com/cgraywang/gluon-nlp-1/tree/lmtransformer/scripts/language_model,,,,1,,,"Architecture: Multi-layer Transformer decoder based language model (GPT variant) and multi-layer bidirectional Transformer encoder (BERT variant). Explores adding LSTM layers before or after the Transformer blocks.; Optimizer: Adam; LR Schedule: Not explicitly stated but a fixed learning rate is used during fine-tuning and in architecture search the learning rate stays constant.; Training: Fine-tuning pre-trained weights, Truncated back-propagation through time; Attention: Self-attention heads (12 in GPT, 12 or 16 in BERT); Regularization: L2 weight decay 0.01, dropout 0.1 (applied to LSTM and final linear layer); Special Algorithms: Coordinate Architecture Search (CAS) - a greedy search algorithm that randomly generates variants of the Transformer architecture and fine-tunes them to find an effective architecture for language modeling.; Initialization: Random initialization for added Linear and LSTM layers; Other: Mixture of Softmax (MoS) with 15 components, FixSubset: Fixing the parameters of a subset of Transformer blocks during fine-tuning, AddLinear: Adding a linear output layer, AddLSTM: Adding an LSTM layer",,,,,,,,,,
365,CryptoGRU,"Bo Feng, Qian Lou, Lei Jiang, Geoffrey C. Fox",2020/10/22,2020,CryptoGRU: Low Latency Privacy-Preserving Text Analysis With GRU,12.0,,https://arxiv.org/pdf/2010.11796,,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,,0.0,0.0,0,0,Recurrent,GRU,,,,,0,,,"Architecture: Hybrid HE and GC GRU network. Replaces tanh with ReLU activations. Quantizes sigmoid and ReLU activations to smaller bit-length.; Special Algorithms: Combination of homomorphic encryption and garbled circuits for privacy-preserving text analysis.; Other: GC-based ReLU instead of GC-based tanh, Quantization of GC-based sigmoid and ReLU activations for smaller bitwidths",,,,,,,,,,
366,Transformer-XL + AutoDropout (PTB),"Hieu Pham, Quoc V. Le",2021/01/05,2021,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,45.0,,https://arxiv.org/abs/2101.01761,2.40E+07,,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,Penn TreeBank,,,54.9,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/google-research/googleresearch/tree/master/auto_dropout,,,,1,Word-level,10000,"Architecture: Transformer-XL; Optimizer: Adam; LR Schedule: cosine annealing with linear warmup; Training: reinforcement learning (REINFORCE) to train a controller, importance sampling for gradient updates, moving average baseline, entropy regularization; Attention: multi-head attention; Regularization: AutoDropout: learned dropout patterns, dropout on embedding layer, dropout on softmax layer, dropout elsewhere in Transformer-XL model, state-value and state-difference regularizations; Special Algorithms: AutoDropout: automated design of dropout patterns using a controller trained via reinforcement learning; Initialization: controller parameters initialized at a normal distribution with zero mean and a standard deviation of 0.02",,,,,,,,,,
367,Transformer-XL + AutoDropout (WT2),"Hieu Pham, Quoc V. Le",2021/01/05,2021,AutoDropout: Learning Dropout Patterns to Regularize Deep Networks,45.0,,https://arxiv.org/abs/2101.01761,3.50E+07,,,UNK,,,0.0,2080000.0,#VALUE!,,2080000.0,WikiText-2,,59.9,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/google-research/googleresearch/tree/master/auto_dropout,,,,1,?,?,"Architecture: Transformer-XL with 16 layers, hidden size of 380, 10 heads each of dimension 38, and positional feed-forward inner size of 900.; Optimizer: Adam; LR Schedule: Cosine annealing, starting at 3e-4 and decaying to 1e-4 throughout 80% of the training process, then remaining constant at 5e-5 for the last 20%.; Training: AutoDropout, Moving average baseline, Entropy regularization; Attention: Multi-head attention with 10 heads; Regularization: embedding dropout (0.5), softmax dropout (0.6), dropout (0.2) elsewhere in Transformer-XL model., state-value and state-difference regularizations; Special Algorithms: REINFORCE algorithm for controller training; Initialization: Controller parameters initialized at a normal distribution with zero mean and a standard deviation of 0.02.",,,,,,,,,,
368,All-attention network + adaptive span,"Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, Armand Joulin",2019/07/02,2019,Augmenting Self-attention with Persistent Memory,94.0,,https://arxiv.org/abs/1907.01470,1.33E+08,,4.6e+19,,,,0.0,103000000.0,0.00E+00,0.0,103000000.0,WikiText-103,20.6,,,0.0,1.0,0,0,Transformer,All-attention network,,,,,1,Word-level,260000,"Architecture: All-attention network. It augments the self-attention layers with persistent memory vectors, removing the feed-forward layer without performance degradation.; Optimizer: Adagrad for character level modeling, Adam for word level modeling; LR Schedule: Linear warmup followed by either dividing by 10 when validation loss plateaus (character level modeling) or using 8k warmup steps (word level modeling); Training: Layer normalization, Skip-connections; Attention: Multi-head self-attention with learned relative position embeddings. Adaptive attention span is used.; Regularization: Dropout applied to attention weights, Gradient clipping; Special Algorithms: Adaptive softmax, Tied adaptive softmax and adaptive input; Initialization: Token and position embeddings are initialized from N(0,1), and the matrices Wq,k,v,o from U(-sqrt(d), sqrt(d)). Persistent vectors are reparameterized by k_t = sqrt(d_h)k and v_t = sqrt(N)v",,,,,,,,,,
369,Search-Proven Best LSTM,"R. Józefowicz, Wojciech Zaremba, Ilya Sutskever",2015/07/06,2015,An Empirical Exploration of Recurrent Network Architectures,2207.0,,https://proceedings.mlr.press/v37/jozefowicz15.pdf,2.00E+07,,,30,,,0.0,929000.0,3.34E+15,,929000.0,,,,79.83,0.0,1.0,0,0,Recurrent,LSTM,,,,,1,Word-level,10000,,,,,,,,,,,
370,Engin-XL(NE),"Zhongping Zhang, Yiwen Gu, Bryan A. Plummer",2021/12/11,2021,Show and Write: Entity-aware Article Generation with Image Information,0.0,0.5,https://arxiv.org/pdf/2112.05917,1.50E+09,,,3,,,0.0,,0.00E+00,,0.0,,,16.3,,0.0,1.0,0,0,,,,,,,0,,,Architecture: GPT2 architecture; Optimizer: Adam; LR Schedule: linear warm-up for 0.06 epochs; Attention: GPT2 attention; Special Algorithms: Entity-aware mechanism to jointly model entity names and their corresponding entity categories; Initialization: GPT2 initialization; Other: CLIP is used to build an open-ended Visual-NER framework,,,,,,,,,,
371,H-LSTM+wg+rcp+rcg+wp,"Hongxu Yin, Guoyang Chen, Yingmin Li, Shuai Che, Weifeng Zhang, Niraj K. Jha",2019/01/30,2019,"Hardware-Guided Symbiotic Training for Compact, Accurate, yet Execution-Efficient LSTM",10.0,,https://arxiv.org/pdf/1901.10997,8.00E+05,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,72.1,0.0,0.0,0,0,Recurrent,LSTM,,University of Liverpool; USC Information Sciences Institute,Academia,,1,,,"Architecture: Internally deeper hidden-layer LSTM (H-LSTM). 1-layer H-LSTM with control gates containing one hidden layer.; Optimizer: Stochastic Gradient Descent (SGD) for language modeling. Nesterov SGD for speech recognition.; LR Schedule: Initial learning rate of 30, decayed by 10 when validation accuracy does not increase in 50 consecutive epochs (language modeling). Initial learning rate to 3e-4 and decay it by 0.99 after every training epoch (speech recognition).; Training: Weight growth, Structured row/column pruning, Hardware profile guided row/column growth, Weight pruning, Batch normalization (speech recognition); Attention: N/A; Regularization: Dropout 0.2 (control gates hidden layers), 0.65 (input embedding layers), 0.1 (input words) (language modeling). L2 regularization with weight decay of 1e-4 (speech recognition)., L2 regularization with weight decay of 1.2 × 10–6 (language modeling); Special Algorithms: Hardware-guided structured grow-and-prune algorithms; Initialization: Start training from a sparse seed architecture that contains a small fraction of connections; Other: Magnitude-based weight pruning",,,,,,,,,,
372,LSTM (2018),"Shaojie Bai, J. Zico Kolter, Vladlen Koltun",2018/03/04,2018,An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling,4024.0,,https://arxiv.org/abs/1803.01271,1.30E+07,,,,,,0.0,929000.0,0.00E+00,,929000.0,Penn TreeBank,,,78.93,0.0,1.0,0,0,Recurrent,LSTM,http://github.com/locuslab/TCN,,,,1,Word-level,10000,"Architecture: Temporal Convolutional Network (TCN) with residual blocks. Each residual block contains two layers of dilated causal convolution and a non-linearity (ReLU). 1x1 convolutions are used to ensure the input and output of a residual block have the same width.; Optimizer: Adam; LR Schedule: Learning rate 0.002 for TCN unless otherwise noted. SGD with learning rate annealing by a factor of 0.5 when validation accuracy plateaus in word-level language modelling.; Training: gradient clipping, weight normalization, spatial dropout after each dilated convolution; Attention: N/A; Regularization: spatial dropout, gradient clipping; Initialization: Weights initialized from a Gaussian distribution N(0, 0.01).; Other: Causal Convolutions, Dilated Convolutions. Dilation factor d is increased exponentially with the depth of the network. d = O(2^l) at level l of the network. This ensures that there is some filter that hits each input within the effective history, while also allowing for an extremely large effective history using deep networks., Weight tying between the encoder and decoder layers for both TCN and RNNs in word-level language modelling.",,,,,,,,,,
373,4 layer QRNN (h=2500),"Stephen Merity, Nitish Shirish Keskar, Richard Socher",2018/03/22,2018,An Analysis of Neural Language Modeling at Multiple Scales,183.0,,https://arxiv.org/abs/1803.08240,2.60E+07,,2.4e+17,14.00,,,0.0,103000000.0,2.25E+17,0.0,103000000.0,WikiText-103,33.0,,,0.0,1.0,0,0,Recurrent/Convolutional,QRNN,https://github.com/salesforce/awd-lstm-lm,,,,1,Word-level,260000,"Architecture: 4-layer QRNN (h=2500) for WikiText-103 dataset, 6-layer QRNN (h=1024) for Penn Treebank character-level dataset, 3-layer LSTM (h=1000) for Penn Treebank character-level dataset, 4-layer QRNN (h=1800) for enwik8 character-level dataset, 3-layer LSTM (h=1840) for enwik8 character-level dataset. Models consist of a trainable embedding layer, one or more layers of a stacked recurrent neural network, and a softmax classifier. LSTM and QRNN cells are evaluated.; Optimizer: Adam; LR Schedule: Learning rate reduced by 10 at specific epochs. WikiText-103: reduced on epoch 12. enwik8: reduced on epochs 25 and 35. Penn Treebank: reduced on epochs 300 and 400.; Training: Truncated backpropagation through time (BPTT), tied weights for the embedding and softmax classifier, adaptive softmax, random sequence lengths; Regularization: embedding dropout, variational dropout, activation regularization (AR), temporal activation regularization (TAR), weight decay, weight dropped LSTM (enwik8) of magnitude 0.2, L2-norm decay",,,,,,,,,,
374,N-gram,"Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato",2014/12/24,2014,Learning Longer Memory in Recurrent Neural Networks,306.0,,https://arxiv.org/abs/1412.7753,UNK,,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,,,,141.2,0.0,1.0,0,0,N-gram,N-gram,"http://github.com/facebook/SCRNNs, ",,,,0,,,"Architecture: Structurally Constrained Recurrent Network (SCRN). Has two hidden layers: a fast layer (hidden layer) with a fully connected recurrent matrix and a slowly changing layer (context layer) with a diagonal recurrent matrix.; Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: The learning rate is divided by 1.5 after each training epoch when the validation error does not decrease.; Training: Back-propagation through time, Batch gradient descent; Attention: N/A; Regularization: Gradient renormalization (equivalent to gradient clipping); Special Algorithms: Structural constraint on the recurrent weight matrix by using a diagonal matrix for the context layer; Initialization: Not mentioned explicitly; Other: Hierarchical Softmax is used in some experiments to speed up the computation. Two levels with the tokens binned into sqrt(d) clusters with same cumulative word frequency.",,,,,,,,,,
375,RNN (SGD+CLR) (PTB),"Yoshua Bengio, Nicolas Boulanger-Lewandowski, Razvan Pascanu",2012/12/04,2012,Advances in Optimizing Recurrent Networks,665.0,,https://arxiv.org/abs/1212.0901,2.05E+06,,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,,,,128.35,0.0,1.0,0,0,Recurrent,RNN,,,,,1,Word-level,10000,"Architecture: RNN (vanilla), RNN-RBM, RNN-NADE; Optimizer: SGD, Nesterov accelerated gradient (NAG); LR Schedule: constant learning rate; Regularization: L1 penalty on outputs of hidden units to promote sparsity of activations; Other: Gradient clipping, Leaky integration units, Rectified linear units",,,,,,,,,,
376,Feedback Transformer,"Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar",2020/02/21,2020,Addressing Some Limitations of Transformers with Feedback Memory,41.0,,https://arxiv.org/abs/2002.09402,1.26E+08,,,267.23,,,0.0,103000000.0,2.08E+19,,103000000.0,WikiText-103,18.3,,,0.0,1.0,0,0,Transformer,Feedback transformer,,,,,1,?,?,"Architecture: Feedback Transformer: a modified Transformer architecture where the self-attention sublayer attends to a memory that aggregates hidden states from all layers at previous timesteps. This allows for recursive computation and access to higher-level representations from the past. The transformer is modified by replacing the context inputs to attention modules with memory vectors computed over the past.  The memory vectors are computed by summing the representations of all layers at the current timestep, weighted by learnable scalar parameters and a softmax function.; Optimizer: Adam is used for language modelling. The specific configurations are defined in table 9.; LR Schedule: Not specified explicitly except for warmup steps; Training: Pre-normalization, Adaptive Span, Increasing BPTT length during training for efficiency; Attention: Multi-head self-attention, attends to a feedback memory vector instead of lower-level activations as in standard transformer; Regularization: Dropout (applied to attention and ReLU activations, and in some cases, to the embedding layer output and the last sublayer output.); Special Algorithms: Feedback memory mechanism: Aggregates hidden states from all layers at previous timesteps, allowing for recursive computation and access to higher-level representations from the past., Key-value projection sharing across all layers during attention computation.; Initialization: Not explicitly stated, but they start from pretrained representations in some experiments.; Other: Hidden Representation Caching, Relative Position Embedding",,,,,,,,,,
377,Transformer LM + MinSen,"Junhao Xu, Shoukang Hu, Jianwei Yu, Xunying Liu, Helen Meng",2021/11/29,2021,Mixed Precision of Quantization of Transformer Language Models for Speech Recognition,9.0,,https://arxiv.org/pdf/2112.11540,,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,56.82,0.0,0.0,0,0,,,,The University of Hong Kong; Sun Yat-sen University,Academia,,0,,,"Architecture: Transformer: Deep stacking of multi-head attention followed by feedforward layers. Includes residual connections and layer normalization.; Optimizer: Alternating Direction Method of Multipliers (ADMM); LR Schedule: Not explicitly mentioned, but implicitly used as part of the optimization algorithm and precision tuning; Training: Quantization, Mixed Precision; Attention: Scaled multi-head dot product self-attention; Regularization: Model complexity penalty (β term in formula 12); Special Algorithms: Quantization Sensitivity Metric based on Hessian trace weighted quantization perturbation, Mixed precision Transformer architecture search, Hutchinson's Algorithm to approximate the Hessian trace; Initialization: Separately trained Transformer LMs using uniform precision (1-bit, 2-bit, 4-bit and 8-bit) with ADMM optimization; Other: GELU activation function",,,,,,,,,,
378,N-gram+Cache,"Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, Marc'Aurelio Ranzato",2014/12/24,2014,Learning Longer Memory in Recurrent Neural Networks,306.0,,https://arxiv.org/abs/1412.7753,UNK,,,UNK,,,0.0,929000.0,#VALUE!,,929000.0,,,,125.0,0.0,1.0,1,0,N-gram,N-gram,"http://github.com/facebook/SCRNNs, ",,,,0,,,"Architecture: Simple Recurrent Network (SRN) with context features: SRN with a fast-changing hidden layer and a slowly changing context layer. The context layer uses a diagonal recurrent matrix to encourage slow changes. They also explore an alternative view of the model as a SRN with a constrained recurrent matrix, where a block is a reweighted identity matrix, and an off-diagonal block is zero (Structurally Constrained Recurrent Network - SCRN); Optimizer: Stochastic Gradient Descent (SGD); LR Schedule: Learning rate is divided by 1.5 after each training epoch when the validation error does not decrease.; Training: Back-propagation through time (BPTT), Batch gradient descent; Attention: Not applicable; Regularization: Gradient renormalization (equivalent to gradient clipping in practice); Special Algorithms: Structurally Constrained Recurrent Network (SCRN) - A modification to SRN which constrains a part of the recurrent matrix to be close to identity.; Initialization: Not explicitly mentioned.; Other: Hierarchical softmax: Used to approximate the softmax function, with binning tokens into √d clusters with same cumulative word frequency.",,,,,,,,,,
379,Transformer+Recurrent Windows of Context,"Davis Yoshida, Allyson Ettinger, Kevin Gimpel",2020/08/16,2020,Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size,4.0,0.5,https://arxiv.org/pdf/2008.07027,1.24E+08,,1.17e+20,2,,,0.0,4000000000.0,5.95E+18,103000000.0,4100000000.0,,26.73,,,0.0,1.0,0,0,Recurrent/Transformer,GPT,,,,,1,GPT2Tokenizer,50257,"Architecture: GPT-2 small model with 12 layers and 768-dimensional hidden state augmented with a small recurrence module (feedforward network with 3 hidden layers of dimension 200) inserted at a single layer, denoted lins, of the pretrained model.; Optimizer: Adam; LR Schedule: linear warmup over 100 steps, then constant; Training: backpropagation through time, gradient checkpointing; Attention: Self-attention within the pretrained transformer; additional embedding is only used as key and value, but not a query, in the self-attention layer.; Special Algorithms: Adding a small recurrence module that computes a fixed-size representation from the transformer hidden states in a window of text.; Initialization: pretrained GPT-2; Other: Mean-pooling of the embeddings produced by the pretrained transformer to produce a fixed-size representation., Recurrent module is inserted at a single layer (denoted lins) of the pretrained model.",,,,,,,,,,
380,Transformer (Adaptive Input Embeddings),"Alexei Baevski, Michael Auli",2018/09/28,2018,Adaptive Input Representations for Neural Language Modeling,337.0,,https://arxiv.org/abs/1809.10853,2.47E+08,,7.3e+18,180,,,0.0,103000000.0,2.75E+19,0.0,103000000.0,WikiText-103,18.7,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/facebookresearch/fairseq,,,,1,Word-level,267735,"Architecture: Decoder-only Transformer network with N=16 blocks. Each block contains two sub-blocks: multi-head self-attention module with H=16 heads, and feed-forward module (FFN) of the form ReLU(W1X+b1)W2+b2.; Optimizer: Nesterov's accelerated gradient method with a momentum of 0.99; LR Schedule: Linearly warmed up from 10^-7 to 1 for 16K steps and then annealed using a cosine learning rate schedule with C cycles. Each cycle runs for twice the number of updates than the previous cycle and the maximum and minimum learning rates are lowered by a rate M compared to the previous cycle. The initial minimum learning rate is 10^-5 and the maximum is 1.; Training: Gradient renormalization (if norm exceeds 0.1), Residual connections, Layer normalization before self-attention and FFN blocks instead of after, Accumulate gradient updates over two batches (WIKITEXT-103); Attention: Multi-head self-attention module with H = 16 heads; Regularization: Dropout (rate of 0.1 for Billion Word, 0.3 for Wikitext-103), Attention dropout (rate of 0.1), ReLU dropout (0.1, used for Wikitext-103), Dropout on the output of the first projection for all clusters in Adaptive Softmax, except for the head; Special Algorithms: Adaptive input embeddings extending adaptive softmax, Byte-pair encoding (BPE) for sub-word models; Initialization: Sinusoidal position embeddings added to the input layer; Other: Weight tying between input and output embeddings (tied softmax), Adaptive Softmax, Linear Projections for different clusters sizes",,,,,,,,,,
381,SparseOPT-175B,"Elias Frantar, Dan Alistarh",2023/01/02,2023,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,8.0,,https://arxiv.org/abs/2301.00774,8.75E+10,,,1.666666667,,,1.0,180000000000.0,1.58E+23,0.0,180000000000.0,,8.21,,,1.0,0.0,0,0,Transformer,OPT,https://github.com/IST-DASLab/sparsegpt,,,,1,,,"Architecture: Transformer; Optimizer: Not specified, but discusses using OBS (Optimal Brain Surgeon) update which relies on a quadratic approximation of the loss function; LR Schedule: Not specified; Attention: Multi-head attention is implicit in Transformer architecture. No specific modifications are indicated.; Special Algorithms: SparseGPT: A one-shot pruning method which reduces the pruning problem to a set of extremely large-scale instances of sparse regression.  It uses a new approximate sparse regression solver., Adaptive Mask Selection via iterative blocking; Initialization: Not specified; Other: OBS (Optimal Brain Surgeon) update for weight reconstruction, Iterative weight reconstruction, Hessian Synchronization: an algorithm to share Hessians between rows with distinct pruning masks",,,,,,,,,,
382,Transformer-XL+AdamP,"Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, Jung-Woo Ha",2020/06/15,2020,AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights,114.0,,https://arxiv.org/pdf/2006.08217,2.57E+08,,,UNK,,,0.0,103000000.0,#VALUE!,103000000.0,206000000.0,,23.26,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/clovaai/adamp,,,,1,Word-level,267735,"Architecture: Transformer-XL; Optimizer: AdamP, SGDP; LR Schedule: cosine annealing, step learning rate schedule (decays learning rates by 1/10 at 70% and 90% of training); Attention: Not explicitly mentioned, assuming standard multi-head attention for Transformer-XL; Regularization: weight decay; Special Algorithms: AdamP/SGDP: projection algorithm to remove the radial component from optimizer update step to slow down effective learning rate decay for scale-invariant weights; Initialization: Pytorch official ImageNet-pretrained ResNet50",,,,,,,,,,
383,Memformer (4 encoder + 16 decoder),"Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, Zhou Yu",2020/10/14,2020,Memformer: A Memory-Augmented Transformer for Sequence Modeling,5.0,,https://arxiv.org/abs/2010.06891,7.62E+07,,1.2e+19,11.93,,,0.0,103000000.0,5.62E+17,0.0,103000000.0,WikiText-103,22.74,,,0.0,0.0,0,0,Transformer,Memformer,,Microsoft,Industry,,1,,,"Architecture: Memformer: A Transformer architecture augmented with a fixed-size external dynamic memory. Consists of an encoder (4 layers used in experiments) and decoder (16 layers used in experiments). The encoder interacts with the external memory by reading and writing. The decoder uses cross attention to the output of the encoder.; Optimizer: Not explicitly mentioned, but learning rate details are given (1e-3); LR Schedule: Warm-up steps were used, details are in the Training Details section of the appendix.; Training: Memory Replay Back-Propagation (MRBP): a variant of gradient checkpointing to reduce memory cost during training recurrent neural networks, Biased Memory Normalization (BMN): A forgetting mechanism specifically designed for slot memory representations. It normalizes memory slots and introduces a learnable bias vector.; Attention: Multi-Head Attention is used in the memory reading and writing modules. Each memory slot attends to itself and the encoder outputs.; Regularization: dropout (0.1), weight decay (0.01), gradient clipping with max gradient norm of 1.0; Special Algorithms: Memory Replay Back-Propagation (MRBP), Biased Memory Normalization (BMN); Initialization: Initial state of v_bias is normalized, this vector is learnable; Other: Segment-level sequence modeling: Splits the input sequence into segments to reduce computation., Memory reading and writing modules to interact with external dynamic memory.",,,,,,,,,,
384,Adaptive Inputs + LayerDrop,"Angela Fan, Edouard Grave, Armand Joulin",2019/09/25,2019,Reducing Transformer Depth on Demand with Structured Dropout,435.0,,https://arxiv.org/abs/1909.11556,4.23E+08,,,,,,0.0,103000000.0,0.00E+00,103000000.0,206000000.0,WikiText-103,17.7,,,0.0,0.0,0,0,Transformer,Transformer-XL,,,,,1,,,"Architecture: Transformer, stack of layers composed of two sub-layers: multi-head self-attention followed by a feedforward sub-layer.; Optimizer: Adam (used for pre-training RoBERTa, Summarization, Bi-directional Pre-training); LR Schedule: Cosine learning rate schedule (used for Wikitext-103 language modeling, WMT en-de machine translation and Summarization). Polynomial decay learning rate schedule (used for RoBERTa pre-training).; Training: LayerDrop (structured dropout), Data augmentation, Label smoothing; Attention: Multi-head self-attention; Regularization: LayerDrop, Dropout; Special Algorithms: Adaptive softmax, Adaptive input; Initialization: Nesterov's accelerated gradient; Other: Structured Pruning, Every Other pruning strategy, Random structured dropout",,,,,,,,,,
385,Transformer-XL+WN+AdamP,"Byeongho Heo, Sanghyuk Chun, Seong Joon Oh, Dongyoon Han, Sangdoo Yun, Gyuwan Kim, Youngjung Uh, Jung-Woo Ha",2020/06/15,2020,AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights,114.0,,https://arxiv.org/pdf/2006.08217,2.57E+08,,,UNK,,,0.0,103000000.0,#VALUE!,,103000000.0,,22.77,,,0.0,1.0,0,0,Transformer,Transformer-XL,https://github.com/clovaai/adamp,,,,1,Word-level,267735,"Architecture: Transformer-XL (with or without weight normalization); Optimizer: AdamW, AdamP, SGD, SGDP; LR Schedule: cosine annealing, step decay, and various learning rates as determined by experimentation.; Training: Weight Normalization (WN) (applied artificially), Gradient clipping threshold (v) introduced in the Grassmann optimizers, Experiments also included data augmentation, stochastic depth, and dropout.; Regularization: Weight decay; Special Algorithms: AdamP and SGDP : Algorithms to project out the radial component from the update to prevent premature decay of effective step sizes.; Initialization: ImageNet-pretrained ResNet models.; Other: Loss Function : Triplet and ProxyAnchor losses in retrieval tasks., cosine similarity is used to detect scale invariances for user convenience.",,,,,,,,,,
386,SPN-4+KN5,"W. Cheng, Stanley Kok, Hoai Vu Pham, Hai Leong Chieu, K. M. A. Chai",2014/01/01,2014,Language modeling with sum-product networks,102.0,,https://spn.cs.washington.edu/papers/is14.pdf,5.00E+06,,4.4e+16,UNK,,,0.0,1010000.0,#VALUE!,,1010000.0,Penn TreeBank,,,80.6,0.0,0.0,0,0,,,https://github.com/stakok/lmspn,,,,1,,,,,,,,,,,,,
387,3-Layer-Tensor-Transformer+AdaHessian,"Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, Michael W. Mahoney",2020/06/01,2020,ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning,151.0,,https://arxiv.org/pdf/2006.00719,1.20E+07,,,30,,,1.0,929000.0,2.01E+15,0.0,929000.0,,,,51.5,0.0,1.0,0,0,Transformer,Tensorized Transformer,https://github.com/amirgholami/ADAHESSIAN.git,University of Massachusetts Amherst,Academia,,1,?,?,"Architecture: Transformer; Optimizer: ADAHESSIAN, a second-order stochastic optimization algorithm; LR Schedule: Same learning rate schedule as Adam/AdamW; Attention: Multi-linear attention mechanism with masking; Regularization: Weight decay (same as Adam/AdamW), Label smoothing (epsilon = 0.1); Special Algorithms: Fast Hutchinson-based method to approximate the curvature matrix, Root-mean-square exponential moving average to smooth out variations of the Hessian diagonal, Block diagonal averaging to reduce the variance of Hessian diagonal elements",,,,,,,,,,
388,6-Layer-Tensor-Transformer+AdaHessian,"Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, Michael W. Mahoney",2020/06/01,2020,ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning,151.0,,https://arxiv.org/pdf/2006.00719,8.53E+07,,,30,,,1.0,103000000.0,1.58E+18,0.0,103000000.0,,19.9,,,0.0,1.0,0,0,Transformer,Tensorized Transformer,https://github.com/amirgholami/ADAHESSIAN.git,University of Massachusetts Amherst,Academia,,1,?,?,"Architecture: Transformer; Optimizer: AdaHessian; LR Schedule: Same learning rate schedule, weight decay, warmup schedule, dropout, batch size, and first/second order moment coefficients as the default first-order baseline optimizers. Also step decay schedule and plateau based schedule used.; Training: Hutchinson's method to approximate the Hessian diagonal, Root-mean-square exponential moving average to smooth out variations of the Hessian diagonal across different iterations, Block diagonal averaging to reduce the variance of Hessian diagonal elements, Spatial averaging of the Hessian diagonal, Hessian Momentum; Attention: Multi-linear attention mechanism with masking (in language modeling tasks); Regularization: Weight decay (same as AdamW); Special Algorithms: AdaHessian: An Adaptive Second Order Optimizer for Machine Learning, which approximates the curvature of the loss function via adaptive estimates of the Hessian.; Initialization: Not mentioned; Other: Hessian Power",,,,,,,,,,
389,"Variational (untied weights, MC) LSTM (Large)","Yarin Gal, Zoubin Ghahramani",2015/12/16,2015,A Theoretically Grounded Application of Dropout in Recurrent Neural Networks,1838.0,,https://arxiv.org/abs/1512.05287?context=stat,6.60E+07,,,16,,,0.0,888000.0,5.62E+15,,888000.0,Penn TreeBank,,,73.4,0.0,1.0,0,0,Recurrent,RNN,https://github.com/yaringal/BayesianRNN,,,,1,?,?,"Architecture: LSTM, GRU, Simple RNN; Optimizer: Adam (mentioned in Section 5.2 for Sentiment Analysis, general method unstated); LR Schedule: Learning rate decay (used as a baseline/comparison, not necessarily the main method); Training: MC Dropout (Monte Carlo Dropout), Dropout, Variational Inference; Attention: N/A; Regularization: L2 Regularization, Weight decay, Dropout (variational dropout); Special Algorithms: Variational inference based dropout; Initialization: Normal prior distributions for weights (w = {W, Uh, bh, Wy, by }); Other: tied weights LSTM (tied-weights LSTM)",,,,,,,,,,
390,FAIRSEQ Adaptive Inputs,"Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli",2019/04/01,2019,"fairseq: A Fast, Extensible Toolkit for Sequence Modeling",2428.0,,https://arxiv.org/abs/1904.01038,2.47E+08,,7.3e+18,,,,0.0,103000000.0,0.00E+00,0.0,103000000.0,WikiText-103,18.7,,,0.0,0.0,0,0,Transformer,Transformer-XL,https://github.com/facebookresearch/fairseq,Amazon Web Services,Industry,,1,,,"Architecture: Transformer based language models, using only a decoder network. Some models had 16 blocks, inner dimension 4K and embedding dimension 1K. Other models had 24 blocks, inner dimension 8K and embedding dimension 1.5K.; Optimizer: Wrappers around most PyTorch optimizers and an implementation of Adafactor, which is a memory-efficient variant of Adam.; LR Schedule: Several popular schedulers, e.g., the inverse square-root scheduler from Vaswani et al. (2017) and cyclical schedulers based on warm restarts (Loshchilov and Hutter, 2016).; Training: Mixed precision training (FP16), Multi-GPU training, Gradient accumulation, Overlapping gradient synchronization; Attention: Multi-head attention in the Transformer architecture (used component-specific caching implementation for inference); Regularization: Dynamic loss scaling to avoid underflows for activations and gradients when using FP16; Other: Adaptive input embeddings",,,,,,,,,,
391,Tensorized Transformer (small),"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",2019/06/24,2019,A Tensorized Transformer for Language Modeling,126.0,,https://arxiv.org/abs/1906.09777,1.20E+07,,,30.00,,,0.0,929000.0,2.01E+15,,929000.0,WikiText-103,,,57.9,0.0,1.0,0,0,Transformer,Tensorized Transformer,,,,,1,Word-level,10000,"Architecture: Transformer with Multi-linear attention (a novel self-attention method with Block-Term Tensor Decomposition (BTD)) replacing the standard multi-head attention; Optimizer: Adam; LR Schedule: Vary the learning rate over the course of training. Formula is followed as described in Attention is All you Need.; Training: Label Smoothing (€=0.1), variational dropout and weight average; Attention: Multi-linear attention combines parameters sharing and low-rank approximation. It is constructed by a Block-Term tensor decomposition. The standard Multi-head attention can be compressed with higher compression ratios.; Regularization: dropout (0.3 in all datasets), weight average; Special Algorithms: Block-Term Tensor Decomposition; Initialization: Xavier initialization for weight matrices; Other: Multi-linear attention builds the strong connection between three factor matrices (queries, keys and values), Positional Encoding",,,,,,,,,,
392,Tensorized Transformer (large PTB),"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",2019/06/24,2019,A Tensorized Transformer for Language Modeling,126.0,,https://arxiv.org/abs/1906.09777,2.40E+07,,,30.00,,,0.0,929000.0,4.01E+15,,929000.0,WikiText-103,,,52.7,0.0,1.0,0,0,Transformer,Tensorized Transformer,,,,,1,Word-level,10000,"Architecture: Tensorized Transformer, which uses Multi-linear attention based on Block-Term Tensor Decomposition (BTD) to compress multi-head attention; Optimizer: Adam; LR Schedule: Vary the learning rate over the course of training using the formula described in Attention is All You Need paper.; Training: Label Smoothing (epsilon=0.1), dropout=0.3 (in all datasets, during training of the base models), variational dropout and weight average (similar to AWD-LSTM-MOS when applied to language modeling tasks on PTB dataset); Attention: Multi-linear attention based on Block-Term Tensor Decomposition (BTD). The Multi-linear attention combines two compression ideas, parameters sharing and low-rank approximation.; Special Algorithms: Multi-linear attention with Block-Term Tensor Decomposition; Initialization: Xavier Normal initialization is used for the parameters of the multi-linear attention mechanism (w_q, w_k, w_v); Other: Scaled Dot-Product Attention, Tensor Decomposition, Parameters Sharing, Low-Rank Approximation, The compression ratios are computed as formulas.",,,,,,,,,,
393,TPM-LVD,"Anji Liu, Honghua Zhang, Guy Van den Broeck",2022/10/10,2022,Scaling up Probabilistic Circuits by Latent Variable Distillation,7.0,,https://arxiv.org/pdf/2210.04398.pdf,1.12E+09,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,,,197.5,,0.0,0.0,0,0,,,,,,,1,,,"Architecture: Probabilistic Circuits (PCs), specifically Hidden Markov Models (HMM) and patch-based PCs; Optimizer: Expectation-Maximization algorithm (EM); LR Schedule: Linear annealing from 0.1 to 0.01 or 0.001 during training; Training: Mini-batch training, Latent variable distillation (LVD); Attention: Not applicable; Special Algorithms: Latent variable distillation (LVD) - using information from deep generative models (Transformer-based, BERT, Masked Autoencoders (MAEs)) to provide extra supervision to PC optimizers, K-means algorithm for clustering embeddings; Initialization: In some cases, pre-trained deep generative models are used to initialize parameters.; Other: Materializing Latent Variables, Inducing Latent Variable Assignments, PC Parameter Learning",,,,,,,,,,
394,Tensorized Transformer (257M),"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",2019/06/24,2019,A Tensorized Transformer for Language Modeling,126.0,,https://arxiv.org/abs/1906.09777,2.57E+08,,,30.00,,,0.0,103000000.0,4.76E+18,,103000000.0,WikiText-103,21.2,,,0.0,1.0,0,0,Transformer,Tensorized Transformer,,,,,1,Word-level,260000,"Architecture: Transformer with multi-linear attention using Block-Term Tensor Decomposition (BTD) to compress the multi-head attention mechanism; Optimizer: Adam; LR Schedule: The vary formula [37] is followed in our work. We also used the warmup_steps = 4000.; Training: Label Smoothing with value 0.1, Variational dropout, Weight average; Attention: Multi-linear attention with Block-Term Tensor Decomposition (BTD); Regularization: dropout 0.3 in all datasets(PTB, WikiText-103 and One-Billion); Special Algorithms: Multi-linear attention, Block-Term Tensor Decomposition; Initialization: xavier_normal_ for weight matrices (Wq, Wk, Wv); Other: Parameters sharing: sharing factor matrices across multiple blocks, Low-rank approximation",,,,,,,,,,
395,WD+LR+M,"Ross M. Clarke, Elre T. Oldewage, José Miguel Hernández-Lobato",2021/10/20,2021,Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation,5.0,,https://arxiv.org/pdf/2110.10461,UNK,,,72,,,1.0,929000.0,#VALUE!,,929000.0,,,,100.0,0.0,1.0,0,1,Recurrent,LSTM,https://github.com/rmclarke/OptimisingWeightUpdateHyperparameters,University of Edinburgh; Toyota Technological Institute at chicago,Industry - Academia Collaboration,,0,,,"Architecture: 2-layer, 650-unit LSTM with learnable embedding; Optimizer: SGD with momentum, Adam (for meta-learning); LR Schedule: Independent learning rates for each model parameter. Hyperparameters are tuned on the validation set. Hyperparameters are updated every T=10 batches.; Training: Gradient clipping to a Euclidean norm of 0.25, Weight updates prior to each hyperparameter update, Weight updates use Jacobian-vector products for memory efficiency, truncated back-propagation and momentum histories; Regularization: Weight decay; Special Algorithms: Approximate hypergradient-based hyperparameter optimiser for differentiable model weight updates, Scalable One-Pass Optimisation of High-Dimensional Weight-Update Hyperparameters by Implicit Differentiation; Initialization: learning rates, weight decays, and momenta are uniformly sampled using logarithmic and sigmoidal transforms (see Appendix B.3), applying each initialisation; Other: Look-back distance (i) is a predefined value to trade off accuracy and computational efficiency",,,,,,,,,,
396,Tensorized Transformer (core-2),"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",2019/06/24,2019,A Tensorized Transformer for Language Modeling,126.0,,https://arxiv.org/abs/1906.09777,8.53E+07,,,30.00,,,0.0,103000000.0,1.58E+18,,103000000.0,WikiText-103,18.9,,,0.0,1.0,0,0,Transformer,Tensorized Transformer,,,,,1,Word-level,260000,"Architecture: Transformer with a modified multi-head attention mechanism called Multi-linear attention based on Block-Term Tensor Decomposition (BTD).  The original multi-head attention is replaced by the proposed multi-linear attention in the encoder.; Optimizer: Adam; LR Schedule: Vary the learning rate during training, and use the formula from 'Attention is all you need' (Vaswani et al., 2017). Also use warmup_steps = 4000.; Training: Label Smoothing (epsilon=0.1), dropout=0.3 in all datasets., variational dropout and weight average to the model, similar to AWD-LSTM-MOS; Attention: Multi-linear attention, which is constructed using Block-Term Tensor Decomposition and parameter sharing to compress the original multi-head attention. Each Linear function in multi-head attention is about a weight matrix W ∈ Rdmodel×d. The number of heads is h in the experiments the value 2 is used.; Regularization: dropout=0.3; Special Algorithms: Multi-linear attention, Block-Term Tensor Decomposition (BTD); Initialization: weight matrices  Wq,Wk,Wv are initialized as xavier_normal; Other: Self-attention (i.e., scaled dot-product attention) in Transformer can be reconstructed.  However, the authors do not consider reconstructing it and choose to split the 3-order tensor (the output of Multi-linear attention).",,,,,,,,,,
397,Tensorized Transformer (151M),"Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, Dawei Song",2019/06/24,2019,A Tensorized Transformer for Language Modeling,126.0,,https://arxiv.org/abs/1906.09777,1.51E+08,,,30.00,,,0.0,103000000.0,2.80E+18,,103000000.0,WikiText-103,18.8,,,0.0,1.0,0,0,Transformer,Tensorized Transformer,,,,,1,Word-level,260000,"Architecture: Transformer with multi-linear attention based on Block-Term Tensor Decomposition (BTD); Optimizer: Adam; LR Schedule: Varying learning rate over course of training, following formula in Attention is All You Need. Warmup steps = 4000.; Training: Label Smoothing (epsilon=0.1), Variational dropout, Weight average; Attention: Multi-linear attention: combination of CP decomposition and Tucker decomposition for self-attention, parameters sharing, low-rank approximation. Replaces multi-head attention in Transformer.; Regularization: dropout = 0.3 (used in all datasets); Special Algorithms: Block-Term Tensor Decomposition (BTD), Tucker decomposition; Initialization: xavier_normal_ for weight matrices Wq, Wk, and Wv; Other: Splitting and concatenating after tensor splitting",,,,,,,,,,
398,SparseOPT-13B,"Elias Frantar, Dan Alistarh",2023/01/02,2023,SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot,8.0,,https://arxiv.org/abs/2301.00774,6.50E+09,,,1.666666667,,,1.0,180000000000.0,1.17E+22,0.0,180000000000.0,,11.17,,,1.0,0.0,0,0,Transformer,OPT,https://github.com/IST-DASLab/sparsegpt,,,,1,,,Architecture: Transformer; Optimizer: Not specified in the paper; LR Schedule: Not specified in the paper; Attention: Not specified in the paper; Special Algorithms: SparseGPT; Initialization: Not specified in the paper,,,,,,,,,,
399,NMST+GPT-2,"Eugene Choi, Cheolhyoung Lee, Kyunghyun Cho",2022/10/03,2022,A Non-monotonic Self-terminating Language Model,0.0,1.0,https://web.archive.org/web/20230220171748/https://arxiv.org/pdf/2210.00660.pdf,1.24E+08,4.10E+09,1.2e+20,2.98,,,0.0,4000000000.0,8.87E+18,103000000.0,4100000000.0,,20.69,,,0.0,1.0,0,0,Transformer,GPT,https://github.com/nyu-dl/non-monotonic-self-terminating-lm,,,,1,Word-level,260000,"Architecture: Transformer architecture inherited from GPT. Additionally uses RNN and LSTM architectures for comparison.; Optimizer: AdamW; LR Schedule: Linear learning rate decay (also halving the learning rate if validation perplexity does not improve for a training epoch, applied to RNN and LSTM); Training: Weight tying (sharing weights between input and output embedding layers, applied to RNN and LSTM), Early stopping (if validation perplexity does not improve for 10 consecutive epochs, applied to RNN and LSTM); Attention: Not specified. Inherited from GPT.; Regularization: Weight decay, Dropout (probabilities of 0.3 and 0.5 for RNN and LSTM, respectively), Decoupled weight decay; Special Algorithms: Non-monotonic Self-Terminating (NMST) language model; Initialization: Not mentioned; Other: BPE tokenization, Bucketing by sequence length for computational efficiency",,,,,,,,,,
400,NMM(LSTM+RNN),"Youssef Oualil, Dietrich Klakow",2017/08/23,2017,A Neural Network Approach for Mixing Language Models,10.0,,https://arxiv.org/pdf/1708.06989,5.18E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,,,,102.0,0.0,1.0,0,0,Recurrent,RNN+LSTM,,,,,1,,10000,"Architecture: Neural Mixture Model (NMM) which combines different NN-based LMs (FNN, RNN, LSTM) through a feature layer and a mixture layer. It uses a single word embedding matrix and a single output layer. The mixture models can be deep architectures with multiple hidden layers.; Optimizer: Stochastic gradient descent; LR Schedule: The learning rate is initialized to 0.4 and halved when no significant improvement in the log-likelihood of the validation data is observed.; Regularization: Weight decay fixed at 4e-5, Model dropout with probability 0.4 (applied only to non-recurrent models); Initialization: Weights initialization follows the normalized initialization proposed in [19] (Glorot and Bengio, 2010).; Other: Back-propagation through time (BPTT) with 5 time steps for all recurrent models",,,,,,,,,,
401,TSLM+MoS (PTB),"Lipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, Dawei Song",2019/01/31,2019,A Generalized Language Model in Tensor Space,21.0,,https://arxiv.org/pdf/1901.11167,2.63E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,Penn TreeBank,,,83.6,0.0,1.0,0,0,,TSLM,,University of Manchester; The Alan Turing Institute,Academia,,1,,10000,Architecture: Tensor Space Language Model (TSLM) based on a high-dimensional semantic space constructed by the tensor product of word vectors. It is a generalization of the n-gram language model.  Uses tensor networks and tensor decomposition to recursively calculate conditional probability for language modeling.; Optimizer: nan; LR Schedule: nan; Attention: nan; Initialization: nan,,,,,,,,,,
402,TSLM+MoS (WT2),"Lipeng Zhang, Peng Zhang, Xindian Ma, Shuqin Gu, Zhan Su, Dawei Song",2019/01/31,2019,A Generalized Language Model in Tensor Space,21.0,,https://arxiv.org/pdf/1901.11167,9.12E+06,,,,,,0.0,2080000.0,0.00E+00,,2080000.0,,,81.0,,0.0,1.0,0,0,,TSLM,,,,,1,,33278,Architecture: Tensor Space Language Model (TSLM) based on high-dimensional semantic space constructed by the tensor product of word vectors. It derives a recursive calculation of conditional probability for language modeling using tensor networks and tensor decomposition.; Optimizer: Not mentioned in the paper; LR Schedule: Not mentioned in the paper; Attention: Not applicable; Special Algorithms: TSLM: A language model constructed using tensor networks and tensor decomposition in a high-dimensional semantic space.; Initialization: Not mentioned in the paper,,,,,,,,,,
403,GPT2-Large+LHOPT,"Diogo Almeida, Clemens Winter, Jie Tang, Wojciech Zaremba",2021/06/02,2021,A Generalizable Approach to Learning Optimizers,13.0,,https://web.archive.org/web/20221027150413/https://arxiv.org/pdf/2106.00958.pdf,7.60E+08,1.03E+08,1.6e+21,1,,,0.0,103000000.0,4.70E+17,0.0,103000000.0,,32.5,,,0.0,1.0,0,1,Transformer,GPT,https://github.com/openai/LHOPT,,,,1,?,?,"Architecture: Transformer (GPT2-Large as base model); Optimizer: Customizable Inner Adaptive Optimizer (CIAO) which generalizes existing optimizers like Adamax and LAMB; LR Schedule: Learned schedule via LHOPT or fixed hyperparameter schedule; Training: Gradient clipping, Reward shaping using intermediate validation losses, Early stopping based on validation loss, PPO for training the LHOPT controller; Attention: Implicit through the use of a Transformer architecture; Regularization: Weight decay; Special Algorithms: Learned Hyperparameter Optimizers (LHOPTs); Initialization: Weights initialized randomly. Random initial hyperparameter values for inner optimizer sampled during training of LHOPT controller; Other: Actions in LHOPT include scaling hyperparameters, logit shifting hyperparameters, and restarting from checkpoints, Unitless features used for LHOPT policy network inputs, Discrete actions used for LHOPT instead of continuous, Policy network is a single hidden layer LSTM with 256 hidden units, Inner optimizer involves automating clipping gradients based on an exponentially weighted moving maximum of the gradient norm, CDF (cumulative distribution function) features used to encode relative values of features within an inner task, Features normalized using exponential moving average and standard deviation, EMA of weights is used for baseline in reward function",,,,,,,,,,
404,2nd order FOFE-FNNLM,"Shiliang Zhang, Hui Jiang, Mingbin Xu, Junfeng Hou, Lirong Dai",2015/05/06,2015,A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models,18.0,,https://arxiv.org/abs/1505.01504,6.00E+06,,,,,,0.0,929000.0,0.00E+00,,929000.0,Penn TreeBank,,,108.0,0.0,1.0,0,0,Feed-forward,FNN-LM,,,,,1,,10000,"Architecture: Feed-forward neural network language model (FNN-LM) with FOFE encoding. The network has a linear projection layer and two hidden layers. The 2nd-order FOFE-FNNLM uses both zt and zt-1 as input for each time t.; Optimizer: Stochastic Gradient Descent (SGD) with mini-batches; LR Schedule: The learning rate is kept fixed as long as the perplexity on the validation set decreases by at least 1. After that, the learning rate is halved after each epoch for six more epochs.; Attention: N/A; Special Algorithms: Fixed-size Ordinally-Forgetting Encoding (FOFE); Initialization: Weights are initialized based on the normalized initialization in [Glorot, 2010].",,,,,,,,,,
405,LBL,"Andriy Mnih, Yee Whye Teh",2012/06/27,2012,A Fast and Simple Algorithm for Training Neural Probabilistic Language Models,835.0,,https://arxiv.org/pdf/1206.6426,2.00E+06,,,45,,,0.0,929000.0,5.02E+14,,929000.0,,,,159.1,0.0,1.0,0,0,Probabilistic,NPLM,,,,,1,,10000,"Architecture: Log-bilinear language (LBL) model with separate feature vector tables for context words (rw) and target words (qw). The model computes the predicted representation for the target word by linearly combining the feature vectors for the context words using position-dependent context weight matrices Ci: q = ∑ Cirwi. The score for the match between the context and the next word is computed by taking the dot product between the predicted representation and the representation of the candidate target word w: so(w, h) = qTw qw + bw.; Optimizer: Not explicitly mentioned, but parameters are updated based on mini-batches. Learning rates were adapted at the end of each epoch.; LR Schedule: Learning rates were adapted at the end of each epoch based on the change in the validation set perplexity since the end of the previous epoch. The rates were halved when the perplexity increased and were left unchanged otherwise.; Training: Noise-Contrastive Estimation (NCE); Attention: Not applicable; Regularization: Weight penalty (small) to avoid overfitting; Special Algorithms: Noise-Contrastive Estimation (NCE); Initialization: Not explicitly mentioned; Other: Diagonal context weight matrices can be used to reduce the complexity of computing the predicted representation.",,,,,,,,,,
406,$\infty$-former (SM),"Pedro Henrique Martins, Zita Marinho, André F. T. Martins",2021/09/01,2021,$\infty$-former: Infinite Memory Transformer,31.0,,https://arxiv.org/abs/2109.00301,1.17E+08,,1.2e+20,1,,,0.0,4000000000.0,2.81E+18,255000000.0,4260000000.0,WikiText-103,16.61,,,1.0,1.0,0,0,Transformer,GPT,https://github.com/deep-spin/infinite-former,,,,1,?,?,"Architecture: Transformer extended with an unbounded long-term memory (LTM), named the ∞-former. The LTM is built using a continuous-space attention framework.; Optimizer: Adam; LR Schedule: Cosine schedule for sorting task, linearly decayed for document grounded generation, cosine schedule for Wikitext 103 experiments; Attention: Multi-head self-attention in transformer layers. Continuous attention mechanism to attend over the long-term memory using a Gaussian probability density.; Regularization: Kullback-Leibler (KL) divergence regularization of attention probability density with a Gaussian prior.; Special Algorithms: Sticky memories: a procedure that enforces the persistence of important information in the LTM, Continuous-space attention framework; Other: Using radial basis functions (RBFs) to represent input sequence as a continuous signal",,,,,,,,,,
407,TransformerXL+RelationLM,"Qi Liu, Dani Yogatama, Phil Blunsom",2022/01/24,2022,Relational Memory-Augmented Language Models,21.0,,https://arxiv.org/pdf/2201.09680,1.24E+08,,3.2e+21,,,,0.0,103000000.0,0.00E+00,,103000000.0,WikiText-103,18.6,,,0.0,0.0,0,0,Transformer,Transformer-XL,,Tianjin University; Microsoft Research; Beijing Institute of Technology,Industry - Academia Collaboration,,1,,,"Architecture: Transformer-XL with a relational memory component.  The relational memory is constructed from relation triples extracted using OpenIE and encoded using an LSTM. The language model uses the representations to predict the next token through a gating mechanism; Optimizer: Adam; LR Schedule: Cosine annealing with 4,000 warmup steps; Training: dropout (rate 0.25), Dynamic OpenIE; Attention: Scaled dot-product attention is used to aggregate the relational memory with the Transformer-XL hidden states; Regularization: Dropout (0.25); Special Algorithms: Dynamic OpenIE: Extracts relations from previously seen text segments of the evaluation set., Gating mechanism to adaptively combine the Transformer-XL representation and the relation memory",,,,,,,,,,
